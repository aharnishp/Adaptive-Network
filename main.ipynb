{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from adaptivenetworks.nnetwork import nnetwork\n",
    "\n",
    "telementary = 1\n",
    "runNum = 0      ## Increment to utilise caching\n",
    "# batch_size=100       ## assumed size of dataset\n",
    "learningRate = 0.03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Grid Plotter for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorList = []\n",
    "boundList = [-0.5]\n",
    "\n",
    "\n",
    "gradDepth = 16\n",
    "\n",
    "for i in range(gradDepth + 1):\n",
    "    value = i/gradDepth\n",
    "    boundList.append(((i+1)/(gradDepth+1)) - 0.5)\n",
    "    colorList.append([value,value,value])\n",
    "\n",
    "# print(\"colorList\", colorList)\n",
    "# print(\"boundList\", boundList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear function\n",
    "# import only system from os\n",
    "from os import system, name\n",
    "\n",
    "# import sleep to show output for some time period\n",
    "from time import sleep\n",
    "\n",
    "def clearScreen():\n",
    "    if name == 'nt':\n",
    "        _ = system('cls')\n",
    "    else:\n",
    "        _ = system('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMap(data, sizes=[10,10]):\n",
    "    # create discrete colormap\n",
    "    cmap = colors.ListedColormap(colorList)\n",
    "    bounds = boundList\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(data, cmap=cmap, norm=norm)\n",
    "\n",
    "    # draw gridlines\n",
    "    ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=1)\n",
    "    ax.set_xticks(np.arange(-.5, sizes[0], 1));\n",
    "    ax.set_yticks(np.arange(-.5, sizes[1], 1));\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('mnist-train.csv')\n",
    "# Adaptive-Matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "X_trainT = X_train.T\n",
    "\n",
    "def one_hot(Y, maxExpected):\n",
    "    one_hot_Y = np.zeros((Y.size, maxExpected + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "## find the index of most probable number guessed by network\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "## find ratio of correct predictions to all data\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with non dynamic 784-10-10-10 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = nnetwork(784, 10, insertDefault=0, learningRate=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "\n",
    "nt.output_layer.activationFn = \"softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38280578]\n",
      " [0.01473069]\n",
      " [0.00964241]\n",
      " [0.01194987]\n",
      " [0.01086046]\n",
      " [0.06467558]\n",
      " [0.03224039]\n",
      " [0.15533696]\n",
      " [0.01298631]\n",
      " [0.30477155]]\n"
     ]
    }
   ],
   "source": [
    "telementary = 0\n",
    "out = (nt.forward_prop(X_train.T[0].T))\n",
    "# telementary = 1\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 0\n",
    "\n",
    "# oldWeights = nt.output_layer.weights\n",
    "\n",
    "Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "predictions = nt.backward_prop(input_values=X_train, trueOutput=Y_train_oneHot)\n",
    "\n",
    "# newWeights = nt.output_layer.weights\n",
    "\n",
    "# diffWeight = newWeights - oldWeights\n",
    "\n",
    "# print(diffWeight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache for runNum = 3\n",
      "activation = [[4.148097   5.31068116 3.91016594 ... 7.01096724 3.31331218 2.74635754]\n",
      " [0.         3.71358264 0.         ... 0.71529534 0.         0.        ]\n",
      " [0.52350337 3.49519173 2.49628156 ... 0.78039136 2.9539097  1.17148021]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.5065627  0.         0.         ... 0.08634991 3.19830613 0.        ]\n",
      " [0.         0.         0.         ... 0.61077675 0.         0.        ]] & cached\n",
      "activation = [[0.         0.63046516 0.         ... 0.05953695 0.         0.05099984]\n",
      " [0.8685252  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.82549148 3.3300388  1.72552994 ... 1.28513567 0.         1.55265463]\n",
      " ...\n",
      " [1.06970333 0.         0.23988245 ... 1.21651719 0.         1.03610634]\n",
      " [3.55115121 0.0795321  1.64125851 ... 2.12586682 1.70555439 1.98496764]\n",
      " [0.         2.65752103 0.84141753 ... 0.54583987 0.         1.26527999]] & cached\n",
      "activation = [[0.04218909 0.46786154 0.23370729 ... 0.10778952 0.16257052 0.16607845]\n",
      " [0.0851106  0.00565861 0.03575724 ... 0.04904476 0.07656891 0.04832625]\n",
      " [0.21341003 0.02264567 0.10469289 ... 0.17865651 0.15849746 0.1435122 ]\n",
      " ...\n",
      " [0.05010548 0.07211867 0.09363219 ... 0.10148485 0.04806358 0.0754992 ]\n",
      " [0.10449547 0.01577    0.06937897 ... 0.16770374 0.16779906 0.08267091]\n",
      " [0.0868777  0.20657135 0.181941   ... 0.13990963 0.05975526 0.24260027]] & cached\n",
      "Re-used Cached Value, runNum =  3\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  3\n",
      "Provided input from cache for runNum = 3\n",
      "iterations = 0\n",
      "Accuracy = 0.07368292682926829\n",
      "Provided input from cache for runNum = 4\n",
      "activation = [[4.13852454 5.29528853 3.90831079 ... 6.99754622 3.29327718 2.7372993 ]\n",
      " [0.         3.59490505 0.         ... 0.60488992 0.         0.        ]\n",
      " [0.4932337  3.44984334 2.47123846 ... 0.75524358 2.93741967 1.14893334]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.51818945 0.         0.         ... 0.12560542 3.23430374 0.        ]\n",
      " [0.         0.         0.         ... 0.62524868 0.         0.        ]] & cached\n",
      "activation = [[0.         0.54207117 0.         ... 0.         0.         0.02939554]\n",
      " [0.84827462 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72249498 3.13217064 1.62310536 ... 1.11030477 0.         1.48054346]\n",
      " ...\n",
      " [1.08138117 0.         0.25120155 ... 1.22287234 0.         1.02553623]\n",
      " [3.48512803 0.05783735 1.58237188 ... 2.11622511 1.63449888 1.93239733]\n",
      " [0.         2.39702155 0.69473407 ... 0.31267776 0.         1.16452943]] & cached\n",
      "activation = [[0.04187888 0.43253672 0.21678786 ... 0.09108367 0.16196537 0.15663938]\n",
      " [0.09193201 0.00742463 0.0402669  ... 0.05568667 0.07777244 0.05246704]\n",
      " [0.21355729 0.02733273 0.10867768 ... 0.18326131 0.15589867 0.14568588]\n",
      " ...\n",
      " [0.04943884 0.07937588 0.09387944 ... 0.09710683 0.04930905 0.07602805]\n",
      " [0.11310087 0.02011142 0.07602209 ... 0.18304342 0.16897645 0.08819779]\n",
      " [0.08221973 0.21192165 0.17545002 ... 0.12631741 0.06144439 0.23202463]] & cached\n",
      "Re-used Cached Value, runNum =  4\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  4\n",
      "Provided input from cache for runNum = 4\n",
      "Provided input from cache for runNum = 5\n",
      "activation = [[4.13057974 5.28181926 3.90842441 ... 6.98501631 3.27484008 2.72967305]\n",
      " [0.         3.49778456 0.         ... 0.51476404 0.         0.        ]\n",
      " [0.47224416 3.41933411 2.45277098 ... 0.74340408 2.9317756  1.13409007]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.52283803 0.         0.         ... 0.15301984 3.26044619 0.        ]\n",
      " [0.         0.         0.         ... 0.64028529 0.         0.        ]] & cached\n",
      "activation = [[0.         0.46886002 0.         ... 0.         0.         0.01107608]\n",
      " [0.82723197 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.64330109 2.9755992  1.54202942 ... 0.9751252  0.         1.42321378]\n",
      " ...\n",
      " [1.08255155 0.         0.25285811 ... 1.21153641 0.         1.00978194]\n",
      " [3.41703879 0.02505627 1.52391559 ... 2.08878417 1.56007218 1.88119403]\n",
      " [0.         2.17822548 0.56863287 ... 0.11962201 0.         1.07918819]] & cached\n",
      "activation = [[0.04217758 0.40255922 0.20396348 ... 0.08124321 0.16148511 0.14995537]\n",
      " [0.09796917 0.00911697 0.04402943 ... 0.06226158 0.0787984  0.05587934]\n",
      " [0.21189934 0.03135859 0.11065225 ... 0.18091539 0.15321173 0.14617518]\n",
      " ...\n",
      " [0.04961871 0.08514115 0.09409516 ... 0.09576056 0.05061248 0.0765534 ]\n",
      " [0.1201127  0.02412882 0.08096623 ... 0.19192424 0.16988242 0.09221314]\n",
      " [0.07958347 0.21602131 0.17034421 ... 0.11789084 0.06334067 0.22389478]] & cached\n",
      "Re-used Cached Value, runNum =  5\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  5\n",
      "Provided input from cache for runNum = 5\n",
      "Provided input from cache for runNum = 6\n",
      "activation = [[4.12427256 5.27024378 3.91016665 ... 6.97372913 3.2581747  2.72336491]\n",
      " [0.         3.41544432 0.         ... 0.4381528  0.         0.        ]\n",
      " [0.45708771 3.39840765 2.43851823 ... 0.73995027 2.93285035 1.12420101]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.52323601 0.         0.         ... 0.17277962 3.28031121 0.        ]\n",
      " [0.         0.01382873 0.         ... 0.65563388 0.         0.        ]] & cached\n",
      "activation = [[0.         0.40712435 0.         ... 0.         0.         0.        ]\n",
      " [0.80662346 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.58024523 2.84605375 1.47573516 ... 0.86620985 0.         1.37607073]\n",
      " ...\n",
      " [1.07771708 0.         0.24867162 ... 1.18964244 0.         0.99136769]\n",
      " [3.35101424 0.         1.46846337 ... 2.05310113 1.48691168 1.83294239]\n",
      " [0.         1.98544128 0.45696151 ... 0.         0.         1.00459041]] & cached\n",
      "activation = [[0.04277812 0.37747702 0.19382292 ... 0.07585804 0.16104636 0.14513738]\n",
      " [0.10335909 0.01075774 0.04722905 ... 0.06751722 0.07964778 0.05886149]\n",
      " [0.20929273 0.03502497 0.11146762 ... 0.17660816 0.15060471 0.14557743]\n",
      " ...\n",
      " [0.05025235 0.08967201 0.09428871 ... 0.09543943 0.05188965 0.07716706]\n",
      " [0.12584914 0.02789239 0.08470112 ... 0.19790984 0.17055117 0.09511776]\n",
      " [0.07806697 0.21854858 0.16618458 ... 0.11219014 0.06528683 0.21751806]] & cached\n",
      "Re-used Cached Value, runNum =  6\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  6\n",
      "Provided input from cache for runNum = 6\n",
      "Provided input from cache for runNum = 7\n",
      "activation = [[4.11951773 5.26059966 3.91329025 ... 6.96400576 3.24338343 2.71830932]\n",
      " [0.         3.3444884  0.         ... 0.37212218 0.         0.        ]\n",
      " [0.44586378 3.38410556 2.42725221 ... 0.74204411 2.93828299 1.11778641]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.52097871 0.         0.         ... 0.18754509 3.29602114 0.        ]\n",
      " [0.         0.06012304 0.         ... 0.67101579 0.         0.        ]] & cached\n",
      "activation = [[0.         0.35692884 0.         ... 0.         0.         0.        ]\n",
      " [0.79202535 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.52755412 2.73386502 1.41918044 ... 0.77618532 0.         1.33635554]\n",
      " ...\n",
      " [1.066285   0.         0.23789257 ... 1.16118705 0.         0.97160862]\n",
      " [3.28919168 0.         1.4172881  ... 2.01403495 1.41697462 1.788165  ]\n",
      " [0.         1.80749061 0.36574178 ... 0.         0.         0.93796539]] & cached\n",
      "activation = [[0.04322223 0.35721937 0.18570346 ... 0.07525132 0.16061194 0.14173664]\n",
      " [0.10849352 0.01242984 0.04993194 ... 0.07102216 0.08033977 0.06165306]\n",
      " [0.20531276 0.03869088 0.1109464  ... 0.17136278 0.14814594 0.14412727]\n",
      " ...\n",
      " [0.05161117 0.09310934 0.09510109 ... 0.09636008 0.05310484 0.07796788]\n",
      " [0.13021695 0.03154095 0.08725178 ... 0.20284523 0.17103148 0.09714191]\n",
      " [0.07766583 0.21890085 0.16319438 ... 0.10879218 0.06720709 0.21252624]] & cached\n",
      "Re-used Cached Value, runNum =  7\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  7\n",
      "Provided input from cache for runNum = 7\n",
      "Provided input from cache for runNum = 8\n",
      "activation = [[4.11620336 5.25281783 3.91758381 ... 6.9558863  3.23036541 2.71441853]\n",
      " [0.         3.28191097 0.         ... 0.31381075 0.         0.        ]\n",
      " [0.43689153 3.37366873 2.41778955 ... 0.74703421 2.9459681  1.1134438 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.51683513 0.         0.         ... 0.19895566 3.30909209 0.        ]\n",
      " [0.         0.10591871 0.         ... 0.68636325 0.         0.        ]] & cached\n",
      "activation = [[0.         0.31195089 0.         ... 0.         0.         0.        ]\n",
      " [0.77816916 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.48348474 2.63801129 1.37090165 ... 0.70003798 0.         1.30237686]\n",
      " ...\n",
      " [1.05384023 0.         0.22570039 ... 1.12995295 0.         0.95186858]\n",
      " [3.23232551 0.         1.3707092  ... 1.97522441 1.35190194 1.74732974]\n",
      " [0.         1.64563961 0.28237704 ... 0.         0.         0.87753378]] & cached\n",
      "activation = [[0.04370632 0.33926892 0.17884698 ... 0.07510395 0.16017431 0.13898901]\n",
      " [0.11320149 0.01403257 0.05232082 ... 0.07415619 0.08089882 0.06412935]\n",
      " [0.20121832 0.04207505 0.11010862 ... 0.16611473 0.14588471 0.14255049]\n",
      " ...\n",
      " [0.05308421 0.09580503 0.09585461 ... 0.09752648 0.05422914 0.07868227]\n",
      " [0.13375439 0.03494033 0.08917804 ... 0.20620978 0.1713638  0.09864305]\n",
      " [0.0776315  0.21872245 0.16069373 ... 0.10624966 0.06903711 0.20828985]] & cached\n",
      "Re-used Cached Value, runNum =  8\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  8\n",
      "Provided input from cache for runNum = 8\n",
      "Provided input from cache for runNum = 9\n",
      "activation = [[4.11411421 5.24661997 3.92282019 ... 6.94922094 3.21891435 2.71148941]\n",
      " [0.         3.22606186 0.         ... 0.26166404 0.         0.        ]\n",
      " [0.42979282 3.36651718 2.40989092 ... 0.75457813 2.9556568  1.11088137]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.51135443 0.         0.         ... 0.20795526 3.32019416 0.        ]\n",
      " [0.         0.1512793  0.         ... 0.70164432 0.         0.        ]] & cached\n",
      "activation = [[0.         0.27075939 0.         ... 0.         0.         0.        ]\n",
      " [0.76498566 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.4459116  2.55471302 1.32885554 ... 0.63413159 0.         1.27285021]\n",
      " ...\n",
      " [1.04101915 0.         0.21270701 ... 1.0970176  0.         0.93250513]\n",
      " [3.18048542 0.         1.32869536 ... 1.93802501 1.2916912  1.71038634]\n",
      " [0.         1.49781511 0.20594975 ... 0.         0.         0.82272982]] & cached\n",
      "activation = [[0.04420045 0.32323314 0.1730103  ... 0.07528552 0.15973142 0.13674596]\n",
      " [0.1175454  0.01555698 0.05445957 ... 0.07700296 0.0813491  0.06635016]\n",
      " [0.19717042 0.04520838 0.1091288  ... 0.16107035 0.14381676 0.14096445]\n",
      " ...\n",
      " [0.054585   0.0978908  0.09651124 ... 0.0987833  0.05526276 0.07930112]\n",
      " [0.13662733 0.03810671 0.09065746 ... 0.20843996 0.17158713 0.0997592 ]\n",
      " [0.07779013 0.21813417 0.15850061 ... 0.10424025 0.07076401 0.20459894]] & cached\n",
      "Re-used Cached Value, runNum =  9\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  9\n",
      "Provided input from cache for runNum = 9\n",
      "Provided input from cache for runNum = 10\n",
      "activation = [[4.11298866 5.24152899 3.92882373 ... 6.94367932 3.2087065  2.70927559]\n",
      " [0.         3.17678681 0.         ... 0.2156187  0.         0.        ]\n",
      " [0.42407151 3.36195278 2.40316746 ... 0.76397935 2.96673692 1.1097175 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.50504056 0.         0.         ... 0.21534239 3.32996191 0.        ]\n",
      " [0.         0.19654054 0.         ... 0.71720712 0.         0.        ]] & cached\n",
      "activation = [[0.         0.23389408 0.         ... 0.         0.         0.        ]\n",
      " [0.75243902 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.4125019  2.48091215 1.29109177 ... 0.57556977 0.         1.2464321 ]\n",
      " ...\n",
      " [1.02835173 0.         0.19947043 ... 1.06336782 0.         0.91371672]\n",
      " [3.13318351 0.         1.29064578 ... 1.9019939  1.23583022 1.67673288]\n",
      " [0.         1.36253117 0.13562813 ... 0.         0.         0.77275892]] & cached\n",
      "activation = [[0.04465998 0.30887909 0.16794867 ... 0.07568813 0.1592824  0.13487276]\n",
      " [0.12161441 0.01700759 0.0564108  ... 0.07962018 0.0817105  0.06837249]\n",
      " [0.19323955 0.04809335 0.1081035  ... 0.15632009 0.14192121 0.13943029]\n",
      " ...\n",
      " [0.05607588 0.09950182 0.09707118 ... 0.10002261 0.05621523 0.07982932]\n",
      " [0.13902617 0.04105231 0.09183983 ... 0.20992216 0.17173212 0.10061476]\n",
      " [0.07803908 0.21721969 0.15651751 ... 0.10257342 0.07239378 0.20130351]] & cached\n",
      "Re-used Cached Value, runNum =  10\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  10\n",
      "Provided input from cache for runNum = 10\n",
      "Provided input from cache for runNum = 11\n",
      "activation = [[4.11256125 5.23720517 3.93534136 ... 6.93896775 3.19941931 2.70761307]\n",
      " [0.         3.13241302 0.         ... 0.17412228 0.         0.        ]\n",
      " [0.41927756 3.35920514 2.39730143 ... 0.77448941 2.97863198 1.10951688]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.49775898 0.         0.         ... 0.22101407 3.33837784 0.        ]\n",
      " [0.         0.24207381 0.         ... 0.73333951 0.         0.        ]] & cached\n",
      "activation = [[0.         0.20015676 0.         ... 0.         0.         0.        ]\n",
      " [0.74077926 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.38328479 2.41557474 1.25743132 ... 0.52371999 0.         1.22296685]\n",
      " ...\n",
      " [1.01619379 0.         0.18633161 ... 1.02973106 0.         0.89572824]\n",
      " [3.09035488 0.         1.25646059 ... 1.86775288 1.18432751 1.6462797 ]\n",
      " [0.         1.23783479 0.07086991 ... 0.         0.         0.72714157]] & cached\n",
      "activation = [[0.04505623 0.29591251 0.16351605 ... 0.07623672 0.15883315 0.13328285]\n",
      " [0.12544671 0.01837626 0.05819721 ... 0.0820434  0.08200056 0.07022286]\n",
      " [0.18941874 0.05075262 0.10706202 ... 0.15185046 0.14019245 0.13796338]\n",
      " ...\n",
      " [0.05757494 0.10071815 0.09755737 ... 0.10126474 0.05708728 0.08028392]\n",
      " [0.14096971 0.04377888 0.0927604  ... 0.21073644 0.1718186  0.1012487 ]\n",
      " [0.07839553 0.2160924  0.15474284 ... 0.10122079 0.07391828 0.19836249]] & cached\n",
      "Re-used Cached Value, runNum =  11\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  11\n",
      "Provided input from cache for runNum = 11\n",
      "Provided input from cache for runNum = 12\n",
      "activation = [[4.11283622 5.23372907 3.94235196 ... 6.93520059 3.19107041 2.70650618]\n",
      " [0.         3.0923059  0.         ... 0.13650213 0.         0.        ]\n",
      " [0.4150426  3.35763297 2.39198653 ... 0.78559443 2.99097757 1.10997596]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.49008327 0.         0.         ... 0.22579046 3.34611711 0.        ]\n",
      " [0.         0.28746612 0.         ... 0.74960214 0.         0.        ]] & cached\n",
      "activation = [[0.         0.16899337 0.         ... 0.         0.         0.        ]\n",
      " [0.72995822 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.35707618 2.35710969 1.22694192 ... 0.4768509  0.         1.20180932]\n",
      " ...\n",
      " [1.00509419 0.         0.1738552  ... 0.99707442 0.         0.87884568]\n",
      " [3.05203945 0.         1.22618099 ... 1.83630744 1.13723762 1.61896945]\n",
      " [0.         1.1225023  0.01111668 ... 0.         0.         0.68537108]] & cached\n",
      "activation = [[0.04536701 0.28413902 0.15956309 ... 0.07685439 0.15839113 0.13189319]\n",
      " [0.12910807 0.01966549 0.05986338 ... 0.08433483 0.08223501 0.07194136]\n",
      " [0.18576111 0.05320963 0.10606229 ... 0.147737   0.13862687 0.13659942]\n",
      " ...\n",
      " [0.05904202 0.10160997 0.09796347 ... 0.10242374 0.05787861 0.08066377]\n",
      " [0.14256073 0.04630682 0.09350105 ... 0.21112577 0.17186211 0.10172794]\n",
      " [0.07879262 0.21480366 0.15312705 ... 0.10005503 0.07532844 0.19569535]] & cached\n",
      "Re-used Cached Value, runNum =  12\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  12\n",
      "Provided input from cache for runNum = 12\n",
      "Provided input from cache for runNum = 13\n",
      "activation = [[4.11368846 5.23084395 3.94973113 ... 6.93210792 3.18354335 2.70580087]\n",
      " [0.         3.05583338 0.         ... 0.10215326 0.         0.        ]\n",
      " [0.41117705 3.35697444 2.38713129 ... 0.79711772 3.00353348 1.1109908 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.48205678 0.         0.         ... 0.22955281 3.353217   0.        ]\n",
      " [0.         0.33256159 0.         ... 0.76588822 0.         0.        ]] & cached\n",
      "activation = [[0.         0.13968629 0.         ... 0.         0.         0.        ]\n",
      " [0.7201106  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.33375879 2.30482067 1.19932504 ... 0.43447679 0.         1.18277169]\n",
      " ...\n",
      " [0.9952996  0.         0.16219858 ... 0.96561079 0.         0.8631329 ]\n",
      " [3.01808323 0.         1.19962149 ... 1.80764653 1.09455588 1.59464679]\n",
      " [0.         1.01577801 0.         ... 0.         0.         0.64724456]] & cached\n",
      "activation = [[0.04558476 0.27341581 0.15860928 ... 0.07751764 0.15796367 0.1306732 ]\n",
      " [0.13262903 0.02086955 0.06096011 ... 0.0865159  0.08242762 0.07354721]\n",
      " [0.18228334 0.05548083 0.10491682 ... 0.14395669 0.13721935 0.13534332]\n",
      " ...\n",
      " [0.060466   0.10222647 0.09854579 ... 0.10349491 0.05859    0.08097325]\n",
      " [0.14383445 0.04864248 0.09421327 ... 0.21116195 0.17187433 0.10207947]\n",
      " [0.07922011 0.21340577 0.15187421 ... 0.0990473  0.07661816 0.1932649 ]] & cached\n",
      "Re-used Cached Value, runNum =  13\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  13\n",
      "Provided input from cache for runNum = 13\n",
      "Provided input from cache for runNum = 14\n",
      "activation = [[4.11490153 5.22829345 3.95737468 ... 6.92947283 3.17655735 2.70536382]\n",
      " [0.         3.02265712 0.         ... 0.07081345 0.         0.        ]\n",
      " [0.40761338 3.35707646 2.38257045 ... 0.80897238 3.01631235 1.11246951]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.47405572 0.         0.         ... 0.23298075 3.36011688 0.        ]\n",
      " [0.         0.37764579 0.         ... 0.7824309  0.         0.        ]] & cached\n",
      "activation = [[0.         0.11248825 0.         ... 0.         0.         0.        ]\n",
      " [0.71121878 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.31266722 2.25795407 1.1742927  ... 0.39590175 0.         1.16559463]\n",
      " ...\n",
      " [0.9867847  0.         0.15149384 ... 0.93553785 0.         0.84854959]\n",
      " [2.9878726  0.         1.17622823 ... 1.78120756 1.05558558 1.57280921]\n",
      " [0.         0.91676088 0.         ... 0.         0.         0.61235851]] & cached\n",
      "activation = [[0.04571289 0.26363435 0.15833325 ... 0.07820287 0.15755032 0.12958798]\n",
      " [0.13605645 0.02199259 0.06189074 ... 0.08860415 0.08258676 0.0750568 ]\n",
      " [0.17897828 0.05756883 0.10381581 ... 0.14048576 0.13594484 0.13418377]\n",
      " ...\n",
      " [0.06183644 0.10262883 0.0991345  ... 0.10446426 0.05923416 0.0812244 ]\n",
      " [0.14484437 0.05078989 0.09483758 ... 0.21094286 0.17186552 0.10232876]\n",
      " [0.07965902 0.21194371 0.15079509 ... 0.09817115 0.07780504 0.19104813]] & cached\n",
      "Re-used Cached Value, runNum =  14\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  14\n",
      "Provided input from cache for runNum = 14\n",
      "Provided input from cache for runNum = 15\n",
      "activation = [[4.11645982 5.2260489  3.96527905 ... 6.92736318 3.17010542 2.70518563]\n",
      " [0.         2.99279146 0.         ... 0.04245873 0.         0.        ]\n",
      " [0.40419876 3.35766144 2.37816433 ... 0.82078697 3.02895434 1.11426088]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.46596087 0.         0.         ... 0.23594017 3.36679476 0.        ]\n",
      " [0.         0.42280178 0.         ... 0.79922758 0.         0.        ]] & cached\n",
      "activation = [[0.         0.08735904 0.         ... 0.         0.         0.        ]\n",
      " [0.70313124 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.29342338 2.21565745 1.15123548 ... 0.36053102 0.         1.14992077]\n",
      " ...\n",
      " [0.97943586 0.         0.1416188  ... 0.90672441 0.         0.83501756]\n",
      " [2.9612296  0.         1.15590751 ... 1.75701998 1.02025028 1.55339862]\n",
      " [0.         0.82450512 0.         ... 0.         0.         0.58023898]] & cached\n",
      "activation = [[0.04575691 0.25467841 0.15807318 ... 0.07890647 0.15715584 0.12860929]\n",
      " [0.13941513 0.02304224 0.06279059 ... 0.09061384 0.08272165 0.07649242]\n",
      " [0.17583297 0.05948758 0.10281695 ... 0.13730244 0.13479752 0.13312552]\n",
      " ...\n",
      " [0.06315825 0.10286262 0.09966512 ... 0.10532591 0.05981282 0.08141813]\n",
      " [0.14563718 0.05276473 0.095381   ... 0.21054295 0.1718423  0.10250265]\n",
      " [0.08009336 0.2104452  0.1497793  ... 0.09737995 0.07888717 0.18898933]] & cached\n",
      "Re-used Cached Value, runNum =  15\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  15\n",
      "Provided input from cache for runNum = 15\n",
      "Provided input from cache for runNum = 16\n",
      "activation = [[4.11831069 5.22403578 3.97340687 ... 6.92568523 3.16414545 2.70522026]\n",
      " [0.         2.96564291 0.         ... 0.01657008 0.         0.        ]\n",
      " [0.40085598 3.35854602 2.37383083 ... 0.83252782 3.04147713 1.11625962]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.45770782 0.         0.         ... 0.23853282 3.3732679  0.        ]\n",
      " [0.         0.46840341 0.         ... 0.81663414 0.         0.        ]] & cached\n",
      "activation = [[0.         0.06360589 0.         ... 0.         0.         0.        ]\n",
      " [0.69596195 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.27617505 2.1777152  1.13021467 ... 0.3281468  0.         1.13567544]\n",
      " ...\n",
      " [0.97372581 0.         0.13310466 ... 0.87994652 0.         0.82278867]\n",
      " [2.93774342 0.         1.13828403 ... 1.73459491 0.98811699 1.53604746]\n",
      " [0.         0.73786499 0.         ... 0.         0.         0.55052215]] & cached\n",
      "activation = [[0.04570112 0.2464076  0.15776355 ... 0.07956819 0.15678037 0.12768445]\n",
      " [0.14273897 0.02401786 0.0636712  ... 0.09257021 0.08283795 0.07786899]\n",
      " [0.17283277 0.06125905 0.10191278 ... 0.13437173 0.13376148 0.13215866]\n",
      " ...\n",
      " [0.06444329 0.10295094 0.10015174 ... 0.10610154 0.06033388 0.08156616]\n",
      " [0.14621909 0.05457536 0.09585023 ... 0.20999604 0.17181008 0.10261463]\n",
      " [0.08055906 0.2089452  0.14887846 ... 0.09671525 0.07987534 0.18710619]] & cached\n",
      "Re-used Cached Value, runNum =  16\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  16\n",
      "Provided input from cache for runNum = 16\n",
      "Provided input from cache for runNum = 17\n",
      "activation = [[4.1202624  5.22191468 3.98159091 ... 6.92410318 3.15839188 2.70530027]\n",
      " [0.         2.94101984 0.         ... 0.         0.         0.        ]\n",
      " [0.39736901 3.35949483 2.369452   ... 0.8439391  3.05360079 1.11833678]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.44904752 0.         0.         ... 0.24033437 3.37921378 0.        ]\n",
      " [0.         0.51403547 0.         ... 0.83422045 0.         0.        ]] & cached\n",
      "activation = [[0.         0.0412869  0.         ... 0.         0.         0.        ]\n",
      " [0.68963192 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.26100383 2.14398159 1.11114692 ... 0.3018873  0.         1.12281809]\n",
      " ...\n",
      " [0.96970608 0.         0.12596675 ... 0.85462075 0.         0.8118466 ]\n",
      " [2.91726905 0.         1.12318466 ... 1.71095366 0.95903586 1.52062624]\n",
      " [0.         0.65704114 0.         ... 0.         0.         0.52326627]] & cached\n",
      "activation = [[0.04555004 0.23881378 0.15740841 ... 0.08034268 0.15642644 0.12681877]\n",
      " [0.14603728 0.02491549 0.06453752 ... 0.094288   0.08294161 0.07919122]\n",
      " [0.16996992 0.06287848 0.10110229 ... 0.13168659 0.13282976 0.13127997]\n",
      " ...\n",
      " [0.06569775 0.10292587 0.10058937 ... 0.10675814 0.06080012 0.08167131]\n",
      " [0.14660611 0.05621718 0.09625558 ... 0.20917882 0.17177262 0.10267471]\n",
      " [0.08106026 0.20746597 0.14807816 ... 0.09626136 0.08077124 0.18538014]] & cached\n",
      "Re-used Cached Value, runNum =  17\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  17\n",
      "Provided input from cache for runNum = 17\n",
      "Provided input from cache for runNum = 18\n",
      "activation = [[4.12248469 5.22006267 3.98994423 ... 6.92294618 3.15311063 2.70555694]\n",
      " [0.         2.91854727 0.         ... 0.         0.         0.        ]\n",
      " [0.39362252 3.3602323  2.36489008 ... 0.85475743 3.06513116 1.12034923]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.440437   0.         0.         ... 0.24209893 3.38527887 0.        ]\n",
      " [0.         0.5595361  0.         ... 0.85178749 0.         0.        ]] & cached\n",
      "activation = [[0.         0.02044012 0.         ... 0.         0.         0.        ]\n",
      " [0.68414173 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.24692805 2.11338373 1.09339916 ... 0.28350503 0.         1.11092108]\n",
      " ...\n",
      " [0.96740856 0.         0.12024942 ... 0.830229   0.         0.80220137]\n",
      " [2.89986321 0.         1.11065309 ... 1.68335697 0.9332057  1.50716328]\n",
      " [0.         0.58081895 0.         ... 0.         0.         0.49798616]] & cached\n",
      "activation = [[0.04530206 0.23160048 0.15699488 ... 0.08136629 0.15609999 0.12598021]\n",
      " [0.1493642  0.02574802 0.06541059 ... 0.09559421 0.08304028 0.08048362]\n",
      " [0.1672568  0.06436634 0.10039739 ... 0.12928528 0.13200618 0.13049914]\n",
      " ...\n",
      " [0.06688875 0.10289812 0.10096975 ... 0.10720651 0.06120809 0.08172983]\n",
      " [0.14685878 0.05774656 0.09663694 ... 0.20812838 0.17173271 0.102711  ]\n",
      " [0.08154073 0.20608654 0.14733156 ... 0.0960209  0.08156489 0.1837589 ]] & cached\n",
      "Re-used Cached Value, runNum =  18\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  18\n",
      "Provided input from cache for runNum = 18\n",
      "Provided input from cache for runNum = 19\n",
      "activation = [[4.12482216 5.2181712  3.99839461 ... 6.92196379 3.14807611 2.70586929]\n",
      " [0.         2.89811151 0.         ... 0.         0.         0.        ]\n",
      " [0.3895458  3.36068207 2.36006426 ... 0.86487458 3.0759459  1.12223977]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.43188347 0.         0.         ... 0.24379565 3.39147304 0.        ]\n",
      " [0.         0.6048413  0.         ... 0.8693125  0.         0.        ]] & cached\n",
      "activation = [[0.00000000e+00 8.54194794e-04 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.79587760e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.33947246e-01 2.08572405e+00 1.07688991e+00 ... 2.66151110e-01\n",
      "  0.00000000e+00 1.09995470e+00]\n",
      " ...\n",
      " [9.66926472e-01 0.00000000e+00 1.16102558e-01 ... 8.08364929e-01\n",
      "  0.00000000e+00 7.93932482e-01]\n",
      " [2.88529666e+00 0.00000000e+00 1.10049743e+00 ... 1.65868939e+00\n",
      "  9.10408364e-01 1.49551525e+00]\n",
      " [0.00000000e+00 5.09221681e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 4.74678308e-01]] & cached\n",
      "activation = [[0.04495587 0.22173882 0.15651034 ... 0.08228192 0.15580169 0.12516278]\n",
      " [0.15273683 0.02640999 0.06629701 ... 0.09691899 0.08313802 0.08175178]\n",
      " [0.1646879  0.0656456  0.09979825 ... 0.127128   0.13128239 0.12981397]\n",
      " ...\n",
      " [0.06801528 0.10435852 0.1012895  ... 0.10753799 0.06156168 0.08174225]\n",
      " [0.14698782 0.05958684 0.09700509 ... 0.20713337 0.1716925  0.1027299 ]\n",
      " [0.08200687 0.206199   0.14664071 ... 0.09580031 0.08226113 0.18224049]] & cached\n",
      "Re-used Cached Value, runNum =  19\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  19\n",
      "Provided input from cache for runNum = 19\n",
      "Provided input from cache for runNum = 20\n",
      "activation = [[4.12725922 5.21623775 4.00697307 ... 6.92110131 3.14326088 2.70620729]\n",
      " [0.         2.87928724 0.         ... 0.         0.         0.        ]\n",
      " [0.38528128 3.36105499 2.35505441 ... 0.87466432 3.08631242 1.12413421]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.42318796 0.         0.         ... 0.24523796 3.39762561 0.        ]\n",
      " [0.         0.65054866 0.         ... 0.887247   0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67580066 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.22258824 2.06138775 1.06208403 ... 0.25052954 0.         1.0901673 ]\n",
      " ...\n",
      " [0.96779761 0.         0.11307174 ... 0.78822537 0.         0.78668265]\n",
      " [2.87321773 0.         1.09248574 ... 1.63623039 0.89019414 1.48542736]\n",
      " [0.         0.44145805 0.         ... 0.         0.         0.45311849]] & cached\n",
      "activation = [[0.04453303 0.21299127 0.15599267 ... 0.08313284 0.1555288  0.12438248]\n",
      " [0.15613739 0.02712142 0.06717954 ... 0.0982322  0.0832361  0.08298748]\n",
      " [0.16220445 0.06633727 0.09926183 ... 0.12512719 0.13064341 0.12918977]\n",
      " ...\n",
      " [0.06912888 0.10626691 0.10157927 ... 0.10782937 0.06186885 0.08172833]\n",
      " [0.14696508 0.06105266 0.09731851 ... 0.20607515 0.17165351 0.10270786]\n",
      " [0.08249683 0.20651704 0.14601903 ... 0.09564325 0.08287339 0.18083299]] & cached\n",
      "Re-used Cached Value, runNum =  20\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  20\n",
      "Provided input from cache for runNum = 20\n",
      "Provided input from cache for runNum = 21\n",
      "activation = [[4.12971694 5.21411825 4.01560287 ... 6.92018287 3.13851963 2.70649937]\n",
      " [0.         2.86229876 0.         ... 0.         0.         0.        ]\n",
      " [0.38075449 3.36127242 2.34979905 ... 0.88399902 3.09606932 1.1260027 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.41430579 0.         0.         ... 0.24624383 3.4036162  0.        ]\n",
      " [0.         0.69646838 0.         ... 0.90548429 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67279825 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.21294149 2.04050226 1.04902791 ... 0.23697069 0.         1.08158767]\n",
      " ...\n",
      " [0.96997238 0.         0.11112626 ... 0.76975526 0.         0.78039868]\n",
      " [2.86341407 0.         1.08644362 ... 1.61579034 0.87238514 1.47675063]\n",
      " [0.         0.37745337 0.         ... 0.         0.         0.43320437]] & cached\n",
      "activation = [[0.04404853 0.20475391 0.1554479  ... 0.08392908 0.15528095 0.12364008]\n",
      " [0.15957761 0.02775316 0.0680572  ... 0.09952513 0.08333709 0.08419156]\n",
      " [0.15980315 0.06687104 0.09878224 ... 0.12326371 0.1300829  0.12862053]\n",
      " ...\n",
      " [0.07022077 0.10816679 0.10183964 ... 0.10809288 0.0621326  0.08169084]\n",
      " [0.14678254 0.06235206 0.09757639 ... 0.2049297  0.17161714 0.10264574]\n",
      " [0.08300723 0.20681644 0.14546481 ... 0.09556085 0.08340594 0.17953026]] & cached\n",
      "Re-used Cached Value, runNum =  21\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  21\n",
      "Provided input from cache for runNum = 21\n",
      "Provided input from cache for runNum = 22\n",
      "activation = [[4.13232166 5.21204649 4.02436827 ... 6.91943329 3.13396917 2.70685761]\n",
      " [0.         2.8468616  0.         ... 0.         0.         0.        ]\n",
      " [0.37588638 3.36114948 2.34424487 ... 0.89275633 3.10519948 1.12773764]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.40552143 0.         0.         ... 0.24733812 3.40979833 0.        ]\n",
      " [0.         0.74241924 0.         ... 0.92380615 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67063521 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.20436042 2.02229276 1.03720961 ... 0.22441207 0.         1.07388465]\n",
      " ...\n",
      " [0.97375156 0.         0.11055259 ... 0.75336459 0.         0.77527886]\n",
      " [2.85600818 0.         1.08242775 ... 1.597756   0.85707811 1.46957178]\n",
      " [0.         0.3165253  0.         ... 0.         0.         0.41465415]] & cached\n",
      "activation = [[0.04349008 0.19690828 0.15484831 ... 0.08463886 0.15506081 0.12290417]\n",
      " [0.16309492 0.02831053 0.06895045 ... 0.10083921 0.08344551 0.08538472]\n",
      " [0.15751027 0.06728683 0.09838211 ... 0.12157919 0.12960281 0.12812474]\n",
      " ...\n",
      " [0.07126011 0.11005085 0.10205428 ... 0.10826669 0.06235101 0.08161965]\n",
      " [0.1464973  0.06352139 0.09782067 ... 0.2038385  0.1715846  0.10257031]\n",
      " [0.08350057 0.20707578 0.14494906 ... 0.09548654 0.08385349 0.17829882]] & cached\n",
      "Re-used Cached Value, runNum =  22\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  22\n",
      "Provided input from cache for runNum = 22\n",
      "Provided input from cache for runNum = 23\n",
      "activation = [[4.13505024 5.20996953 4.03327108 ... 6.91882881 3.12966227 2.7072426 ]\n",
      " [0.         2.83273697 0.         ... 0.         0.         0.        ]\n",
      " [0.37077113 3.36088391 2.33841232 ... 0.90114644 3.11382919 1.12942202]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.39670113 0.         0.         ... 0.2482922  3.41610284 0.        ]\n",
      " [0.         0.78865686 0.         ... 0.94242458 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.66907878 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19666564 2.00624439 1.02635747 ... 0.21272128 0.         1.06685364]\n",
      " ...\n",
      " [0.97895305 0.         0.11122965 ... 0.73884277 0.         0.77115636]\n",
      " [2.85061284 0.         1.08014057 ... 1.58150349 0.84394078 1.46359644]\n",
      " [0.         0.25834583 0.         ... 0.         0.         0.39734546]] & cached\n",
      "activation = [[0.04287089 0.18940909 0.15419415 ... 0.08526457 0.1548656  0.12217608]\n",
      " [0.16669554 0.02880059 0.06986528 ... 0.10217589 0.0835616  0.08657141]\n",
      " [0.1553031  0.06759684 0.0980526  ... 0.12003887 0.12919237 0.1276916 ]\n",
      " ...\n",
      " [0.07225731 0.11192636 0.10222472 ... 0.10836904 0.06252987 0.08151777]\n",
      " [0.14612398 0.06457555 0.0980578  ... 0.20278714 0.17155638 0.10248699]\n",
      " [0.08398048 0.2072833  0.14446695 ... 0.09543692 0.08422639 0.1771285 ]] & cached\n",
      "Re-used Cached Value, runNum =  23\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  23\n",
      "Provided input from cache for runNum = 23\n",
      "Provided input from cache for runNum = 24\n",
      "activation = [[4.13784761 5.20785369 4.04228888 ... 6.91835303 3.12551484 2.70765025]\n",
      " [0.         2.81986013 0.         ... 0.         0.         0.        ]\n",
      " [0.36523648 3.36014312 2.33221033 ... 0.90882993 3.12173511 1.13091185]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.38789572 0.         0.         ... 0.24920002 3.42251129 0.        ]\n",
      " [0.         0.83513525 0.         ... 0.96122972 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.66825457 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.18998339 1.99254363 1.01664376 ... 0.20209158 0.         1.06062358]\n",
      " ...\n",
      " [0.98572507 0.         0.11328852 ... 0.72639556 0.         0.76816564]\n",
      " [2.84739604 0.         1.07974909 ... 1.56745623 0.83306859 1.45900374]\n",
      " [0.         0.20291283 0.         ... 0.         0.         0.38129741]] & cached\n",
      "activation = [[0.04218471 0.18222761 0.15347767 ... 0.08579917 0.15469756 0.1214489 ]\n",
      " [0.17039735 0.02921789 0.0708053  ... 0.10354655 0.08368915 0.0877564 ]\n",
      " [0.15317373 0.06779741 0.09779266 ... 0.11864343 0.12885378 0.12732175]\n",
      " ...\n",
      " [0.07321799 0.11381362 0.10235383 ... 0.10840177 0.06266726 0.08138749]\n",
      " [0.14565492 0.06550639 0.09828181 ... 0.20176664 0.171533   0.1023917 ]\n",
      " [0.08445262 0.20745812 0.14402351 ... 0.09541015 0.08451989 0.17602183]] & cached\n",
      "Re-used Cached Value, runNum =  24\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  24\n",
      "Provided input from cache for runNum = 24\n",
      "Provided input from cache for runNum = 25\n",
      "activation = [[4.14073406 5.2056786  4.05145004 ... 6.91795901 3.12149971 2.70807845]\n",
      " [0.         2.80826941 0.         ... 0.         0.         0.        ]\n",
      " [0.35932829 3.35904014 2.32561819 ... 0.91585998 3.12891272 1.13225396]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.37912341 0.         0.         ... 0.24996673 3.42899895 0.        ]\n",
      " [0.         0.88203888 0.         ... 0.98037214 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.6681194  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.18422146 1.98100365 1.00795659 ... 0.19247935 0.         1.05511715]\n",
      " ...\n",
      " [0.99411921 0.         0.11678022 ... 0.71605501 0.         0.76630236]\n",
      " [2.84603041 0.         1.0809772  ... 1.55505441 0.82419666 1.45556075]\n",
      " [0.         0.14985371 0.         ... 0.         0.         0.36634048]] & cached\n",
      "activation = [[0.04144139 0.1753406  0.15269633 ... 0.0862408  0.15455429 0.12067459]\n",
      " [0.17420844 0.02956656 0.07177154 ... 0.10494365 0.0838282  0.08894231]\n",
      " [0.15112497 0.06789544 0.09760214 ... 0.11737815 0.12857854 0.1269294 ]\n",
      " ...\n",
      " [0.07413138 0.11571019 0.1024384  ... 0.10836812 0.0627677  0.08133768]\n",
      " [0.14510594 0.06631533 0.09850473 ... 0.20079332 0.1715146  0.10231239]\n",
      " [0.08491278 0.20759    0.14361574 ... 0.09541356 0.08474231 0.17499922]] & cached\n",
      "Re-used Cached Value, runNum =  25\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  25\n",
      "Provided input from cache for runNum = 25\n",
      "Provided input from cache for runNum = 26\n",
      "activation = [[4.14374828 5.20354588 4.0607584  ... 6.91773765 3.11763948 2.70856131]\n",
      " [0.         2.79777194 0.         ... 0.         0.         0.        ]\n",
      " [0.35310568 3.35757631 2.31867649 ... 0.92235501 3.13550907 1.13345434]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.37065072 0.         0.         ... 0.25103699 3.4358531  0.        ]\n",
      " [0.         0.9288343  0.         ... 0.99936606 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.66854412 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17922628 1.97161414 1.00032323 ... 0.1838552  0.         1.05033823]\n",
      " ...\n",
      " [1.00407094 0.         0.12161497 ... 0.70768901 0.         0.76546573]\n",
      " [2.84651835 0.         1.08382535 ... 1.54472246 0.81724524 1.45324526]\n",
      " [0.         0.09920392 0.         ... 0.         0.         0.35243318]] & cached\n",
      "activation = [[0.04066013 0.16880396 0.15186649 ... 0.08660936 0.15443551 0.1198961 ]\n",
      " [0.17814029 0.0298498  0.07276267 ... 0.10637131 0.08398021 0.09012929]\n",
      " [0.14916981 0.06789746 0.09748065 ... 0.11626027 0.12836375 0.12657252]\n",
      " ...\n",
      " [0.07496858 0.1175856  0.10247363 ... 0.10824159 0.06283241 0.08128768]\n",
      " [0.14448653 0.06699184 0.0987265  ... 0.19988225 0.17150125 0.10223505]\n",
      " [0.08533393 0.20765753 0.14322988 ... 0.09541402 0.08489551 0.17402607]] & cached\n",
      "Re-used Cached Value, runNum =  26\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  26\n",
      "Provided input from cache for runNum = 26\n",
      "Provided input from cache for runNum = 27\n",
      "activation = [[4.14684247 5.20136775 4.07021612 ... 6.9176482  3.11390742 2.70905289]\n",
      " [0.         2.78849653 0.         ... 0.         0.         0.        ]\n",
      " [0.3464959  3.35567841 2.31130289 ... 0.92820901 3.14146794 1.1344627 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.36200993 0.         0.         ... 0.25182411 3.44265054 0.        ]\n",
      " [0.         0.9757844  0.         ... 1.01846135 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.66952815 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17543824 1.96469128 0.99396935 ... 0.17672223 0.         1.04642718]\n",
      " ...\n",
      " [1.01544885 0.         0.12774181 ... 0.70114602 0.         0.76564602]\n",
      " [2.84876676 0.         1.08827778 ... 1.53613231 0.81212427 1.45202793]\n",
      " [0.         0.05089179 0.         ... 0.         0.         0.33954032]] & cached\n",
      "activation = [[0.03983929 0.16256578 0.15098374 ... 0.08690439 0.15434062 0.11912403]\n",
      " [0.18218882 0.03006065 0.07377544 ... 0.1078171  0.0841463  0.09131685]\n",
      " [0.14725548 0.06779701 0.09740605 ... 0.11523109 0.12820625 0.12626406]\n",
      " ...\n",
      " [0.0757865  0.11947106 0.10248485 ... 0.1080908  0.06286285 0.08121266]\n",
      " [0.14376864 0.06752692 0.09892936 ... 0.19895671 0.17149291 0.10214232]\n",
      " [0.08575729 0.20769199 0.14288087 ... 0.09545383 0.08498197 0.17310796]] & cached\n",
      "Re-used Cached Value, runNum =  27\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  27\n",
      "Provided input from cache for runNum = 27\n",
      "Provided input from cache for runNum = 28\n",
      "activation = [[4.15003393 5.19918305 4.07982127 ... 6.91761594 3.11029891 2.70955665]\n",
      " [0.         2.78017019 0.         ... 0.         0.         0.        ]\n",
      " [0.33971641 3.35372661 2.30362674 ... 0.9338418  3.14702108 1.13542197]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.35343    0.         0.         ... 0.25271102 3.44965068 0.        ]\n",
      " [0.         1.02250246 0.         ... 1.03732824 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67099997 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17227735 1.95937337 0.98837866 ... 0.17027033 0.         1.04303508]\n",
      " ...\n",
      " [1.02810248 0.         0.13502415 ... 0.69614963 0.         0.76666951]\n",
      " [2.85233946 0.         1.0939488  ... 1.52896146 0.80833923 1.45160771]\n",
      " [0.         0.00472474 0.         ... 0.         0.         0.3275392 ]] & cached\n",
      "activation = [[0.03899811 0.15667059 0.15005594 ... 0.08713692 0.15426374 0.11836   ]\n",
      " [0.18637753 0.03021881 0.07481689 ... 0.10929315 0.0843228  0.09250971]\n",
      " [0.1453995  0.06762045 0.09738409 ... 0.11430557 0.128091   0.12600068]\n",
      " ...\n",
      " [0.07653875 0.1213158  0.10245654 ... 0.10787232 0.06286775 0.08111147]\n",
      " [0.14297992 0.06794284 0.09913872 ... 0.19808781 0.17148921 0.10205088]\n",
      " [0.08614739 0.20763603 0.14254554 ... 0.09549179 0.08501902 0.17222207]] & cached\n",
      "Re-used Cached Value, runNum =  28\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  28\n",
      "Provided input from cache for runNum = 28\n",
      "Provided input from cache for runNum = 29\n",
      "activation = [[4.153397   5.19711998 4.08961891 ... 6.91784158 3.10694597 2.71014519]\n",
      " [0.         2.77267807 0.         ... 0.         0.         0.        ]\n",
      " [0.33252292 3.35128984 2.29550376 ... 0.93884017 3.15183267 1.13614735]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.34478872 0.         0.         ... 0.25345489 3.45669672 0.        ]\n",
      " [0.         1.06857912 0.         ... 1.05558221 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67291694 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17013547 1.95614466 0.98384553 ... 0.16510009 0.         1.04038779]\n",
      " ...\n",
      " [1.04238868 0.         0.14374529 ... 0.69315682 0.         0.76879339]\n",
      " [2.85747286 0.         1.10103605 ... 1.52374605 0.80623059 1.45220397]\n",
      " [0.         0.         0.         ... 0.         0.         0.31647812]] & cached\n",
      "activation = [[0.03813347 0.15326605 0.14906203 ... 0.08729041 0.15420951 0.11759252]\n",
      " [0.19071277 0.03011199 0.0758902  ... 0.11080596 0.08451496 0.09371137]\n",
      " [0.14360059 0.06723499 0.09741349 ... 0.1134818  0.12802782 0.12578865]\n",
      " ...\n",
      " [0.077232   0.1234685  0.10239687 ... 0.10760405 0.06284072 0.08098202]\n",
      " [0.14211087 0.06828196 0.09934604 ... 0.1972391  0.17149036 0.10195348]\n",
      " [0.08651854 0.2076621  0.1422424  ... 0.09554716 0.08499364 0.17138191]] & cached\n",
      "Re-used Cached Value, runNum =  29\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  29\n",
      "Provided input from cache for runNum = 29\n",
      "Provided input from cache for runNum = 30\n",
      "activation = [[4.15687361 5.19507251 4.09960339 ... 6.91819619 3.10372172 2.71077148]\n",
      " [0.         2.7658284  0.         ... 0.         0.         0.        ]\n",
      " [0.32512987 3.34866434 2.2870334  ... 0.94350945 3.15622335 1.13679104]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.33652962 0.         0.         ... 0.25462218 3.46424214 0.        ]\n",
      " [0.         1.11435414 0.         ... 1.0735197  0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.67540981 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.16844827 1.95439879 0.98004725 ... 0.16046677 0.         1.03823859]\n",
      " ...\n",
      " [1.05768062 0.         0.15334609 ... 0.69142198 0.         0.77159842]\n",
      " [2.86400075 0.         1.10938357 ... 1.52008493 0.80561292 1.45361566]\n",
      " [0.         0.         0.         ... 0.         0.         0.30611501]] & cached\n",
      "activation = [[0.03725671 0.15021849 0.14804695 ... 0.08739784 0.15417592 0.1168366 ]\n",
      " [0.19520668 0.02995104 0.0769928  ... 0.11235515 0.08472209 0.09492211]\n",
      " [0.14184967 0.06679299 0.09748311 ... 0.11274975 0.12801088 0.12561446]\n",
      " ...\n",
      " [0.07785315 0.12567395 0.10230398 ... 0.10727006 0.06278493 0.08082855]\n",
      " [0.14117505 0.06853371 0.09955247 ... 0.19644782 0.17149604 0.10185286]\n",
      " [0.08683706 0.20767161 0.14193271 ... 0.09557762 0.08491211 0.17055757]] & cached\n",
      "Re-used Cached Value, runNum =  30\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  30\n",
      "Provided input from cache for runNum = 30\n",
      "Provided input from cache for runNum = 31\n",
      "activation = [[4.16055118 5.19320956 4.10981342 ... 6.91881389 3.10076417 2.71150692]\n",
      " [0.         2.75983376 0.         ... 0.         0.         0.        ]\n",
      " [0.31743743 3.345676   2.27816636 ... 0.94771966 3.16006101 1.13727083]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.3285874  0.         0.         ... 0.25608372 3.47225094 0.        ]\n",
      " [0.         1.1598715  0.         ... 1.09117831 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.6783017  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.16736393 1.95436594 0.97708853 ... 0.15658781 0.         1.03662807]\n",
      " ...\n",
      " [1.0744536  0.         0.16425522 ... 0.69150166 0.         0.77532113]\n",
      " [2.87188893 0.         1.11896356 ... 1.51800834 0.8064315  1.4558251 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.29631954]] & cached\n",
      "activation = [[0.03636869 0.14726687 0.14699152 ... 0.08744383 0.15416237 0.11607833]\n",
      " [0.19985049 0.02975899 0.07812579 ... 0.11393523 0.08494484 0.09614237]\n",
      " [0.14017323 0.06630815 0.09761358 ... 0.11213488 0.12803835 0.12549449]\n",
      " ...\n",
      " [0.07838637 0.12789509 0.10216128 ... 0.10684645 0.06270121 0.08064048]\n",
      " [0.14020411 0.06868247 0.09977689 ... 0.19575069 0.17150604 0.10176104]\n",
      " [0.0871033  0.20765936 0.14162189 ... 0.09558422 0.08477609 0.16974434]] & cached\n",
      "Re-used Cached Value, runNum =  31\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  31\n",
      "Provided input from cache for runNum = 31\n",
      "Provided input from cache for runNum = 32\n",
      "activation = [[4.16431879 5.19137804 4.12021903 ... 6.91955188 3.09795683 2.7122702 ]\n",
      " [0.         2.75447388 0.         ... 0.         0.         0.        ]\n",
      " [0.30960025 3.34249177 2.26897461 ... 0.95161707 3.16343344 1.13767551]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.32071969 0.         0.         ... 0.25755978 3.4804898  0.        ]\n",
      " [0.         1.20509527 0.         ... 1.10855616 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.68154069 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.16710129 1.95614464 0.97508527 ... 0.15375573 0.         1.0356244 ]\n",
      " ...\n",
      " [1.09241764 0.         0.17622525 ... 0.69297177 0.         0.77979638]\n",
      " [2.88079649 0.         1.12952359 ... 1.51700297 0.80840708 1.45864593]\n",
      " [0.         0.         0.         ... 0.         0.         0.28711272]] & cached\n",
      "activation = [[0.03547767 0.14441528 0.14590377 ... 0.08744222 0.15416558 0.11532373]\n",
      " [0.20464261 0.02953586 0.07928301 ... 0.11553301 0.08518091 0.09736781]\n",
      " [0.1385284  0.06578041 0.09777793 ... 0.11158471 0.12810159 0.12540617]\n",
      " ...\n",
      " [0.07886405 0.13012957 0.10198998 ... 0.10638744 0.0625946  0.0804318 ]\n",
      " [0.13916339 0.06872383 0.09999428 ... 0.19506555 0.17151971 0.10166262]\n",
      " [0.08734781 0.20762866 0.14132984 ... 0.0956059  0.08459566 0.16896177]] & cached\n",
      "Re-used Cached Value, runNum =  32\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  32\n",
      "Provided input from cache for runNum = 32\n",
      "Provided input from cache for runNum = 33\n",
      "activation = [[4.16817599 5.189567   4.13079323 ... 6.92035333 3.09526262 2.71306244]\n",
      " [0.         2.7497952  0.         ... 0.         0.         0.        ]\n",
      " [0.30141242 3.33884661 2.25933571 ... 0.95491779 3.1662146  1.13786405]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.31299942 0.         0.         ... 0.25908573 3.48904381 0.        ]\n",
      " [0.         1.25042188 0.         ... 1.12598256 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.68536633 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.1677068  1.95983315 0.97414322 ... 0.152087   0.         1.03530737]\n",
      " ...\n",
      " [1.11174236 0.         0.18945786 ... 0.69611287 0.         0.78515842]\n",
      " [2.89090609 0.         1.14119125 ... 1.51715618 0.81165854 1.46219137]\n",
      " [0.         0.         0.         ... 0.         0.         0.27856296]] & cached\n",
      "activation = [[0.03457289 0.14162758 0.14477044 ... 0.08738007 0.15418719 0.11457091]\n",
      " [0.20959953 0.02927818 0.08046804 ... 0.11715021 0.08543269 0.0985995 ]\n",
      " [0.13691339 0.06520439 0.09797831 ... 0.1110994  0.12820398 0.12535638]\n",
      " ...\n",
      " [0.07928999 0.13240107 0.10179351 ... 0.10589706 0.06246273 0.08019932]\n",
      " [0.13805564 0.06865688 0.10020778 ... 0.19440298 0.17153703 0.10155868]\n",
      " [0.0875701  0.20759087 0.14105921 ... 0.09564495 0.08436611 0.16820911]] & cached\n",
      "Re-used Cached Value, runNum =  33\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  33\n",
      "Provided input from cache for runNum = 33\n",
      "Provided input from cache for runNum = 34\n",
      "activation = [[4.17217073 5.1878163  4.14158824 ... 6.92131496 3.09272511 2.71391565]\n",
      " [0.         2.74588561 0.         ... 0.         0.         0.        ]\n",
      " [0.29301771 3.33492644 2.24935851 ... 0.9578828  3.16852016 1.13793728]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.30550568 0.         0.         ... 0.26090837 3.49802519 0.        ]\n",
      " [0.         1.2955318  0.         ... 1.14311673 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.68961134 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.16889893 1.96225066 0.97401654 ... 0.15119182 0.         1.03553655]\n",
      " ...\n",
      " [1.13225435 0.         0.20375706 ... 0.70067614 0.         0.79127933]\n",
      " [2.9020591  0.         1.15383066 ... 1.51847959 0.81607502 1.46637118]\n",
      " [0.         0.         0.         ... 0.         0.         0.27052503]] & cached\n",
      "activation = [[0.03366531 0.13844906 0.14360891 ... 0.08727154 0.15422602 0.11382112]\n",
      " [0.21471922 0.02899183 0.08168169 ... 0.11879221 0.08569968 0.09983941]\n",
      " [0.13533294 0.06457471 0.09821604 ... 0.11068827 0.12834202 0.12534291]\n",
      " ...\n",
      " [0.07965049 0.13495089 0.10156366 ... 0.10535301 0.06230743 0.07994227]\n",
      " [0.13690216 0.06862566 0.10042668 ... 0.19379229 0.17155766 0.10145388]\n",
      " [0.08775239 0.20749211 0.14078931 ... 0.09567237 0.0840913  0.16747158]] & cached\n",
      "Re-used Cached Value, runNum =  34\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  34\n",
      "Provided input from cache for runNum = 34\n",
      "Provided input from cache for runNum = 35\n",
      "activation = [[4.17632871 5.18625573 4.15260195 ... 6.92252563 3.090402   2.71488908]\n",
      " [0.         2.74250654 0.         ... 0.         0.         0.        ]\n",
      " [0.28455958 3.33089715 2.23914662 ... 0.96062276 3.17055697 1.13796181]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.29823883 0.         0.         ... 0.2630308  3.50746009 0.        ]\n",
      " [0.         1.34068669 0.         ... 1.16023792 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.69416439 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17057764 1.9645421  0.97459894 ... 0.15089786 0.         1.03621126]\n",
      " ...\n",
      " [1.15346958 0.         0.21864919 ... 0.70606988 0.         0.79785812]\n",
      " [2.91411251 0.         1.16732364 ... 1.5207069  0.82144829 1.47109144]\n",
      " [0.         0.         0.         ... 0.         0.         0.26287242]] & cached\n",
      "activation = [[0.03276713 0.13511675 0.14244991 ... 0.08713946 0.15427969 0.11308506]\n",
      " [0.21999732 0.02867868 0.08291872 ... 0.12045246 0.08597989 0.10108658]\n",
      " [0.13375954 0.06389695 0.09847103 ... 0.11031965 0.12850929 0.12535139]\n",
      " ...\n",
      " [0.0799636  0.1376506  0.10131221 ... 0.10478088 0.06213249 0.07966759]\n",
      " [0.13569279 0.06856426 0.10063716 ... 0.19320503 0.17158106 0.10134221]\n",
      " [0.08789286 0.20731316 0.14050584 ... 0.09568459 0.08377881 0.16673835]] & cached\n",
      "Re-used Cached Value, runNum =  35\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  35\n",
      "Provided input from cache for runNum = 35\n",
      "Provided input from cache for runNum = 36\n",
      "activation = [[4.18057089 5.18472615 4.16379233 ... 6.92380356 3.08821018 2.71589465]\n",
      " [0.         2.73990352 0.         ... 0.         0.         0.        ]\n",
      " [0.27602964 3.32673131 2.22866529 ... 0.96319012 3.17224922 1.13793927]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.29130346 0.         0.         ... 0.26542158 3.51734061 0.        ]\n",
      " [0.         1.38584508 0.         ... 1.1773689  0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.6991187  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17273573 1.96833141 0.97594292 ... 0.15131361 0.         1.03736943]\n",
      " ...\n",
      " [1.17572204 0.         0.23449372 ... 0.71265026 0.         0.80507597]\n",
      " [2.9269844  0.         1.18160697 ... 1.52366964 0.82773573 1.47628497]\n",
      " [0.         0.         0.         ... 0.         0.         0.25569843]] & cached\n",
      "activation = [[0.03187353 0.13184911 0.14127266 ... 0.08697276 0.15434774 0.1123583 ]\n",
      " [0.22543321 0.02833519 0.08418126 ... 0.12212387 0.08627329 0.10233767]\n",
      " [0.13221813 0.06317179 0.09875769 ... 0.11000814 0.12870442 0.12538979]\n",
      " ...\n",
      " [0.08020721 0.14038503 0.10102821 ... 0.10416584 0.06193854 0.07937165]\n",
      " [0.13444913 0.06839773 0.10085294 ... 0.19266269 0.17160704 0.10123101]\n",
      " [0.08798832 0.20707652 0.14021919 ... 0.09568994 0.08343017 0.16601565]] & cached\n",
      "Re-used Cached Value, runNum =  36\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  36\n",
      "Provided input from cache for runNum = 36\n",
      "Provided input from cache for runNum = 37\n",
      "activation = [[4.18491694 5.18322544 4.17518135 ... 6.92517243 3.08610565 2.71693221]\n",
      " [0.         2.73795065 0.         ... 0.         0.         0.        ]\n",
      " [0.26749917 3.32262184 2.21797785 ... 0.96568653 3.17376465 1.13794328]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.2847046  0.         0.         ... 0.2681635  3.52773289 0.        ]\n",
      " [0.         1.43086398 0.         ... 1.19438526 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.70428823 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.1753443  1.97356554 0.97803622 ... 0.15244194 0.         1.03900396]\n",
      " ...\n",
      " [1.19854397 0.         0.25083995 ... 0.71983658 0.02765924 0.81264805]\n",
      " [2.94047746 0.         1.19650442 ... 1.52723549 0.8346837  1.48182683]\n",
      " [0.         0.         0.         ... 0.         0.         0.24890593]] & cached\n",
      "activation = [[0.03100278 0.12868229 0.14010872 ... 0.08680284 0.15197214 0.11165398]\n",
      " [0.23100937 0.02796702 0.08545849 ... 0.1237895  0.08700179 0.10358609]\n",
      " [0.13069455 0.06240758 0.09905542 ... 0.10973299 0.13003832 0.12544292]\n",
      " ...\n",
      " [0.08038657 0.14312561 0.1007245  ... 0.1035253  0.06125352 0.07906277]\n",
      " [0.13316525 0.06812734 0.10105828 ... 0.19213657 0.17290433 0.10111178]\n",
      " [0.08803291 0.20677506 0.13992072 ... 0.09568014 0.08354405 0.16529931]] & cached\n",
      "Re-used Cached Value, runNum =  37\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  37\n",
      "Provided input from cache for runNum = 37\n",
      "Provided input from cache for runNum = 38\n",
      "activation = [[4.18928898 5.18165644 4.18671408 ... 6.9265611  3.08402579 2.71794942]\n",
      " [0.         2.73658613 0.         ... 0.         0.         0.        ]\n",
      " [0.25899305 3.31860074 2.20710579 ... 0.96817079 3.17515526 1.13798948]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.27837402 0.         0.         ... 0.2710811  3.53853583 0.        ]\n",
      " [0.         1.47567096 0.         ... 1.21121242 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.70978102 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.17850711 1.98024142 0.98089477 ... 0.15434394 0.         1.04108373]\n",
      " ...\n",
      " [1.22169269 0.         0.26740912 ... 0.72722105 0.05942123 0.82036371]\n",
      " [2.95456338 0.         1.21201792 ... 1.53140209 0.84224073 1.48766535]\n",
      " [0.         0.         0.         ... 0.         0.         0.24249269]] & cached\n",
      "activation = [[0.03015607 0.12559723 0.13898002 ... 0.086651   0.1492754  0.11098579]\n",
      " [0.23671103 0.02757366 0.08674373 ... 0.12543795 0.08780036 0.10482596]\n",
      " [0.12917842 0.06160322 0.09935391 ... 0.10947651 0.13155651 0.12550358]\n",
      " ...\n",
      " [0.08051571 0.14588188 0.10040801 ... 0.10287714 0.06048453 0.07874535]\n",
      " [0.13183316 0.06775552 0.10124492 ... 0.19160037 0.17437848 0.10098217]\n",
      " [0.08803052 0.20641188 0.13959826 ... 0.0956502  0.08369225 0.16458009]] & cached\n",
      "Re-used Cached Value, runNum =  38\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  38\n",
      "Provided input from cache for runNum = 38\n",
      "Provided input from cache for runNum = 39\n",
      "activation = [[4.19371447 5.18011796 4.1984083  ... 6.9280334  3.08206101 2.71898735]\n",
      " [0.         2.7357733  0.         ... 0.         0.         0.        ]\n",
      " [0.25052291 3.31460192 2.19608814 ... 0.97061956 3.17640565 1.13805217]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.27218629 0.         0.         ... 0.27413891 3.54967402 0.        ]\n",
      " [0.         1.5202004  0.         ... 1.22776379 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.71549391 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.18204803 1.98800671 0.98428139 ... 0.15676782 0.         1.04354053]\n",
      " ...\n",
      " [1.24551287 0.         0.2845874  ... 0.73539258 0.09229392 0.82854816]\n",
      " [2.96913702 0.         1.22806606 ... 1.53612609 0.85034713 1.49384476]\n",
      " [0.         0.         0.         ... 0.         0.         0.23653782]] & cached\n",
      "activation = [[0.02932438 0.12258789 0.13784834 ... 0.08647949 0.14652689 0.1103318 ]\n",
      " [0.24255283 0.02716083 0.08805251 ... 0.12709194 0.08862175 0.10606719]\n",
      " [0.12767096 0.06076604 0.09966642 ... 0.10925536 0.13313667 0.12558283]\n",
      " ...\n",
      " [0.08059255 0.14864751 0.10006953 ... 0.10220281 0.05968469 0.07841357]\n",
      " [0.13047227 0.06729741 0.10143313 ... 0.1910957  0.17588971 0.10085081]\n",
      " [0.08798859 0.20597482 0.13926441 ... 0.0956078  0.08382183 0.16386919]] & cached\n",
      "Re-used Cached Value, runNum =  39\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  39\n",
      "Provided input from cache for runNum = 39\n",
      "Provided input from cache for runNum = 40\n",
      "activation = [[4.19813922 5.17850408 4.21021867 ... 6.92944756 3.08007299 2.72000015]\n",
      " [0.         2.73538793 0.         ... 0.         0.         0.        ]\n",
      " [0.24217176 3.31078239 2.1849981  ... 0.9732161  3.17765503 1.1381793 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.26621327 0.         0.         ... 0.27736728 3.56117083 0.        ]\n",
      " [0.         1.56422464 0.         ... 1.24394322 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72148736 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.18615964 1.99718726 0.98844241 ... 0.16003593 0.         1.04648886]\n",
      " ...\n",
      " [1.26941026 0.         0.30172137 ... 0.74338468 0.12563301 0.83677612]\n",
      " [2.98415408 0.         1.24459469 ... 1.54133968 0.85890198 1.5002979 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.2309975 ]] & cached\n",
      "activation = [[0.02852021 0.11966205 0.13676815 ... 0.08634641 0.14378342 0.10972066]\n",
      " [0.24850431 0.02672606 0.08936127 ... 0.12871317 0.08945441 0.1072944 ]\n",
      " [0.12615448 0.05989339 0.09996448 ... 0.10903543 0.1347501  0.12566166]\n",
      " ...\n",
      " [0.08063567 0.15141991 0.09972876 ... 0.10153762 0.05886754 0.07807907]\n",
      " [0.12905993 0.06674319 0.1015916  ... 0.19055449 0.17740694 0.10070263]\n",
      " [0.08790704 0.20547734 0.13890531 ... 0.09554556 0.08392427 0.16315843]] & cached\n",
      "Re-used Cached Value, runNum =  40\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  40\n",
      "Provided input from cache for runNum = 40\n",
      "Provided input from cache for runNum = 41\n",
      "activation = [[4.20256968 5.17681249 4.22215586 ... 6.93085941 3.07810774 2.72100175]\n",
      " [0.         2.7355526  0.         ... 0.         0.         0.        ]\n",
      " [0.2338545  3.30696453 2.17374241 ... 0.97572038 3.17876782 1.13828454]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.26028439 0.         0.         ... 0.2806112  3.57288822 0.        ]\n",
      " [0.         1.60807537 0.         ... 1.25998364 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72772533 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19091152 2.0078582  0.99335281 ... 0.16421406 0.         1.04993769]\n",
      " ...\n",
      " [1.29371331 0.         0.31926221 ... 0.75181698 0.15976336 0.8453224 ]\n",
      " [2.99959929 0.         1.2616268  ... 1.54696622 0.86798423 1.50706407]\n",
      " [0.         0.         0.         ... 0.         0.         0.22590065]] & cached\n",
      "activation = [[0.02773703 0.11681317 0.13570391 ... 0.08621677 0.14101951 0.1091348 ]\n",
      " [0.25457089 0.02626914 0.09068101 ... 0.13031358 0.09030395 0.10851272]\n",
      " [0.12463424 0.05898489 0.10026338 ... 0.10882652 0.13641295 0.12575015]\n",
      " ...\n",
      " [0.08064067 0.15420133 0.09937565 ... 0.1008752  0.05802652 0.07773659]\n",
      " [0.12760083 0.0660932  0.10173413 ... 0.18999268 0.17894334 0.10054358]\n",
      " [0.08779671 0.20492401 0.13853739 ... 0.09548627 0.08400089 0.16245747]] & cached\n",
      "Re-used Cached Value, runNum =  41\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  41\n",
      "Provided input from cache for runNum = 41\n",
      "Provided input from cache for runNum = 42\n",
      "activation = [[4.20709141 5.17516536 4.23426135 ... 6.93238512 3.07628938 2.72204505]\n",
      " [0.         2.73631178 0.         ... 0.         0.         0.        ]\n",
      " [0.22565176 3.30335249 2.16239515 ... 0.97835164 3.17981686 1.13848505]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.25452585 0.         0.         ... 0.28384684 3.58488425 0.        ]\n",
      " [0.         1.65183301 0.         ... 1.27592792 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73420679 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19620027 2.01991165 0.99899731 ... 0.16918368 0.         1.0537868 ]\n",
      " ...\n",
      " [1.31819168 0.         0.33690589 ... 0.76025372 0.19442967 0.85391785]\n",
      " [3.01537267 0.         1.27902844 ... 1.55275001 0.87743537 1.5139583 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.22100519]] & cached\n",
      "activation = [[0.02697663 0.1140284  0.13467641 ... 0.08610909 0.13825671 0.10857911]\n",
      " [0.26074788 0.02579155 0.09200183 ... 0.13187847 0.09116366 0.10971648]\n",
      " [0.12309686 0.05804207 0.10054952 ... 0.10860952 0.13811049 0.12583791]\n",
      " ...\n",
      " [0.08061884 0.15699516 0.09901944 ... 0.10023112 0.05716951 0.0773902 ]\n",
      " [0.12609501 0.06535453 0.10185281 ... 0.189399   0.18048574 0.10037256]\n",
      " [0.08765758 0.20431152 0.13815405 ... 0.09542651 0.08405235 0.16175681]] & cached\n",
      "Re-used Cached Value, runNum =  42\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  42\n",
      "Provided input from cache for runNum = 42\n",
      "Provided input from cache for runNum = 43\n",
      "activation = [[4.21172009 5.1736815  4.24653024 ... 6.93410596 3.07464234 2.72316358]\n",
      " [0.         2.73740691 0.         ... 0.         0.         0.        ]\n",
      " [0.217401   3.29966318 2.15088899 ... 0.98085827 3.18062232 1.13865651]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.24874041 0.         0.         ... 0.28688193 3.59690839 0.        ]\n",
      " [0.         1.69529421 0.         ... 1.29168681 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74078281 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.20207631 2.03315806 1.00526871 ... 0.17504733 0.         1.05801847]\n",
      " ...\n",
      " [1.34323853 0.         0.35506606 ... 0.7693532  0.23006971 0.86286318]\n",
      " [3.03151891 0.         1.29683624 ... 1.55886759 0.88732802 1.52106931]\n",
      " [0.         0.         0.         ... 0.         0.         0.21633754]] & cached\n",
      "activation = [[0.02623724 0.11132848 0.13365822 ... 0.08599483 0.13546147 0.10803793]\n",
      " [0.26702793 0.02529975 0.09333281 ... 0.1334159  0.09204028 0.11091042]\n",
      " [0.1215626  0.05707482 0.10084369 ... 0.10840825 0.13986322 0.12594255]\n",
      " ...\n",
      " [0.08055523 0.15977847 0.09864505 ... 0.09958594 0.05628818 0.07702991]\n",
      " [0.12456622 0.06453851 0.1019715  ... 0.18880851 0.18205163 0.10020126]\n",
      " [0.08748827 0.20363629 0.13775678 ... 0.09537029 0.08408163 0.16105598]] & cached\n",
      "Re-used Cached Value, runNum =  43\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  43\n",
      "Provided input from cache for runNum = 43\n",
      "Provided input from cache for runNum = 44\n",
      "activation = [[4.2164471  5.17226038 4.25897403 ... 6.93593666 3.0731379  2.72431629]\n",
      " [0.         2.73892122 0.         ... 0.         0.         0.        ]\n",
      " [0.20928509 3.29618299 2.13938467 ... 0.98357094 3.18148956 1.13895894]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.24312021 0.         0.         ... 0.2900061  3.60925239 0.        ]\n",
      " [0.         1.7386738  0.         ... 1.30746793 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74733948 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.20845796 2.04767533 1.01220736 ... 0.1817133  0.         1.0626413 ]\n",
      " ...\n",
      " [1.36856642 0.         0.3733818  ... 0.77861291 0.26627634 0.87189406]\n",
      " [3.0478792  0.         1.31487038 ... 1.56497817 0.89743872 1.52823893]\n",
      " [0.         0.         0.         ... 0.         0.         0.2119395 ]] & cached\n",
      "activation = [[0.02552426 0.10869172 0.13267716 ... 0.08590162 0.13266721 0.10752952]\n",
      " [0.27337461 0.02479154 0.09465858 ... 0.13490039 0.09292349 0.11208151]\n",
      " [0.12003008 0.05607943 0.10113258 ... 0.10821263 0.14164799 0.12605042]\n",
      " ...\n",
      " [0.08045972 0.1625649  0.0982594  ... 0.09894491 0.05539421 0.07666482]\n",
      " [0.12302556 0.06364556 0.10207736 ... 0.18821075 0.18362086 0.10002391]\n",
      " [0.0872873  0.20290185 0.13734145 ... 0.09531322 0.08408975 0.16035482]] & cached\n",
      "Re-used Cached Value, runNum =  44\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  44\n",
      "Provided input from cache for runNum = 44\n",
      "Provided input from cache for runNum = 45\n",
      "activation = [[4.22113273 5.17075813 4.27150629 ... 6.93773211 3.07164033 2.72543199]\n",
      " [0.         2.7408325  0.         ... 0.         0.         0.        ]\n",
      " [0.20134625 3.29298226 2.1278709  ... 0.98645405 3.18246527 1.139364  ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.23759236 0.         0.         ... 0.29313076 3.62179658 0.        ]\n",
      " [0.0206509  1.78166219 0.         ... 1.32293722 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74807826 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.21437383 2.06291938 1.01948451 ... 0.18878764 0.         1.06752125]\n",
      " ...\n",
      " [1.3936506  0.         0.39153964 ... 0.7876514  0.30271343 0.88090108]\n",
      " [3.05390564 0.         1.33318666 ... 1.57135554 0.90768813 1.53556305]\n",
      " [0.         0.         0.         ... 0.         0.         0.20788057]] & cached\n",
      "activation = [[0.02482587 0.10612158 0.13174531 ... 0.08583957 0.12990133 0.10705798]\n",
      " [0.27883122 0.02427566 0.09598683 ... 0.13635248 0.09380637 0.11323607]\n",
      " [0.11816786 0.05506823 0.10141    ... 0.10801758 0.14344911 0.12615752]\n",
      " ...\n",
      " [0.08114212 0.16533724 0.09786405 ... 0.09830613 0.05449537 0.07629722]\n",
      " [0.1219132  0.0626993  0.10217193 ... 0.18760395 0.18517715 0.09983946]\n",
      " [0.08761853 0.20209271 0.13688743 ... 0.09523196 0.08407393 0.15964502]] & cached\n",
      "Re-used Cached Value, runNum =  45\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  45\n",
      "Provided input from cache for runNum = 45\n",
      "Provided input from cache for runNum = 46\n",
      "activation = [[4.22583197 5.16926999 4.28414091 ... 6.93959092 3.07021174 2.72656514]\n",
      " [0.         2.74320248 0.         ... 0.         0.         0.        ]\n",
      " [0.19361617 3.29003077 2.11639937 ... 0.98951868 3.18351405 1.13988323]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.23232642 0.         0.         ... 0.29642069 3.63469055 0.        ]\n",
      " [0.04638469 1.8242484  0.         ... 1.33807174 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74752187 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.22043651 2.07908634 1.02714408 ... 0.19622191 0.         1.07263614]\n",
      " ...\n",
      " [1.41849778 0.00390078 0.409474   ... 0.79641204 0.33932619 0.88981695]\n",
      " [3.05750191 0.         1.35175118 ... 1.57802778 0.91818368 1.54300756]\n",
      " [0.         0.         0.         ... 0.         0.         0.20408777]] & cached\n",
      "activation = [[0.02415777 0.10342482 0.13087699 ... 0.08582051 0.12716954 0.1066294 ]\n",
      " [0.28403284 0.02377481 0.09730456 ... 0.13775061 0.09468898 0.11436523]\n",
      " [0.11624475 0.05412192 0.10168103 ... 0.10783737 0.14526829 0.12627052]\n",
      " ...\n",
      " [0.08199598 0.16793781 0.09745511 ... 0.09765591 0.05359122 0.07592312]\n",
      " [0.12088746 0.06177519 0.10226358 ... 0.18702473 0.18671672 0.0996553 ]\n",
      " [0.0880446  0.20143247 0.13638203 ... 0.09510472 0.08402874 0.15891308]] & cached\n",
      "Re-used Cached Value, runNum =  46\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  46\n",
      "Provided input from cache for runNum = 46\n",
      "Provided input from cache for runNum = 47\n",
      "activation = [[4.23061342 5.16787839 4.29692964 ... 6.94165272 3.06896572 2.72775446]\n",
      " [0.         2.74580999 0.         ... 0.         0.         0.        ]\n",
      " [0.18602609 3.28731368 2.10494151 ... 0.99267658 3.18462525 1.14049574]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.22698664 0.         0.         ... 0.29956721 3.64761253 0.        ]\n",
      " [0.07201273 1.86656755 0.         ... 1.35299808 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74686917 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.22735736 2.09651288 1.03554312 ... 0.20467172 0.         1.07818999]\n",
      " ...\n",
      " [1.4431324  0.02449442 0.42721693 ... 0.80491167 0.37607306 0.89865898]\n",
      " [3.06129146 0.         1.37056561 ... 1.58491782 0.92885979 1.55057091]\n",
      " [0.         0.         0.         ... 0.         0.         0.20046683]] & cached\n",
      "activation = [[0.02352304 0.09994413 0.13006169 ... 0.08584101 0.12447446 0.10623406]\n",
      " [0.28918683 0.02335435 0.09860898 ... 0.13909273 0.09556876 0.11546897]\n",
      " [0.11430832 0.0534836  0.10192664 ... 0.10763786 0.14710215 0.12637367]\n",
      " ...\n",
      " [0.08286777 0.16987738 0.0970495  ... 0.09703774 0.05268425 0.0755523 ]\n",
      " [0.11979786 0.06110948 0.102318   ... 0.18637506 0.18823622 0.0994517 ]\n",
      " [0.08846575 0.20161356 0.13586329 ... 0.09498564 0.08395594 0.1581862 ]] & cached\n",
      "Re-used Cached Value, runNum =  47\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  47\n",
      "Provided input from cache for runNum = 47\n",
      "Provided input from cache for runNum = 48\n",
      "activation = [[4.23537975 5.16643644 4.3098079  ... 6.94374684 3.06777392 2.72892636]\n",
      " [0.         2.74885035 0.         ... 0.         0.         0.        ]\n",
      " [0.17868088 3.28489194 2.09356733 ... 0.99600033 3.18584262 1.14125357]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.22177044 0.         0.         ... 0.30284443 3.66078443 0.        ]\n",
      " [0.09747685 1.90849032 0.         ... 1.36760783 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74625002 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.23479464 2.11502492 1.04447418 ... 0.21371649 0.         1.08404135]\n",
      " ...\n",
      " [1.4673889  0.04463246 0.44455184 ... 0.81295327 0.41272229 0.90726956]\n",
      " [3.06523812 0.         1.38958845 ... 1.59205948 0.93971746 1.55818507]\n",
      " [0.         0.         0.         ... 0.         0.         0.1970512 ]] & cached\n",
      "activation = [[0.02292274 0.09661117 0.12932434 ... 0.08591883 0.12183354 0.10588742]\n",
      " [0.29427199 0.02291767 0.09988863 ... 0.14036084 0.0964416  0.11653638]\n",
      " [0.11238663 0.05280664 0.10215886 ... 0.10744813 0.14894197 0.12647472]\n",
      " ...\n",
      " [0.0837318  0.17178596 0.09663429 ... 0.09641625 0.05177884 0.07518053]\n",
      " [0.11867923 0.06036652 0.10235592 ... 0.18572557 0.18972498 0.09924289]\n",
      " [0.08885437 0.20170299 0.13529801 ... 0.09483022 0.08385199 0.15744101]] & cached\n",
      "Re-used Cached Value, runNum =  48\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  48\n",
      "Provided input from cache for runNum = 48\n",
      "Provided input from cache for runNum = 49\n",
      "activation = [[4.2401437  5.16499045 4.32278214 ... 6.94590911 3.06666954 2.73009287]\n",
      " [0.         2.75215665 0.         ... 0.         0.         0.        ]\n",
      " [0.17159692 3.28277817 2.08225611 ... 0.99954971 3.18718659 1.14217012]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.21671921 0.         0.         ... 0.30620229 3.67421209 0.        ]\n",
      " [0.12283433 1.95007428 0.         ... 1.38200867 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74569544 0.         0.         ... 0.         0.01344274 0.        ]\n",
      " [0.24271945 2.13457925 1.05394002 ... 0.22341358 0.         1.09019865]\n",
      " ...\n",
      " [1.4912207  0.06424656 0.46143111 ... 0.82037471 0.44926526 0.91559724]\n",
      " [3.06922738 0.         1.40870498 ... 1.59923184 0.95064788 1.56576973]\n",
      " [0.         0.         0.         ... 0.         0.         0.19379858]] & cached\n",
      "activation = [[0.02235333 0.09342222 0.12865687 ... 0.08605379 0.11849497 0.10558592]\n",
      " [0.29928737 0.02246617 0.10114252 ... 0.14155077 0.09775376 0.11756609]\n",
      " [0.11046743 0.05209261 0.10236272 ... 0.107242   0.15102252 0.12656374]\n",
      " ...\n",
      " [0.08460021 0.17366639 0.09622385 ... 0.0958187  0.05074332 0.07481373]\n",
      " [0.11752617 0.05955099 0.10236833 ... 0.18504531 0.19058554 0.09902257]\n",
      " [0.08922198 0.20170056 0.13470213 ... 0.0946583  0.08385265 0.15668951]] & cached\n",
      "Re-used Cached Value, runNum =  49\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  49\n",
      "Provided input from cache for runNum = 49\n",
      "Provided input from cache for runNum = 50\n",
      "activation = [[4.24489401e+00 5.16350701e+00 4.33580851e+00 ... 6.94813369e+00\n",
      "  3.06558235e+00 2.73125861e+00]\n",
      " [0.00000000e+00 2.75585778e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.64882015e-01 3.28115683e+00 2.07114854e+00 ... 1.00355539e+00\n",
      "  3.18889261e+00 1.14332266e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.21184135e+00 0.00000000e+00 4.96447948e-03 ... 3.09592793e-01\n",
      "  3.68785716e+00 0.00000000e+00]\n",
      " [1.48126712e-01 1.99129577e+00 0.00000000e+00 ... 1.39618493e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7451594  0.         0.         ... 0.         0.02853775 0.        ]\n",
      " [0.25064164 2.15449655 1.06107155 ... 0.23309083 0.         1.09637888]\n",
      " ...\n",
      " [1.51468074 0.08337716 0.47870146 ... 0.82726205 0.48569452 0.9236957 ]\n",
      " [3.07312113 0.         1.42871852 ... 1.60631837 0.96143256 1.57330432]\n",
      " [0.         0.         0.         ... 0.         0.         0.19074261]] & cached\n",
      "activation = [[0.02180851 0.09007238 0.12801518 ... 0.08622646 0.11514631 0.1053194 ]\n",
      " [0.3042505  0.02206088 0.10247138 ... 0.14269032 0.09910949 0.11856909]\n",
      " [0.1085529  0.05114239 0.10273208 ... 0.10703161 0.1531245  0.12664581]\n",
      " ...\n",
      " [0.08546391 0.17601144 0.09563763 ... 0.09522314 0.04970139 0.0744479 ]\n",
      " [0.11635958 0.05842711 0.10257671 ... 0.18437858 0.19131827 0.09880283]\n",
      " [0.08956215 0.20223945 0.13388042 ... 0.09445884 0.08384463 0.15592537]] & cached\n",
      "Re-used Cached Value, runNum =  50\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  50\n",
      "Provided input from cache for runNum = 50\n",
      "Provided input from cache for runNum = 51\n",
      "activation = [[4.2496594  5.16205961 4.34892097 ... 6.95050865 3.06460404 2.73246093]\n",
      " [0.         2.75989473 0.         ... 0.         0.         0.        ]\n",
      " [0.15847467 3.27992653 2.06019024 ... 1.0077835  3.19078714 1.14466062]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.2071927  0.         0.01632316 ... 0.31312893 3.701789   0.        ]\n",
      " [0.17330026 2.03209846 0.         ... 1.41004121 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74466211 0.         0.         ... 0.         0.04364036 0.        ]\n",
      " [0.25849066 2.17471197 1.06514253 ... 0.24262918 0.         1.10256541]\n",
      " ...\n",
      " [1.53764331 0.1018448  0.49656167 ... 0.83356667 0.52182563 0.93150736]\n",
      " [3.07701013 0.         1.44996137 ... 1.61356254 0.97225127 1.58085137]\n",
      " [0.         0.         0.         ... 0.         0.         0.18779766]] & cached\n",
      "activation = [[0.02128997 0.08658295 0.12739554 ... 0.08644363 0.11189434 0.10509081]\n",
      " [0.30914155 0.02170227 0.10389552 ... 0.14377476 0.10045339 0.11954201]\n",
      " [0.10665358 0.04995521 0.10332744 ... 0.10682804 0.15521753 0.12672659]\n",
      " ...\n",
      " [0.08631908 0.17885895 0.09482602 ... 0.09461898 0.04867018 0.07407941]\n",
      " [0.11519538 0.05699926 0.1030491  ... 0.18375657 0.19198727 0.09858734]\n",
      " [0.08986215 0.20332933 0.13276742 ... 0.09420663 0.08380223 0.15513604]] & cached\n",
      "Re-used Cached Value, runNum =  51\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  51\n",
      "Provided input from cache for runNum = 51\n",
      "Provided input from cache for runNum = 52\n",
      "activation = [[4.25430459 5.16045049 4.36203284 ... 6.95277035 3.06354662 2.73358953]\n",
      " [0.         2.76413965 0.         ... 0.         0.         0.        ]\n",
      " [0.1523051  3.27900041 2.04932859 ... 1.01228076 3.19285119 1.1461526 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.20250171 0.         0.02770553 ... 0.31638392 3.71569544 0.        ]\n",
      " [0.19850889 2.07278982 0.         ... 1.42389831 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74426196 0.         0.         ... 0.         0.05887883 0.        ]\n",
      " [0.26693493 2.19594191 1.06970527 ... 0.25294016 0.         1.10905596]\n",
      " ...\n",
      " [1.5601377  0.11974151 0.51400983 ... 0.83921124 0.55761504 0.93902759]\n",
      " [3.08068724 0.         1.47116724 ... 1.62043697 0.98296111 1.58831966]\n",
      " [0.         0.         0.         ... 0.         0.         0.18508898]] & cached\n",
      "activation = [[0.02079698 0.08326572 0.12683044 ... 0.08671276 0.10873192 0.10490569]\n",
      " [0.3139176  0.02132627 0.10529003 ... 0.14476661 0.10178519 0.12047135]\n",
      " [0.10475128 0.04875522 0.10389556 ... 0.1065862  0.15729729 0.12678958]\n",
      " ...\n",
      " [0.08719951 0.18168033 0.09402389 ... 0.0940628  0.04765198 0.07372084]\n",
      " [0.11399476 0.05553877 0.10349499 ... 0.18307153 0.19258485 0.09835784]\n",
      " [0.09016943 0.20429908 0.13164522 ... 0.09397305 0.0837325  0.15435332]] & cached\n",
      "Re-used Cached Value, runNum =  52\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  52\n",
      "Provided input from cache for runNum = 52\n",
      "Provided input from cache for runNum = 53\n",
      "activation = [[4.25897745 5.15892024 4.37522629 ... 6.95519008 3.06262595 2.73476267]\n",
      " [0.         2.76857617 0.         ... 0.         0.         0.        ]\n",
      " [0.14635387 3.27836125 2.03856774 ... 1.01700147 3.19506824 1.14778043]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.19781267 0.         0.03911129 ... 0.31960261 3.72966165 0.        ]\n",
      " [0.22380853 2.11337447 0.         ... 1.43779469 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74377422 0.         0.         ... 0.         0.07403157 0.        ]\n",
      " [0.27570015 2.21781244 1.0733205  ... 0.26360171 0.         1.1157082 ]\n",
      " ...\n",
      " [1.58234437 0.13730525 0.53113917 ... 0.84446417 0.59330273 0.94634658]\n",
      " [3.08421976 0.         1.49300163 ... 1.62718711 0.99364317 1.59577533]\n",
      " [0.         0.         0.         ... 0.         0.         0.18244835]] & cached\n",
      "activation = [[0.02032652 0.08010083 0.12614544 ... 0.08701797 0.10565261 0.104751  ]\n",
      " [0.31857592 0.02093787 0.10684079 ... 0.14568475 0.10310038 0.12136488]\n",
      " [0.10285818 0.04755678 0.10427657 ... 0.10633223 0.15937264 0.12684753]\n",
      " ...\n",
      " [0.088098   0.18445372 0.09337876 ... 0.09352433 0.04664401 0.07336263]\n",
      " [0.11279037 0.05406994 0.10385808 ... 0.18238711 0.19313048 0.09812571]\n",
      " [0.09047213 0.20514293 0.13059145 ... 0.09372981 0.08363336 0.15356312]] & cached\n",
      "Re-used Cached Value, runNum =  53\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  53\n",
      "Provided input from cache for runNum = 53\n",
      "iterations = 50\n",
      "Accuracy = 0.2444390243902439\n",
      "Provided input from cache for runNum = 54\n",
      "activation = [[4.26361612 5.15733492 4.38845408 ... 6.95764155 3.06171899 2.73591405]\n",
      " [0.         2.77317776 0.         ... 0.         0.         0.        ]\n",
      " [0.14067642 3.27810271 2.02795193 ... 1.02202031 3.19752419 1.14958984]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.19327976 0.         0.05060927 ... 0.3229402  3.7439029  0.        ]\n",
      " [0.24901248 2.15354061 0.         ... 1.45142253 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.743477   0.         0.         ... 0.         0.08938643 0.        ]\n",
      " [0.28481088 2.24044747 1.07669573 ... 0.27464083 0.         1.12259265]\n",
      " ...\n",
      " [1.6040912  0.15418126 0.54776891 ... 0.84901968 0.62864208 0.95333682]\n",
      " [3.08786104 0.         1.51530785 ... 1.63414646 1.00442378 1.603294  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.17996509]] & cached\n",
      "activation = [[0.01987673 0.07709487 0.12544708 ... 0.08737921 0.10265753 0.1046381 ]\n",
      " [0.32313846 0.0205365  0.10845998 ... 0.14653204 0.10440675 0.12221896]\n",
      " [0.10097521 0.04635178 0.1045549  ... 0.10606922 0.16144335 0.12689305]\n",
      " ...\n",
      " [0.08900208 0.18720218 0.09280307 ... 0.0929989  0.04564591 0.07300999]\n",
      " [0.1115619  0.0525839  0.1041664  ... 0.18168523 0.19360118 0.09788308]\n",
      " [0.09076068 0.20586384 0.12954778 ... 0.0934616  0.08349965 0.15276584]] & cached\n",
      "Re-used Cached Value, runNum =  54\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  54\n",
      "Provided input from cache for runNum = 54\n",
      "Provided input from cache for runNum = 55\n",
      "activation = [[4.26818343 5.15564021 4.40168245 ... 6.96006925 3.06078297 2.73701296]\n",
      " [0.         2.77799775 0.         ... 0.         0.         0.        ]\n",
      " [0.1353545  3.27831656 2.01754418 ... 1.02743415 3.20032948 1.15161752]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.18891098 0.         0.06220875 ... 0.32633157 3.75830211 0.        ]\n",
      " [0.27406972 2.19331774 0.         ... 1.46482617 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74312957 0.         0.         ... 0.         0.1046114  0.        ]\n",
      " [0.29388954 2.26337908 1.08003584 ... 0.28568436 0.         1.12951021]\n",
      " ...\n",
      " [1.6254619  0.17052675 0.5640599  ... 0.85305558 0.66367273 0.9600689 ]\n",
      " [3.09134192 0.         1.53756309 ... 1.64095266 1.01499465 1.61071693]\n",
      " [0.         0.         0.         ... 0.         0.         0.17764069]] & cached\n",
      "activation = [[0.0194479  0.07423742 0.124794   ... 0.08778053 0.09975603 0.10455837]\n",
      " [0.32758248 0.02012744 0.11007199 ... 0.14730876 0.10568591 0.1230342 ]\n",
      " [0.09911838 0.04515674 0.104823   ... 0.10580889 0.16349472 0.12693148]\n",
      " ...\n",
      " [0.08989959 0.1898982  0.09221167 ... 0.09247139 0.04466544 0.0726595 ]\n",
      " [0.11034731 0.05110764 0.10447936 ... 0.18100619 0.19401487 0.09764318]\n",
      " [0.09103105 0.20644535 0.12847445 ... 0.09317166 0.08334229 0.15196179]] & cached\n",
      "Re-used Cached Value, runNum =  55\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  55\n",
      "Provided input from cache for runNum = 55\n",
      "Provided input from cache for runNum = 56\n",
      "activation = [[4.2726692  5.15379122 4.41491785 ... 6.96243814 3.0598315  2.73803689]\n",
      " [0.         2.78293911 0.         ... 0.         0.         0.        ]\n",
      " [0.13033509 3.27894017 2.00732052 ... 1.03313795 3.20336604 1.15385221]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.18466517 0.         0.07388257 ... 0.32968087 3.77283532 0.        ]\n",
      " [0.29924917 2.23300697 0.         ... 1.47827863 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74275709 0.         0.         ... 0.         0.11983964 0.        ]\n",
      " [0.30318071 2.28673331 1.08349386 ... 0.296957   0.         1.13655495]\n",
      " ...\n",
      " [1.64646189 0.18637811 0.57999714 ... 0.85650009 0.69847232 0.96654854]\n",
      " [3.09457743 0.         1.55982684 ... 1.64750055 1.02545925 1.61808708]\n",
      " [0.         0.         0.         ... 0.         0.         0.17555126]] & cached\n",
      "activation = [[0.01903803 0.07150981 0.12418647 ... 0.08822603 0.09693489 0.10451478]\n",
      " [0.33188608 0.01971079 0.11167316 ... 0.14800383 0.10694372 0.1238078 ]\n",
      " [0.09727601 0.04396615 0.10507386 ... 0.10553647 0.16553576 0.1269593 ]\n",
      " ...\n",
      " [0.09081387 0.1925582  0.09161171 ... 0.09195959 0.04369869 0.07231435]\n",
      " [0.10913336 0.04963473 0.10478605 ... 0.18032242 0.19436807 0.09740049]\n",
      " [0.09130471 0.20691079 0.12737961 ... 0.09287596 0.08315967 0.15115786]] & cached\n",
      "Re-used Cached Value, runNum =  56\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  56\n",
      "Provided input from cache for runNum = 56\n",
      "Provided input from cache for runNum = 57\n",
      "activation = [[4.27710999 5.15191838 4.4281753  ... 6.9648715  3.05893711 2.73904301]\n",
      " [0.         2.78797841 0.         ... 0.         0.         0.        ]\n",
      " [0.12564229 3.27994308 1.99728024 ... 1.03914635 3.20668205 1.15627239]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.18049966 0.         0.08557399 ... 0.33292973 3.78740055 0.        ]\n",
      " [0.32428627 2.27237946 0.         ... 1.49157001 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74236824 0.         0.         ... 0.         0.1349419  0.        ]\n",
      " [0.31271866 2.31060826 1.08714952 ... 0.30861065 0.         1.14372388]\n",
      " ...\n",
      " [1.6669365  0.20150135 0.59542276 ... 0.85921116 0.73281303 0.97265782]\n",
      " [3.0976785  0.         1.58206992 ... 1.6539432  1.03579531 1.62537259]\n",
      " [0.         0.         0.         ... 0.         0.         0.17352582]] & cached\n",
      "activation = [[0.01865198 0.06893894 0.12363939 ... 0.08872892 0.09421198 0.10450923]\n",
      " [0.33604302 0.01928784 0.11324952 ... 0.14860669 0.10817133 0.12453444]\n",
      " [0.09545891 0.04278763 0.10530407 ... 0.1052496  0.1675547  0.1269758 ]\n",
      " ...\n",
      " [0.09172728 0.19516711 0.09100644 ... 0.0914677  0.04275106 0.07197387]\n",
      " [0.10792328 0.04817163 0.10508407 ... 0.1796258  0.19465998 0.09715554]\n",
      " [0.09156924 0.20725663 0.1262588  ... 0.09256786 0.08295004 0.15034671]] & cached\n",
      "Re-used Cached Value, runNum =  57\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  57\n",
      "Provided input from cache for runNum = 57\n",
      "Provided input from cache for runNum = 58\n",
      "activation = [[4.28146418 5.1499313  4.44141517 ... 6.9673043  3.05800668 2.73999557]\n",
      " [0.         2.79320088 0.         ... 0.         0.         0.        ]\n",
      " [0.12124168 3.28132247 1.9874125  ... 1.04544832 3.21023417 1.1588771 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.17639216 0.         0.09728065 ... 0.3360014  3.80196101 0.        ]\n",
      " [0.3492653  2.31152268 0.         ... 1.50479759 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74207105 0.         0.         ... 0.         0.15009124 0.        ]\n",
      " [0.32236531 2.33480794 1.09084006 ... 0.32038737 0.         1.15095864]\n",
      " ...\n",
      " [1.68707363 0.21615282 0.61051574 ... 0.86141742 0.76688065 0.97855444]\n",
      " [3.10062925 0.         1.60430524 ... 1.66023339 1.04602518 1.63263268]\n",
      " [0.         0.         0.         ... 0.         0.         0.17167922]] & cached\n",
      "activation = [[0.01828169 0.0664871  0.12313388 ... 0.08926936 0.09156498 0.10453303]\n",
      " [0.34006594 0.01885984 0.11481238 ... 0.14913679 0.10937572 0.12522226]\n",
      " [0.09366714 0.0416218  0.10552216 ... 0.10495512 0.1695626  0.12698572]\n",
      " ...\n",
      " [0.09264144 0.19772935 0.09038957 ... 0.09098632 0.04181774 0.07163586]\n",
      " [0.10672131 0.04672492 0.10538386 ... 0.17893655 0.19488897 0.09691207]\n",
      " [0.09183028 0.20748983 0.12511584 ... 0.0922501  0.08271645 0.14953319]] & cached\n",
      "Re-used Cached Value, runNum =  58\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  58\n",
      "Provided input from cache for runNum = 58\n",
      "Provided input from cache for runNum = 59\n",
      "activation = [[4.28581104 5.14797338 4.45469335 ... 6.96986877 3.05719436 2.74098084]\n",
      " [0.         2.7985183  0.         ... 0.         0.         0.        ]\n",
      " [0.11706106 3.28296902 1.97767032 ... 1.05189931 3.2138731  1.16161589]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.17233016 0.         0.10900507 ... 0.33888743 3.81655419 0.        ]\n",
      " [0.37423829 2.35051865 0.         ... 1.51799731 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74168677 0.         0.         ... 0.         0.16518761 0.        ]\n",
      " [0.33206093 2.35924036 1.09452375 ... 0.3322774  0.         1.15823919]\n",
      " ...\n",
      " [1.70691674 0.23042783 0.62528056 ... 0.86312712 0.8007714  0.98422746]\n",
      " [3.10356058 0.         1.6266671  ... 1.66657774 1.05640716 1.63995044]\n",
      " [0.         0.         0.         ... 0.         0.         0.16996553]] & cached\n",
      "activation = [[0.01792638 0.06414115 0.12266576 ... 0.08984611 0.08899226 0.10458453]\n",
      " [0.3439584  0.01842952 0.11636933 ... 0.14960572 0.11055833 0.12587502]\n",
      " [0.09189666 0.04046948 0.10572685 ... 0.1046526  0.17157119 0.12699073]\n",
      " ...\n",
      " [0.09356354 0.20024778 0.08976241 ... 0.09051505 0.04089457 0.07129864]\n",
      " [0.10553301 0.045297   0.10568467 ... 0.17825135 0.19506307 0.09667009]\n",
      " [0.09208589 0.20762813 0.12394811 ... 0.09191768 0.08244885 0.14871198]] & cached\n",
      "Re-used Cached Value, runNum =  59\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  59\n",
      "Provided input from cache for runNum = 59\n",
      "Provided input from cache for runNum = 60\n",
      "activation = [[4.29012501 5.14598203 4.46797942 ... 6.97250452 3.0564382  2.74195226]\n",
      " [0.         2.80390206 0.         ... 0.         0.         0.        ]\n",
      " [0.11312218 3.28490186 1.96808625 ... 1.05857114 3.21771452 1.16450012]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.16839348 0.         0.12075524 ... 0.34173234 3.83120094 0.        ]\n",
      " [0.39931503 2.38949572 0.         ... 1.53131679 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74117562 0.         0.         ... 0.         0.18013115 0.        ]\n",
      " [0.34178331 2.38388979 1.098168   ... 0.34417238 0.         1.16551006]\n",
      " ...\n",
      " [1.7264986  0.24433146 0.63972825 ... 0.86441229 0.83438517 0.98963818]\n",
      " [3.10628206 0.         1.64902655 ... 1.67271303 1.06669922 1.64719401]\n",
      " [0.         0.         0.         ... 0.         0.         0.16837914]] & cached\n",
      "activation = [[0.01758657 0.06190422 0.12223505 ... 0.09045541 0.08650054 0.10466615]\n",
      " [0.34769044 0.01799711 0.11791152 ... 0.14999938 0.11170805 0.12648474]\n",
      " [0.09015402 0.03933768 0.10591999 ... 0.10434628 0.17356625 0.12699175]\n",
      " ...\n",
      " [0.09449574 0.20270566 0.08912498 ... 0.09005045 0.03998783 0.07096227]\n",
      " [0.10437292 0.04389641 0.10599427 ... 0.17759421 0.19518482 0.09643524]\n",
      " [0.09234245 0.20766386 0.12275743 ... 0.09157297 0.0821564  0.14788308]] & cached\n",
      "Re-used Cached Value, runNum =  60\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  60\n",
      "Provided input from cache for runNum = 60\n",
      "Provided input from cache for runNum = 61\n",
      "activation = [[4.29437578 5.1439272  4.48123557 ... 6.97519088 3.05570738 2.74289203]\n",
      " [0.         2.80955259 0.         ... 0.         0.         0.        ]\n",
      " [0.10945759 3.2871759  1.95867427 ... 1.06547799 3.22179693 1.16755017]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.16446097 0.         0.1324495  ... 0.34439143 3.84579917 0.        ]\n",
      " [0.42431827 2.42814218 0.         ... 1.54443298 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74075813 0.         0.         ... 0.         0.19508158 0.        ]\n",
      " [0.35194295 2.4092909  1.10217186 ... 0.35657615 0.         1.17297375]\n",
      " ...\n",
      " [1.74567006 0.25762934 0.65369897 ... 0.86505545 0.86755233 0.99471586]\n",
      " [3.10883516 0.         1.67132628 ... 1.67873693 1.07685802 1.65435213]\n",
      " [0.         0.         0.         ... 0.         0.         0.16690267]] & cached\n",
      "activation = [[0.01726275 0.05978189 0.12185669 ... 0.09111721 0.08408979 0.10478282]\n",
      " [0.35125039 0.01755861 0.11941861 ... 0.15030202 0.11282658 0.12704405]\n",
      " [0.08843167 0.03821735 0.10608488 ... 0.10402019 0.17554202 0.1269771 ]\n",
      " ...\n",
      " [0.09544723 0.20512447 0.08849126 ... 0.08961224 0.03909934 0.07063419]\n",
      " [0.10320878 0.04250784 0.10628377 ... 0.17690808 0.19524143 0.09619412]\n",
      " [0.09261638 0.20761385 0.12156298 ... 0.09123126 0.08184138 0.14705984]] & cached\n",
      "Re-used Cached Value, runNum =  61\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  61\n",
      "Provided input from cache for runNum = 61\n",
      "Provided input from cache for runNum = 62\n",
      "activation = [[4.29862861 5.14192668 4.49452277 ... 6.97806166 3.05511152 2.74385633]\n",
      " [0.         2.81517898 0.         ... 0.         0.         0.        ]\n",
      " [0.10594249 3.28962745 1.94934637 ... 1.07245932 3.22594313 1.17067587]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.1606154  0.         0.14416698 ... 0.34695381 3.86047508 0.        ]\n",
      " [0.44933985 2.46663132 0.         ... 1.5575335  0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74037968 0.         0.         ... 0.         0.21003607 0.        ]\n",
      " [0.36237004 2.43508624 1.106306   ... 0.3692513  0.         1.18056309]\n",
      " ...\n",
      " [1.76476491 0.27085061 0.66753455 ... 0.86546406 0.90077875 0.99970729]\n",
      " [3.11132594 0.         1.69372828 ... 1.68479047 1.08714924 1.66157683]\n",
      " [0.         0.         0.         ... 0.         0.         0.1655613 ]] & cached\n",
      "activation = [[0.01694982 0.05774678 0.12150406 ... 0.09180514 0.08173106 0.10491943]\n",
      " [0.35466074 0.01712032 0.12091142 ... 0.15053927 0.11392285 0.12756641]\n",
      " [0.08673841 0.03711868 0.10624068 ... 0.10368885 0.17752819 0.12696106]\n",
      " ...\n",
      " [0.0964045  0.20748041 0.08784749 ... 0.08918421 0.03821776 0.07030558]\n",
      " [0.10205857 0.04114631 0.10657736 ... 0.17623061 0.19524826 0.09595504]\n",
      " [0.09290146 0.20750555 0.1203572  ... 0.09088555 0.0814973  0.14623797]] & cached\n",
      "Re-used Cached Value, runNum =  62\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  62\n",
      "Provided input from cache for runNum = 62\n",
      "Provided input from cache for runNum = 63\n",
      "activation = [[4.30284168 5.1399016  4.50781286 ... 6.98103189 3.05458613 2.74481048]\n",
      " [0.         2.82084161 0.         ... 0.         0.         0.        ]\n",
      " [0.10269256 3.29244978 1.94019811 ... 1.07977602 3.23036366 1.17398401]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.15675894 0.         0.15585042 ... 0.34929153 3.87511297 0.        ]\n",
      " [0.47447259 2.50496305 0.         ... 1.57064561 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73996119 0.         0.         ... 0.         0.22492642 0.        ]\n",
      " [0.37280595 2.46089319 1.11030761 ... 0.38185327 0.         1.18811947]\n",
      " ...\n",
      " [1.78353369 0.28360264 0.68097489 ... 0.86526317 0.93368222 1.00445125]\n",
      " [3.11360433 0.         1.71614563 ... 1.69067071 1.09739289 1.66879667]\n",
      " [0.         0.         0.         ... 0.         0.         0.16433608]] & cached\n",
      "activation = [[0.01664729 0.05579549 0.12118475 ... 0.09252994 0.07944592 0.10508038]\n",
      " [0.35790909 0.01668313 0.12239317 ... 0.15071266 0.11498574 0.12805342]\n",
      " [0.0850624  0.03603682 0.10637898 ... 0.10334242 0.17950198 0.12693745]\n",
      " ...\n",
      " [0.09738997 0.20979882 0.0871978  ... 0.08877046 0.03735222 0.06997906]\n",
      " [0.1009269  0.03981345 0.10687625 ... 0.17556295 0.19519809 0.09572002]\n",
      " [0.09319918 0.20730864 0.1191333  ... 0.09053055 0.08112872 0.14541205]] & cached\n",
      "Re-used Cached Value, runNum =  63\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  63\n",
      "Provided input from cache for runNum = 63\n",
      "Provided input from cache for runNum = 64\n",
      "activation = [[4.30697834 5.13779676 4.52106041 ... 6.98405773 3.05408836 2.74572629]\n",
      " [0.         2.8265627  0.         ... 0.         0.         0.        ]\n",
      " [0.09970334 3.29560936 1.93120219 ... 1.08732404 3.23501326 1.17744306]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.1529184  0.         0.16749813 ... 0.3514243  3.88971339 0.        ]\n",
      " [0.49968113 2.54317763 0.         ... 1.58378909 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73949509 0.         0.         ... 0.         0.23975311 0.        ]\n",
      " [0.38323657 2.48680036 1.11425059 ... 0.39443316 0.         1.19564519]\n",
      " ...\n",
      " [1.80194051 0.29582098 0.69397776 ... 0.86444696 0.96620044 1.00889889]\n",
      " [3.11556317 0.         1.73848763 ... 1.69629652 1.10747141 1.67592978]\n",
      " [0.         0.         0.         ... 0.         0.         0.16325668]] & cached\n",
      "activation = [[0.01635649 0.05393275 0.12090231 ... 0.09329264 0.07723462 0.10526978]\n",
      " [0.36098253 0.0162473  0.12385418 ... 0.15081586 0.11601094 0.12849902]\n",
      " [0.08340917 0.03497389 0.10649644 ... 0.10297736 0.18145654 0.12690365]\n",
      " ...\n",
      " [0.09839804 0.21207624 0.08654572 ... 0.08837518 0.03650522 0.06965682]\n",
      " [0.09981866 0.03851023 0.10717727 ... 0.17490306 0.19509018 0.09548959]\n",
      " [0.09351175 0.20702869 0.1178976  ... 0.09017098 0.08074095 0.14458521]] & cached\n",
      "Re-used Cached Value, runNum =  64\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  64\n",
      "Provided input from cache for runNum = 64\n",
      "Provided input from cache for runNum = 65\n",
      "activation = [[4.31115964 5.13580556 4.534346   ... 6.98734789 3.05377767 2.74669611]\n",
      " [0.         2.83230119 0.         ... 0.         0.         0.        ]\n",
      " [0.09678857 3.29881686 1.92227832 ... 1.09481961 3.2396137  1.18092032]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.14896448 0.         0.17905618 ... 0.35322036 3.90415991 0.        ]\n",
      " [0.52481561 2.58114923 0.         ... 1.59679271 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73899926 0.         0.         ... 0.         0.25453691 0.        ]\n",
      " [0.39397675 2.51311533 1.11836846 ... 0.40740841 0.         1.20332774]\n",
      " ...\n",
      " [1.82034658 0.3081492  0.70691384 ... 0.86353421 0.99877888 1.0133504 ]\n",
      " [3.11757273 0.         1.76096297 ... 1.70215078 1.11775172 1.6832077 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.16225499]] & cached\n",
      "activation = [[0.01607488 0.05214646 0.12063592 ... 0.09407411 0.07507286 0.10547012]\n",
      " [0.36391579 0.01581576 0.12530316 ... 0.1508699  0.11700968 0.12891379]\n",
      " [0.08178152 0.03393795 0.10660088 ... 0.10260359 0.18342341 0.12687043]\n",
      " ...\n",
      " [0.09941349 0.21428095 0.08588785 ... 0.0879926  0.0356651  0.06933211]\n",
      " [0.09872164 0.03723916 0.10747802 ... 0.17423755 0.19493511 0.09525977]\n",
      " [0.09383955 0.20673009 0.11666119 ... 0.08981286 0.08032233 0.14376246]] & cached\n",
      "Re-used Cached Value, runNum =  65\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  65\n",
      "Provided input from cache for runNum = 65\n",
      "Provided input from cache for runNum = 66\n",
      "activation = [[4.3153186  5.13380033 4.54764302 ... 6.99078284 3.05353534 2.74767064]\n",
      " [0.         2.83811551 0.         ... 0.         0.         0.        ]\n",
      " [0.09403519 3.30219332 1.91347448 ... 1.1023906  3.24428684 1.18447566]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.1450289  0.         0.19058763 ... 0.35481188 3.91853869 0.        ]\n",
      " [0.54995155 2.61889098 0.         ... 1.6097199  0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73848644 0.         0.         ... 0.         0.26932008 0.        ]\n",
      " [0.40492673 2.53989895 1.122649   ... 0.42073241 0.         1.21117926]\n",
      " ...\n",
      " [1.83866633 0.32038894 0.71971517 ... 0.86239494 1.03134313 1.01775338]\n",
      " [3.11947228 0.         1.78348377 ... 1.70801643 1.12807816 1.69054844]\n",
      " [0.         0.         0.         ... 0.         0.         0.16142898]] & cached\n",
      "activation = [[0.01580186 0.05043003 0.12039073 ... 0.09488248 0.07296124 0.10568901]\n",
      " [0.36668599 0.01538563 0.12673188 ... 0.150859   0.11797761 0.12929016]\n",
      " [0.08017955 0.03292273 0.10669139 ... 0.10222055 0.18539411 0.12683383]\n",
      " ...\n",
      " [0.1004465  0.21643065 0.08522518 ... 0.08762331 0.03483499 0.06900849]\n",
      " [0.09764301 0.03599542 0.10777967 ... 0.17357055 0.19472862 0.09503058]\n",
      " [0.09418717 0.2063943  0.11542457 ... 0.08945794 0.07987943 0.14294763]] & cached\n",
      "Re-used Cached Value, runNum =  66\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  66\n",
      "Provided input from cache for runNum = 66\n",
      "Provided input from cache for runNum = 67\n",
      "activation = [[4.31946294 5.13181832 4.56094323 ... 6.99436832 3.053408   2.74866689]\n",
      " [0.         2.84402488 0.         ... 0.         0.         0.        ]\n",
      " [0.09148798 3.30580647 1.90481864 ... 1.11011611 3.249077   1.18812473]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.14121524 0.         0.20214766 ... 0.35641384 3.93298772 0.        ]\n",
      " [0.5750664  2.65644447 0.         ... 1.62257664 0.         0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73790846 0.         0.         ... 0.         0.2839831  0.        ]\n",
      " [0.41587422 2.56674251 1.12684311 ... 0.43403269 0.         1.21914123]\n",
      " ...\n",
      " [1.85687576 0.33243758 0.73233026 ... 0.86100364 1.06381375 1.02206663]\n",
      " [3.12124433 0.         1.80600543 ... 1.71387459 1.13842332 1.69794467]\n",
      " [0.         0.         0.         ... 0.         0.         0.16063712]] & cached\n",
      "activation = [[0.01553748 0.04878354 0.12016569 ... 0.09571449 0.07090696 0.10592193]\n",
      " [0.36929965 0.01496009 0.12814493 ... 0.15079321 0.11890801 0.1296314 ]\n",
      " [0.07860861 0.03193262 0.10677401 ... 0.10183985 0.18736252 0.12679366]\n",
      " ...\n",
      " [0.10148716 0.21852097 0.08455201 ... 0.08725257 0.03401755 0.06868443]\n",
      " [0.09659427 0.03478813 0.10809476 ... 0.17293337 0.19447534 0.09480331]\n",
      " [0.09454624 0.2060068  0.11417909 ... 0.08909197 0.07941267 0.14213593]] & cached\n",
      "Re-used Cached Value, runNum =  67\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  67\n",
      "Provided input from cache for runNum = 67\n",
      "Provided input from cache for runNum = 68\n",
      "activation = [[4.32359117 5.12988096 4.57425465 ... 6.99816722 3.05341957 2.74969529]\n",
      " [0.         2.85004904 0.         ... 0.         0.         0.        ]\n",
      " [0.08912664 3.30963329 1.89629946 ... 1.11792541 3.25396067 1.19188366]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.13732816 0.         0.21362922 ... 0.35766616 3.94730763 0.        ]\n",
      " [0.60031026 2.69401864 0.         ... 1.63559602 0.00755992 0.        ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7372179  0.         0.         ... 0.         0.2963938  0.        ]\n",
      " [0.42711465 2.59402597 1.13119941 ... 0.4477     0.         1.23134171]\n",
      " ...\n",
      " [1.87508173 0.34453494 0.74487125 ... 0.85943316 1.09618727 1.02808306]\n",
      " [3.12276998 0.         1.82850881 ... 1.71955986 1.14475003 1.70770166]\n",
      " [0.         0.         0.         ... 0.         0.         0.15777627]] & cached\n",
      "activation = [[0.0152794  0.04718516 0.11995323 ... 0.0965681  0.06896692 0.10603837]\n",
      " [0.37172469 0.01453613 0.129539   ... 0.15066174 0.11963371 0.12997964]\n",
      " [0.07705636 0.03095904 0.10683987 ... 0.10144387 0.18914223 0.12671864]\n",
      " ...\n",
      " [0.10256896 0.2205665  0.08387693 ... 0.08690093 0.0332822  0.06836267]\n",
      " [0.09556145 0.03360512 0.10840793 ... 0.17228732 0.19430203 0.09438008]\n",
      " [0.09494425 0.20559173 0.11294394 ... 0.08874187 0.07908314 0.14146501]] & cached\n",
      "Re-used Cached Value, runNum =  68\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  68\n",
      "Provided input from cache for runNum = 68\n",
      "Provided input from cache for runNum = 69\n",
      "activation = [[4.32773227 5.12801287 4.58758499 ... 7.00216711 3.05358209 2.75075949]\n",
      " [0.         2.8560294  0.         ... 0.         0.         0.        ]\n",
      " [0.08691102 3.31360363 1.88788155 ... 1.12579436 3.25890153 1.19572582]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.13341927 0.         0.22501873 ... 0.35872587 3.96157738 0.        ]\n",
      " [0.62544919 2.73128878 0.         ... 1.64845139 0.02014312 0.00714802]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73653533 0.         0.         ... 0.         0.30736441 0.        ]\n",
      " [0.43839083 2.62140005 1.13552954 ... 0.46146794 0.         1.24380923]\n",
      " ...\n",
      " [1.89312872 0.35643915 0.75717155 ... 0.85750635 1.12835309 1.03384709]\n",
      " [3.12431781 0.         1.851146   ... 1.72546404 1.14856372 1.71375337]\n",
      " [0.         0.         0.         ... 0.         0.         0.15315977]] & cached\n",
      "activation = [[0.01502854 0.04565143 0.11976282 ... 0.09745386 0.06711912 0.10605114]\n",
      " [0.37401193 0.01411849 0.13092158 ... 0.15048368 0.12021426 0.1302024 ]\n",
      " [0.07552957 0.03000866 0.10689139 ... 0.10104414 0.19079708 0.12649333]\n",
      " ...\n",
      " [0.10366068 0.22256254 0.08319561 ... 0.08655189 0.03260173 0.06811967]\n",
      " [0.09454795 0.03245585 0.10872602 ... 0.1716514  0.19416061 0.09391476]\n",
      " [0.09535187 0.20514747 0.11169983 ... 0.08837731 0.07883219 0.14105026]] & cached\n",
      "Re-used Cached Value, runNum =  69\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  69\n",
      "Provided input from cache for runNum = 69\n",
      "Provided input from cache for runNum = 70\n",
      "activation = [[4.33184246 5.12616176 4.6009019  ... 7.00635551 3.05384547 2.75184803]\n",
      " [0.         2.86200835 0.         ... 0.         0.         0.        ]\n",
      " [0.08489144 3.31773183 1.87961461 ... 1.13373308 3.26390487 1.1996446 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.12959326 0.         0.2364083  ... 0.35969586 3.97591861 0.        ]\n",
      " [0.6506619  2.76844967 0.         ... 1.66131997 0.03278    0.02145561]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73582318 0.         0.         ... 0.         0.31832388 0.        ]\n",
      " [0.44981579 2.64900389 1.13992258 ... 0.475436   0.         1.25666836]\n",
      " ...\n",
      " [1.91107878 0.36819939 0.76928817 ... 0.85530482 1.16048688 1.03939285]\n",
      " [3.12574116 0.         1.87386837 ... 1.73146228 1.15245504 1.71605363]\n",
      " [0.         0.         0.         ... 0.         0.         0.14679893]] & cached\n",
      "activation = [[0.01478461 0.04417384 0.11959481 ... 0.09837021 0.0653136  0.10595923]\n",
      " [0.37612652 0.01370494 0.13228367 ... 0.15024991 0.120758   0.13029165]\n",
      " [0.07403145 0.0290794  0.10693417 ... 0.10064261 0.19245328 0.12611448]\n",
      " ...\n",
      " [0.10477312 0.22450913 0.08250509 ... 0.08620568 0.0319282  0.06795843]\n",
      " [0.09355744 0.03133756 0.10904951 ... 0.17102392 0.19397085 0.09340277]\n",
      " [0.09578299 0.20466858 0.1104514  ... 0.08800682 0.07855865 0.14090237]] & cached\n",
      "Re-used Cached Value, runNum =  70\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  70\n",
      "Provided input from cache for runNum = 70\n",
      "Provided input from cache for runNum = 71\n",
      "activation = [[4.3359601  5.12436821 4.61421209 ... 7.01075726 3.05424049 2.7529654 ]\n",
      " [0.         2.86808222 0.         ... 0.         0.         0.        ]\n",
      " [0.08294456 3.32189599 1.87141719 ... 1.14159554 3.26886364 1.20357981]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.12575212 0.         0.24772184 ... 0.36044996 3.99014711 0.        ]\n",
      " [0.67583841 2.80539173 0.         ... 1.67413585 0.04539726 0.03573231]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73603212 0.         0.         ... 0.         0.32929695 0.        ]\n",
      " [0.46102369 2.67684175 1.14434211 ... 0.48952722 0.         1.26965135]\n",
      " ...\n",
      " [1.92867012 0.38012158 0.78136805 ... 0.85302155 1.19274657 1.04496075]\n",
      " [3.12728703 0.         1.89671443 ... 1.73767894 1.1565032  1.71843474]\n",
      " [0.         0.         0.         ... 0.         0.         0.14052382]] & cached\n",
      "activation = [[0.01452268 0.04274617 0.11943755 ... 0.09930312 0.06354196 0.10587565]\n",
      " [0.37823426 0.01329672 0.13363251 ... 0.14997819 0.12126753 0.13034689]\n",
      " [0.0724647  0.02817633 0.10697046 ... 0.10023971 0.19411973 0.12573447]\n",
      " ...\n",
      " [0.10601018 0.22638343 0.08180472 ... 0.0858599  0.03125847 0.0677945 ]\n",
      " [0.0924995  0.03025467 0.10938507 ... 0.17041625 0.1937341  0.09289645]\n",
      " [0.09625406 0.20417045 0.1092001  ... 0.08762773 0.07826005 0.14075737]] & cached\n",
      "Re-used Cached Value, runNum =  71\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  71\n",
      "Provided input from cache for runNum = 71\n",
      "Provided input from cache for runNum = 72\n",
      "activation = [[4.34010974 5.12266474 4.62753538 ... 7.01541658 3.05481171 2.75414626]\n",
      " [0.         2.87408766 0.         ... 0.         0.         0.        ]\n",
      " [0.08110302 3.32614031 1.86330796 ... 1.14944342 3.27381015 1.20755979]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.12210196 0.         0.25909239 ... 0.36124696 4.00459747 0.        ]\n",
      " [0.70095245 2.84221299 0.         ... 1.68694747 0.05803403 0.05000647]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73682682 0.         0.         ... 0.         0.34042963 0.        ]\n",
      " [0.47193668 2.70464407 1.14860402 ... 0.50345225 0.         1.2827265 ]\n",
      " ...\n",
      " [1.94611676 0.39215107 0.79339467 ... 0.85065708 1.22516926 1.05056225]\n",
      " [3.1289586  0.         1.91972472 ... 1.74414454 1.16077122 1.7209277 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.13429786]] & cached\n",
      "activation = [[0.01425431 0.04136751 0.11929214 ... 0.10025228 0.06179778 0.1057984 ]\n",
      " [0.38029389 0.0128962  0.13497118 ... 0.14967248 0.1217483  0.13037071]\n",
      " [0.07089052 0.02730049 0.10701004 ... 0.09985052 0.19580155 0.12535609]\n",
      " ...\n",
      " [0.10729094 0.22818802 0.08108675 ... 0.08549847 0.03059041 0.0676257 ]\n",
      " [0.0914309  0.02921    0.10974676 ... 0.16986157 0.19344178 0.09239792]\n",
      " [0.09673668 0.20365324 0.10793365 ... 0.08722393 0.07793439 0.14061096]] & cached\n",
      "Re-used Cached Value, runNum =  72\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  72\n",
      "Provided input from cache for runNum = 72\n",
      "Provided input from cache for runNum = 73\n",
      "activation = [[4.34425972 5.12102213 4.64084386 ... 7.02029469 3.05552698 2.75535744]\n",
      " [0.         2.87999289 0.         ... 0.         0.         0.        ]\n",
      " [0.07937257 3.33051511 1.85528474 ... 1.15734079 3.27881309 1.21158272]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.11827477 0.         0.27034527 ... 0.36159483 4.01875924 0.        ]\n",
      " [0.7259367  2.87880834 0.         ... 1.69962717 0.07058589 0.0642232 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73769626 0.         0.         ... 0.         0.35152336 0.        ]\n",
      " [0.4833059  2.73286323 1.15312463 ... 0.51792468 0.         1.29605867]\n",
      " ...\n",
      " [1.96326865 0.40381544 0.80504978 ... 0.84765884 1.25728132 1.05600233]\n",
      " [3.13060283 0.         1.94279957 ... 1.75070055 1.16513082 1.7234842 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.128124  ]] & cached\n",
      "activation = [[0.01399552 0.04005267 0.11918183 ... 0.10125718 0.06010723 0.10574143]\n",
      " [0.38217835 0.01250105 0.13627594 ... 0.14929649 0.12218778 0.13035223]\n",
      " [0.06933461 0.02644433 0.10702118 ... 0.09942893 0.19747237 0.12496366]\n",
      " ...\n",
      " [0.10860967 0.2299569  0.08037525 ... 0.08516706 0.02993367 0.06746127]\n",
      " [0.09036313 0.02819249 0.11009736 ... 0.16926814 0.19309597 0.09189478]\n",
      " [0.09725019 0.20312549 0.1066713  ... 0.08682373 0.07758473 0.14047116]] & cached\n",
      "Re-used Cached Value, runNum =  73\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  73\n",
      "Provided input from cache for runNum = 73\n",
      "Provided input from cache for runNum = 74\n",
      "activation = [[4.34845223 5.11947984 4.65416512 ... 7.02542287 3.0564357  2.75662883]\n",
      " [0.         2.88587474 0.         ... 0.         0.         0.        ]\n",
      " [0.0777886  3.33502675 1.84739913 ... 1.16529772 3.28387528 1.21566104]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.11443828 0.         0.28149948 ... 0.36170363 4.03281418 0.        ]\n",
      " [0.75079824 2.91507834 0.         ... 1.71214946 0.08308444 0.07833525]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73838349 0.         0.         ... 0.         0.36236617 0.        ]\n",
      " [0.49458168 2.76095199 1.15752553 ... 0.5323855  0.         1.30945335]\n",
      " ...\n",
      " [1.98045751 0.41564452 0.81664112 ... 0.84450013 1.28949308 1.06149973]\n",
      " [3.13219281 0.         1.96588757 ... 1.7573854  1.16953072 1.72613249]\n",
      " [0.         0.         0.         ... 0.         0.         0.12204179]] & cached\n",
      "activation = [[0.01374284 0.03878192 0.11907669 ... 0.10228028 0.05845767 0.10568822]\n",
      " [0.38392028 0.01211534 0.13756976 ... 0.14888717 0.1225788  0.13030618]\n",
      " [0.06780965 0.02561591 0.10702267 ... 0.0990034  0.19914262 0.12457052]\n",
      " ...\n",
      " [0.10994312 0.23165589 0.07965667 ... 0.0848358  0.02928508 0.06729311]\n",
      " [0.08933179 0.02721363 0.11046482 ... 0.16869814 0.19271706 0.09139949]\n",
      " [0.09777848 0.20259352 0.10541072 ... 0.08641506 0.07721421 0.14033412]] & cached\n",
      "Re-used Cached Value, runNum =  74\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  74\n",
      "Provided input from cache for runNum = 74\n",
      "Provided input from cache for runNum = 75\n",
      "activation = [[4.35263347 5.11792749 4.66748109 ... 7.03071042 3.05741858 2.75790872]\n",
      " [0.         2.89176697 0.         ... 0.         0.         0.        ]\n",
      " [0.07643349 3.33975331 1.8396931  ... 1.17340174 3.28908334 1.21985542]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.11054653 0.         0.29255357 ... 0.36141066 4.04665401 0.        ]\n",
      " [0.77562271 2.9511796  0.         ... 1.72464417 0.09561614 0.09244953]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73891802 0.         0.         ... 0.         0.37303303 0.        ]\n",
      " [0.50575746 2.78903649 1.16183304 ... 0.54694264 0.         1.32293642]\n",
      " ...\n",
      " [1.99740363 0.42717793 0.82788902 ... 0.84080623 1.32141048 1.06687459]\n",
      " [3.13361599 0.         1.98897354 ... 1.76403149 1.17384169 1.72876795]\n",
      " [0.         0.         0.         ... 0.         0.         0.11606639]] & cached\n",
      "activation = [[0.01349683 0.03755646 0.11899188 ... 0.10333978 0.05686143 0.1056483 ]\n",
      " [0.38551038 0.01173661 0.13884856 ... 0.14843568 0.12291911 0.13022677]\n",
      " [0.06630364 0.02480522 0.10699913 ... 0.09854958 0.20079256 0.12416272]\n",
      " ...\n",
      " [0.11131247 0.23332705 0.07894053 ... 0.08452584 0.02865067 0.06712914]\n",
      " [0.088323   0.0262632  0.11083445 ... 0.16811372 0.19229427 0.09090509]\n",
      " [0.09832935 0.20204126 0.10415262 ... 0.08600913 0.07682769 0.1402065 ]] & cached\n",
      "Re-used Cached Value, runNum =  75\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  75\n",
      "Provided input from cache for runNum = 75\n",
      "Provided input from cache for runNum = 76\n",
      "activation = [[4.35684804 5.11644679 4.68081239 ... 7.03622024 3.05852069 2.75923279]\n",
      " [0.         2.89770306 0.         ... 0.         0.         0.        ]\n",
      " [0.07522946 3.34461543 1.83210958 ... 1.18154152 3.29438588 1.22409595]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.10669892 0.         0.30351105 ... 0.36099642 4.0604945  0.        ]\n",
      " [0.80053017 2.98722723 0.         ... 1.73714575 0.10824128 0.10658538]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73937993 0.         0.         ... 0.         0.38362622 0.        ]\n",
      " [0.51704807 2.81732258 1.16621627 ... 0.56164304 0.         1.33658772]\n",
      " ...\n",
      " [2.01416758 0.43843168 0.83880591 ... 0.83668724 1.35312732 1.07214518]\n",
      " [3.13489517 0.         2.01211202 ... 1.77076311 1.1781222  1.73143627]\n",
      " [0.         0.         0.         ... 0.         0.         0.1101407 ]] & cached\n",
      "activation = [[0.01325771 0.03637824 0.11893538 ... 0.10443846 0.05531002 0.10562243]\n",
      " [0.3869222  0.01136288 0.14009893 ... 0.14793309 0.1232136  0.13011023]\n",
      " [0.0648223  0.02401437 0.1069523  ... 0.09807942 0.20242892 0.12374508]\n",
      " ...\n",
      " [0.11272236 0.23495453 0.07822735 ... 0.08422692 0.02802758 0.06696663]\n",
      " [0.08734061 0.02534191 0.1112087  ... 0.16753885 0.19182405 0.09041234]\n",
      " [0.09890689 0.20145899 0.10289361 ... 0.08559373 0.07642373 0.14008421]] & cached\n",
      "Re-used Cached Value, runNum =  76\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  76\n",
      "Provided input from cache for runNum = 76\n",
      "Provided input from cache for runNum = 77\n",
      "activation = [[4.36105366 5.11496743 4.69411976 ... 7.04190785 3.05971942 2.76055871]\n",
      " [0.         2.90375365 0.         ... 0.         0.         0.        ]\n",
      " [0.07416422 3.34961889 1.82464108 ... 1.18971241 3.29975427 1.22838704]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.10287342 0.         0.31446074 ... 0.36039867 4.07427854 0.        ]\n",
      " [0.82549758 3.02320711 0.         ... 1.74971104 0.12095742 0.12074615]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73979023 0.         0.         ... 0.         0.39408792 0.        ]\n",
      " [0.52841133 2.84581881 1.17061919 ... 0.57654879 0.         1.3503782 ]\n",
      " ...\n",
      " [2.03079491 0.44949706 0.8494943  ... 0.83221284 1.38472039 1.07732929]\n",
      " [3.13605598 0.         2.03530132 ... 1.77755233 1.1823768  1.73408588]\n",
      " [0.         0.         0.         ... 0.         0.         0.10427192]] & cached\n",
      "activation = [[0.01302402 0.03524094 0.11889805 ... 0.10557142 0.05380069 0.10560748]\n",
      " [0.38817285 0.01099448 0.14132774 ... 0.14738585 0.12346069 0.12995853]\n",
      " [0.06336621 0.02324242 0.10688957 ... 0.0975926  0.20405348 0.1233161 ]\n",
      " ...\n",
      " [0.11416658 0.23653888 0.07751161 ... 0.0839394  0.02741511 0.06680628]\n",
      " [0.08638144 0.02444834 0.11159059 ... 0.16696179 0.19131145 0.08992185]\n",
      " [0.09951137 0.20085408 0.10163658 ... 0.0851776  0.07600255 0.13997132]] & cached\n",
      "Re-used Cached Value, runNum =  77\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  77\n",
      "Provided input from cache for runNum = 77\n",
      "Provided input from cache for runNum = 78\n",
      "activation = [[4.36530572 5.11360863 4.70743078 ... 7.04789572 3.06109392 2.76195561]\n",
      " [0.         2.90973944 0.         ... 0.         0.         0.        ]\n",
      " [0.07323488 3.35471223 1.81728804 ... 1.19785723 3.3052042  1.23270946]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0991407  0.         0.32538846 ... 0.35967803 4.08810391 0.        ]\n",
      " [0.85039348 3.05893267 0.         ... 1.76211115 0.13361626 0.13483606]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74011406 0.         0.         ... 0.         0.40444647 0.        ]\n",
      " [0.5397315  2.87433686 1.17496905 ... 0.59153695 0.         1.3642788 ]\n",
      " ...\n",
      " [2.04743962 0.46058677 0.86007016 ... 0.82757389 1.41632657 1.08251221]\n",
      " [3.13722866 0.         2.05859733 ... 1.78464349 1.18679106 1.73683526]\n",
      " [0.         0.         0.         ... 0.         0.         0.09846517]] & cached\n",
      "activation = [[0.01279492 0.03414458 0.11887068 ... 0.10672985 0.05232686 0.10560002]\n",
      " [0.38929646 0.01063499 0.14254365 ... 0.14680941 0.12366589 0.12977808]\n",
      " [0.06194026 0.02249447 0.10681568 ... 0.09710381 0.20567864 0.12288518]\n",
      " ...\n",
      " [0.11562465 0.23806585 0.07679009 ... 0.08364783 0.02680926 0.06664235]\n",
      " [0.08545341 0.02358735 0.11198878 ... 0.16640834 0.19075684 0.08943831]\n",
      " [0.10013131 0.2002547  0.10037899 ... 0.08474629 0.07555694 0.13985894]] & cached\n",
      "Re-used Cached Value, runNum =  78\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  78\n",
      "Provided input from cache for runNum = 78\n",
      "Provided input from cache for runNum = 79\n",
      "activation = [[4.36947218 5.11214896 4.72068509 ... 7.05396313 3.06248079 2.76331892]\n",
      " [0.         2.91575902 0.         ... 0.         0.         0.        ]\n",
      " [0.07246337 3.35997468 1.81006118 ... 1.20607469 3.31076891 1.23710727]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.09540775 0.         0.3362478  ... 0.35872585 4.10186675 0.        ]\n",
      " [0.8751707  3.09443658 0.         ... 1.77437923 0.1462979  0.14885165]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7405459  0.         0.         ... 0.         0.41482702 0.        ]\n",
      " [0.55099589 2.90285969 1.17924855 ... 0.60660903 0.         1.37826165]\n",
      " ...\n",
      " [2.06397155 0.4715392  0.87042882 ... 0.82257079 1.44782473 1.08763428]\n",
      " [3.13826429 0.         2.08185459 ... 1.79173084 1.19112073 1.73956599]\n",
      " [0.         0.         0.         ... 0.         0.         0.09279617]] & cached\n",
      "activation = [[0.0125689  0.0330866  0.11885673 ... 0.10792227 0.0508879  0.10560414]\n",
      " [0.39029136 0.01028304 0.14374071 ... 0.14619023 0.12382702 0.12956507]\n",
      " [0.06053838 0.02176738 0.10672333 ... 0.09660106 0.20729043 0.12244569]\n",
      " ...\n",
      " [0.11710701 0.23954609 0.07606857 ... 0.08336401 0.02621334 0.06647946]\n",
      " [0.08454608 0.0227562  0.11239905 ... 0.16586463 0.19015171 0.08896082]\n",
      " [0.10077546 0.19964281 0.0991274  ... 0.08431146 0.07509768 0.13975427]] & cached\n",
      "Re-used Cached Value, runNum =  79\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  79\n",
      "Provided input from cache for runNum = 79\n",
      "Provided input from cache for runNum = 80\n",
      "activation = [[4.3736484  5.11076711 4.73392605 ... 7.06028628 3.06402946 2.7647217 ]\n",
      " [0.         2.92171906 0.         ... 0.         0.         0.        ]\n",
      " [0.07178744 3.36528026 1.80293789 ... 1.21426571 3.31636034 1.24151308]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.09182545 0.         0.34709475 ... 0.35772165 4.11567398 0.        ]\n",
      " [0.89987327 3.12973766 0.         ... 1.78649745 0.1589416  0.16279694]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74106345 0.         0.         ... 0.         0.42520713 0.        ]\n",
      " [0.56233498 2.93164718 1.18364759 ... 0.62193126 0.         1.39243846]\n",
      " ...\n",
      " [2.08041591 0.48233748 0.88052894 ... 0.81721321 1.47921499 1.09265243]\n",
      " [3.13939775 0.         2.10528433 ... 1.79920483 1.19569677 1.74241613]\n",
      " [0.         0.         0.         ... 0.         0.         0.08718722]] & cached\n",
      "activation = [[0.01234778 0.03207122 0.11886853 ... 0.10915756 0.04948472 0.10562382]\n",
      " [0.39117098 0.00993833 0.14491217 ... 0.14553536 0.12395144 0.12931979]\n",
      " [0.05916702 0.02106112 0.10661592 ... 0.09608791 0.20890174 0.1219989 ]\n",
      " ...\n",
      " [0.11859675 0.24097979 0.07534437 ... 0.0830823  0.02562397 0.06631596]\n",
      " [0.08366147 0.0219527  0.11281704 ... 0.1653263  0.18949753 0.08848557]\n",
      " [0.1014327  0.19903484 0.09787321 ... 0.0838598  0.0746111  0.13964973]] & cached\n",
      "Re-used Cached Value, runNum =  80\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  80\n",
      "Provided input from cache for runNum = 80\n",
      "Provided input from cache for runNum = 81\n",
      "activation = [[4.37779573 5.10937681 4.74712722 ... 7.06680177 3.06567399 2.76612956]\n",
      " [0.         2.92758662 0.         ... 0.         0.         0.        ]\n",
      " [0.07127739 3.37076255 1.79595449 ... 1.2225397  3.32209634 1.24598337]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0883207  0.         0.35791596 ... 0.35670027 4.1295108  0.        ]\n",
      " [0.92464616 3.16502294 0.         ... 1.79869321 0.17172219 0.17680674]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74155646 0.         0.         ... 0.         0.43542485 0.        ]\n",
      " [0.573635   2.96042585 1.18798253 ... 0.63728899 0.         1.40675012]\n",
      " ...\n",
      " [2.09674232 0.49298907 0.89041769 ... 0.81156523 1.51050226 1.09764582]\n",
      " [3.14034565 0.         2.12871172 ... 1.8066908  1.2001589  1.74523828]\n",
      " [0.         0.         0.         ... 0.         0.         0.08161224]] & cached\n",
      "activation = [[0.0121309  0.03109122 0.11889649 ... 0.11042676 0.04811923 0.10564889]\n",
      " [0.39190416 0.00960137 0.14606116 ... 0.14483854 0.12402538 0.12904353]\n",
      " [0.05782418 0.02037515 0.10649964 ... 0.09557387 0.21049521 0.12154342]\n",
      " ...\n",
      " [0.12011312 0.24236743 0.07461426 ... 0.08279696 0.02504604 0.06615278]\n",
      " [0.08280452 0.02117788 0.11325129 ... 0.16481043 0.18880497 0.0880148 ]\n",
      " [0.10211761 0.19841701 0.09662177 ... 0.08340079 0.07411244 0.13955612]] & cached\n",
      "Re-used Cached Value, runNum =  81\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  81\n",
      "Provided input from cache for runNum = 81\n",
      "Provided input from cache for runNum = 82\n",
      "activation = [[4.38202101 5.10814046 4.76035355 ... 7.07365495 3.0675551  2.76761683]\n",
      " [0.         2.93344062 0.         ... 0.         0.         0.        ]\n",
      " [0.07079628 3.37621171 1.78901945 ... 1.23072748 3.32780137 1.2504324 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.08474889 0.         0.36862152 ... 0.35537453 4.14313438 0.        ]\n",
      " [0.94931425 3.20007371 0.         ... 1.81076393 0.18448521 0.19075097]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74187923 0.         0.         ... 0.         0.44534997 0.        ]\n",
      " [0.58508011 2.98939214 1.19243025 ... 0.65298677 0.         1.42124183]\n",
      " ...\n",
      " [2.11302251 0.50367837 0.90016414 ... 0.80566467 1.54171404 1.10266346]\n",
      " [3.14142799 0.         2.15233382 ... 1.81457769 1.20483042 1.74823728]\n",
      " [0.         0.         0.         ... 0.         0.         0.07611552]] & cached\n",
      "activation = [[0.0119185  0.03014359 0.11893465 ... 0.1117293  0.04679376 0.10567968]\n",
      " [0.39251988 0.00927266 0.1472012  ... 0.14412096 0.12405313 0.12874633]\n",
      " [0.05650061 0.01970854 0.10636057 ... 0.0950395  0.21208323 0.12108222]\n",
      " ...\n",
      " [0.12166127 0.24371079 0.07388638 ... 0.08252094 0.02447672 0.06598724]\n",
      " [0.08196511 0.02042872 0.11368795 ... 0.16428214 0.18808246 0.08754605]\n",
      " [0.10282399 0.19782026 0.09537769 ... 0.08293623 0.07358841 0.13946661]] & cached\n",
      "Re-used Cached Value, runNum =  82\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  82\n",
      "Provided input from cache for runNum = 82\n",
      "Provided input from cache for runNum = 83\n",
      "activation = [[4.38630169 5.1069986  4.77359941 ... 7.08080625 3.06963061 2.76916334]\n",
      " [0.         2.93936988 0.         ... 0.         0.         0.        ]\n",
      " [0.07043239 3.38174733 1.78219822 ... 1.2389313  3.33354373 1.25492821]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.08137385 0.         0.37934019 ... 0.35409153 4.15694305 0.        ]\n",
      " [0.97407957 3.23515871 0.         ... 1.822925   0.19736046 0.20477594]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74224993 0.         0.         ... 0.         0.45528941 0.        ]\n",
      " [0.5963126  3.01826899 1.19670869 ... 0.66857896 0.         1.43580399]\n",
      " ...\n",
      " [2.12935644 0.51444137 0.90982558 ... 0.79963864 1.57300036 1.10771069]\n",
      " [3.14254426 0.         2.17616624 ... 1.82275271 1.20968575 1.75130973]\n",
      " [0.         0.         0.         ... 0.         0.         0.0706545 ]] & cached\n",
      "activation = [[0.01170683 0.02922163 0.11897587 ... 0.11305146 0.04549418 0.10571093]\n",
      " [0.39305379 0.00895211 0.1483428  ... 0.14339276 0.1240433  0.12842963]\n",
      " [0.05520333 0.01906153 0.10621476 ... 0.0945061  0.21367272 0.12061623]\n",
      " ...\n",
      " [0.12321992 0.24500205 0.0731492  ... 0.08223487 0.02391249 0.06581869]\n",
      " [0.08115176 0.0197069  0.11414667 ... 0.16378637 0.18731376 0.08708396]\n",
      " [0.1035448  0.19722202 0.09413041 ... 0.08245601 0.07304116 0.13938251]] & cached\n",
      "Re-used Cached Value, runNum =  83\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  83\n",
      "Provided input from cache for runNum = 83\n",
      "Provided input from cache for runNum = 84\n",
      "activation = [[4.39055567 5.10585205 4.78680538 ... 7.08815779 3.07181066 2.77072172]\n",
      " [0.         2.94533287 0.         ... 0.         0.         0.        ]\n",
      " [0.07016294 3.38734997 1.77546632 ... 1.24710809 3.33933968 1.25943851]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.07788587 0.         0.38988769 ... 0.35240894 4.17045953 0.        ]\n",
      " [0.99878085 3.27002563 0.         ... 1.83489144 0.21019052 0.21872618]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74269031 0.         0.         ... 0.         0.46514242 0.        ]\n",
      " [0.60753099 3.04717798 1.20101744 ... 0.68434058 0.         1.4504951 ]\n",
      " ...\n",
      " [2.14547281 0.5249966  0.91918142 ... 0.79313254 1.6040025  1.11269242]\n",
      " [3.14371291 0.         2.20016443 ... 1.83130726 1.21467017 1.754531  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.06529412]] & cached\n",
      "activation = [[0.01149685 0.02832565 0.11903567 ... 0.11441896 0.04423439 0.10575345]\n",
      " [0.39347606 0.0086382  0.14947397 ... 0.14264585 0.12399236 0.12809145]\n",
      " [0.05391981 0.01842894 0.10604231 ... 0.09394918 0.21524688 0.12014186]\n",
      " ...\n",
      " [0.12482737 0.24626685 0.07241613 ... 0.08195773 0.02335877 0.0656493 ]\n",
      " [0.08034871 0.019007   0.11460784 ... 0.16328115 0.18650288 0.08662507]\n",
      " [0.10428937 0.19661041 0.09288773 ... 0.08196348 0.07247336 0.13930009]] & cached\n",
      "Re-used Cached Value, runNum =  84\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  84\n",
      "Provided input from cache for runNum = 84\n",
      "Provided input from cache for runNum = 85\n",
      "activation = [[4.39487319 5.10482906 4.80001602 ... 7.09579051 3.07417325 2.77233537]\n",
      " [0.         2.95118366 0.         ... 0.         0.         0.        ]\n",
      " [0.06997729 3.39302589 1.7688265  ... 1.25527756 3.34518523 1.26397252]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.07439939 0.         0.40033012 ... 0.35046037 4.18382254 0.        ]\n",
      " [1.02346417 3.30470231 0.         ... 1.84673236 0.22300487 0.23262763]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74296775 0.         0.         ... 0.         0.47472409 0.        ]\n",
      " [0.61862317 3.07596137 1.20522275 ... 0.700172   0.         1.46523308]\n",
      " ...\n",
      " [2.16148706 0.53549694 0.92831576 ... 0.78630897 1.63482525 1.11763901]\n",
      " [3.14483864 0.         2.22424373 ... 1.84014008 1.21973631 1.75783172]\n",
      " [0.         0.         0.         ... 0.         0.         0.06000817]] & cached\n",
      "activation = [[0.01129204 0.02746808 0.11910866 ... 0.11582273 0.04301386 0.10580306]\n",
      " [0.39379198 0.00833482 0.15059215 ... 0.14187777 0.12389375 0.12773186]\n",
      " [0.05266335 0.01782047 0.10585566 ... 0.0933803  0.21680539 0.11966368]\n",
      " ...\n",
      " [0.12644736 0.24747561 0.07168029 ... 0.08168062 0.0228159  0.06547666]\n",
      " [0.07957754 0.0183376  0.11508799 ... 0.16278887 0.18566326 0.08617515]\n",
      " [0.10505063 0.19602363 0.09164679 ... 0.08145839 0.07188737 0.13921834]] & cached\n",
      "Re-used Cached Value, runNum =  85\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  85\n",
      "Provided input from cache for runNum = 85\n",
      "Provided input from cache for runNum = 86\n",
      "activation = [[4.39924987 5.10392088 4.81323795 ... 7.10373819 3.07675064 2.77400126]\n",
      " [0.         2.95696969 0.         ... 0.         0.         0.        ]\n",
      " [0.06986616 3.39873271 1.7622755  ... 1.26340279 3.35102962 1.26852288]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.07098866 0.         0.41073828 ... 0.34836451 4.19714023 0.        ]\n",
      " [1.04808357 3.33920085 0.         ... 1.85848384 0.23580379 0.24649761]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74322051 0.         0.         ... 0.         0.48414581 0.        ]\n",
      " [0.62962239 3.10470303 1.20938106 ... 0.71610074 0.         1.48010572]\n",
      " ...\n",
      " [2.17743284 0.54594631 0.93729374 ... 0.77921963 1.66558231 1.12259334]\n",
      " [3.14600004 0.         2.24844725 ... 1.84926742 1.22497865 1.76122413]\n",
      " [0.         0.         0.         ... 0.         0.         0.05478218]] & cached\n",
      "activation = [[0.01109004 0.0266406  0.11919136 ... 0.1172614  0.04182534 0.10585815]\n",
      " [0.39401252 0.00803994 0.15169883 ... 0.14108745 0.12375341 0.12734928]\n",
      " [0.05142929 0.01723181 0.10565297 ... 0.09279868 0.21835688 0.11918129]\n",
      " ...\n",
      " [0.1280904  0.24863538 0.07094355 ... 0.08140399 0.02228098 0.06530121]\n",
      " [0.07883174 0.0176941  0.11558764 ... 0.16231409 0.18478985 0.08573142]\n",
      " [0.10582886 0.19544616 0.09040898 ... 0.08093996 0.07128012 0.13914125]] & cached\n",
      "Re-used Cached Value, runNum =  86\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  86\n",
      "Provided input from cache for runNum = 86\n",
      "Provided input from cache for runNum = 87\n",
      "activation = [[4.403599   5.10301234 4.8264337  ... 7.11188476 3.07943155 2.7756818 ]\n",
      " [0.         2.96281097 0.         ... 0.         0.         0.        ]\n",
      " [0.06989243 3.40457661 1.75583921 ... 1.27158119 3.35699959 1.27312995]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.06769464 0.         0.42116015 ... 0.34617991 4.21049726 0.        ]\n",
      " [1.07262403 3.37350709 0.         ... 1.87011705 0.24860184 0.26032409]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74359996 0.         0.         ... 0.         0.49356034 0.        ]\n",
      " [0.6407938  3.13380531 1.21369054 ... 0.73240433 0.         1.4952561 ]\n",
      " ...\n",
      " [2.19332453 0.55633752 0.94611474 ... 0.77183907 1.69623233 1.12755246]\n",
      " [3.14718314 0.         2.27277907 ... 1.85864148 1.23032243 1.76471721]\n",
      " [0.         0.         0.         ... 0.         0.         0.04967908]] & cached\n",
      "activation = [[0.01089002 0.02583828 0.1192906  ... 0.11874945 0.04066552 0.1059254 ]\n",
      " [0.39411186 0.00775052 0.15278076 ... 0.14025222 0.12357484 0.12693585]\n",
      " [0.05021841 0.01665835 0.10543607 ... 0.09220955 0.21989768 0.11869273]\n",
      " ...\n",
      " [0.12976837 0.24975065 0.0702062  ... 0.08112646 0.02175419 0.065125  ]\n",
      " [0.07810044 0.01707034 0.11609914 ... 0.16184438 0.18387416 0.08529004]\n",
      " [0.10663829 0.19486712 0.08917833 ... 0.08041281 0.0706555  0.13907279]] & cached\n",
      "Re-used Cached Value, runNum =  87\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  87\n",
      "Provided input from cache for runNum = 87\n",
      "Provided input from cache for runNum = 88\n",
      "activation = [[4.4081051  5.10239802 4.83970962 ... 7.12053188 3.08247524 2.77750848]\n",
      " [0.         2.96851575 0.         ... 0.         0.         0.        ]\n",
      " [0.06992946 3.4103602  1.74943064 ... 1.27961404 3.36291173 1.27769702]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.06436896 0.         0.43143718 ... 0.34371361 4.22371957 0.        ]\n",
      " [1.09701621 3.40753389 0.         ... 1.88151935 0.2612485  0.27407501]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74390813 0.         0.         ... 0.         0.50276434 0.        ]\n",
      " [0.65193197 3.16281248 1.21799282 ... 0.74884255 0.         1.51052106]\n",
      " ...\n",
      " [2.20919183 0.56682434 0.95479354 ... 0.76423946 1.72680201 1.13256608]\n",
      " [3.1486894  0.         2.29745056 ... 1.86871077 1.23624151 1.7685094 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.0446083 ]] & cached\n",
      "activation = [[0.01069279 0.02506419 0.11939823 ... 0.12027282 0.03953876 0.10599466]\n",
      " [0.39415953 0.00747137 0.15386522 ... 0.13941931 0.12336426 0.12651366]\n",
      " [0.04902886 0.01610506 0.10520268 ... 0.09160895 0.22144698 0.11820538]\n",
      " ...\n",
      " [0.13145485 0.25081855 0.06946579 ... 0.08084252 0.02123223 0.06494095]\n",
      " [0.07738836 0.01647184 0.11662518 ... 0.16138507 0.18293004 0.08485535]\n",
      " [0.10745335 0.1943292  0.08794661 ... 0.07986235 0.06999467 0.13899794]] & cached\n",
      "Re-used Cached Value, runNum =  88\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  88\n",
      "Provided input from cache for runNum = 88\n",
      "Provided input from cache for runNum = 89\n",
      "activation = [[4.41265609 5.10187312 4.85298458 ... 7.12945902 3.08568357 2.77937576]\n",
      " [0.         2.97413248 0.         ... 0.         0.         0.        ]\n",
      " [0.07009557 3.41626452 1.74315354 ... 1.28769258 3.36894668 1.28231655]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.06107149 0.         0.44164558 ... 0.34102154 4.23682445 0.        ]\n",
      " [1.12153629 3.44162549 0.         ... 1.8930727  0.27408344 0.28792742]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74405502 0.         0.         ... 0.         0.51170983 0.        ]\n",
      " [0.6630038  3.19182384 1.22226645 ... 0.76545425 0.         1.52589618]\n",
      " ...\n",
      " [2.22484512 0.57708088 0.96316579 ... 0.75615664 1.75705401 1.13746744]\n",
      " [3.15001149 0.         2.32215724 ... 1.87881789 1.2420577  1.7722436 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.03954542]] & cached\n",
      "activation = [[0.01049962 0.02431695 0.11952628 ... 0.12184904 0.03845019 0.10607305]\n",
      " [0.39406938 0.00719928 0.15492641 ... 0.13854542 0.12310561 0.12606225]\n",
      " [0.04786068 0.01556788 0.10495351 ... 0.09098951 0.22296666 0.11770802]\n",
      " ...\n",
      " [0.13318774 0.25184827 0.06872575 ... 0.08056551 0.02072409 0.06475775]\n",
      " [0.07670224 0.01589505 0.11716811 ... 0.16092862 0.18196115 0.08442719]\n",
      " [0.10830078 0.19378433 0.08672044 ... 0.0793077  0.06932659 0.13893189]] & cached\n",
      "Re-used Cached Value, runNum =  89\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  89\n",
      "Provided input from cache for runNum = 89\n",
      "Provided input from cache for runNum = 90\n",
      "activation = [[4.41720918 5.10136363 4.86623022 ... 7.13862518 3.08903091 2.78126038]\n",
      " [0.         2.97968351 0.         ... 0.         0.         0.        ]\n",
      " [0.0703592  3.42222376 1.73697264 ... 1.29574475 3.37500697 1.2869609 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.05777786 0.         0.45181568 ... 0.33823349 4.24984612 0.        ]\n",
      " [1.14610479 3.47564025 0.         ... 1.90458004 0.28700673 0.3017894 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74416976 0.         0.         ... 0.         0.52054467 0.        ]\n",
      " [0.67398336 3.22082463 1.22644954 ... 0.78207625 0.         1.5413605 ]\n",
      " ...\n",
      " [2.24033719 0.58722965 0.97133137 ... 0.74774454 1.78712784 1.1423366 ]\n",
      " [3.15120865 0.         2.34691189 ... 1.88911229 1.24790413 1.77599684]\n",
      " [0.         0.         0.         ... 0.         0.         0.03453206]] & cached\n",
      "activation = [[0.01030845 0.0235918  0.11966401 ... 0.12346505 0.03739154 0.10615596]\n",
      " [0.39386931 0.00693442 0.15597554 ... 0.13765119 0.12280725 0.12559028]\n",
      " [0.04671197 0.01504588 0.10469072 ... 0.09035996 0.22446846 0.11720371]\n",
      " ...\n",
      " [0.13496292 0.25283969 0.06798468 ... 0.08028571 0.020226   0.06457326]\n",
      " [0.07603589 0.01533857 0.11773144 ... 0.16049041 0.1809604  0.08400606]\n",
      " [0.10917878 0.19324051 0.08550151 ... 0.07874239 0.06864641 0.13887371]] & cached\n",
      "Re-used Cached Value, runNum =  90\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  90\n",
      "Provided input from cache for runNum = 90\n",
      "Provided input from cache for runNum = 91\n",
      "activation = [[4.42183276 5.10097633 4.87947819 ... 7.14810452 3.09260812 2.78320701]\n",
      " [0.         2.98514547 0.         ... 0.         0.         0.        ]\n",
      " [0.07069539 3.42821248 1.73087191 ... 1.30375855 3.38107398 1.29162459]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0544457  0.         0.46187487 ... 0.33523466 4.26268851 0.        ]\n",
      " [1.17057738 3.50941811 0.         ... 1.91596337 0.2998506  0.31561327]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74414883 0.         0.         ... 0.         0.52911285 0.        ]\n",
      " [0.68502084 3.24986855 1.23073202 ... 0.79897403 0.         1.55701281]\n",
      " ...\n",
      " [2.25577301 0.59741708 0.97938125 ... 0.73912656 1.81707312 1.14727881]\n",
      " [3.15259676 0.         2.3719139  ... 1.89991435 1.25409395 1.77995875]\n",
      " [0.         0.         0.         ... 0.         0.         0.02963431]] & cached\n",
      "activation = [[0.01011993 0.02288775 0.1198095  ... 0.12512086 0.03636461 0.10624276]\n",
      " [0.39358564 0.00667752 0.1570185  ... 0.13674558 0.12247305 0.12510435]\n",
      " [0.04558304 0.01453942 0.1044115  ... 0.08971809 0.22596659 0.11669667]\n",
      " ...\n",
      " [0.13676915 0.25379114 0.06724314 ... 0.08000221 0.01973558 0.06438447]\n",
      " [0.07538725 0.01480177 0.11830653 ... 0.16005374 0.179939   0.08358931]\n",
      " [0.11007647 0.19272117 0.08428993 ... 0.07816471 0.06794218 0.13881999]] & cached\n",
      "Re-used Cached Value, runNum =  91\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  91\n",
      "Provided input from cache for runNum = 91\n",
      "Provided input from cache for runNum = 92\n",
      "activation = [[4.42655552 5.10077223 4.89274171 ... 7.15798906 3.09643672 2.7852453 ]\n",
      " [0.         2.99043874 0.         ... 0.         0.         0.        ]\n",
      " [0.07104705 3.43416827 1.72482264 ... 1.31164702 3.38713181 1.29626482]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.05130589 0.         0.47195306 ... 0.33230921 4.27552332 0.        ]\n",
      " [1.19505021 3.54308623 0.         ... 1.92726904 0.31273705 0.32941983]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74416922 0.         0.         ... 0.         0.53751296 0.        ]\n",
      " [0.69589372 3.27882052 1.23497536 ... 0.81595841 0.         1.57279235]\n",
      " ...\n",
      " [2.27115998 0.60757814 0.98729869 ... 0.73034656 1.84689929 1.15221941]\n",
      " [3.15414328 0.         2.39718208 ... 1.911297   1.26053659 1.78408651]\n",
      " [0.         0.         0.         ... 0.         0.         0.02473108]] & cached\n",
      "activation = [[0.00993243 0.02220422 0.11996061 ... 0.12681242 0.03536591 0.10633189]\n",
      " [0.39325244 0.00642879 0.15806229 ... 0.1358377  0.1221044  0.12460513]\n",
      " [0.04447652 0.01404811 0.10411921 ... 0.08907256 0.22745813 0.1161861 ]\n",
      " ...\n",
      " [0.13858853 0.2547064  0.06649831 ... 0.07970578 0.01925298 0.064191  ]\n",
      " [0.07475891 0.01428459 0.11890082 ... 0.15963772 0.17889255 0.08317856]\n",
      " [0.11098654 0.1922218  0.08308125 ... 0.07756634 0.06721852 0.13876747]] & cached\n",
      "Re-used Cached Value, runNum =  92\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  92\n",
      "Provided input from cache for runNum = 92\n",
      "Provided input from cache for runNum = 93\n",
      "activation = [[4.43131491 5.10065679 4.90600311 ... 7.16818287 3.10047265 2.78733495]\n",
      " [0.         2.99573359 0.         ... 0.         0.         0.        ]\n",
      " [0.07146376 3.44016302 1.71883569 ... 1.31946516 3.39317954 1.30091909]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.04801172 0.         0.48181516 ... 0.32889971 4.28799737 0.        ]\n",
      " [1.21953025 3.57674914 0.         ... 1.93860626 0.32572663 0.34326609]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74403092 0.         0.         ... 0.         0.54557946 0.        ]\n",
      " [0.70719662 3.30831758 1.23967583 ... 0.83377577 0.         1.58897092]\n",
      " ...\n",
      " [2.28632525 0.61763256 0.99495543 ... 0.72111984 1.87641138 1.15718016]\n",
      " [3.15565938 0.         2.42256098 ... 1.92288884 1.26709417 1.78830686]\n",
      " [0.         0.         0.         ... 0.         0.         0.01990939]] & cached\n",
      "activation = [[0.00974918 0.02154251 0.12013459 ... 0.12856758 0.03440278 0.10642845]\n",
      " [0.39276725 0.00618568 0.15907477 ... 0.13488403 0.12169596 0.12407998]\n",
      " [0.04338239 0.01356833 0.10379198 ... 0.08838293 0.22892965 0.11565981]\n",
      " ...\n",
      " [0.1404785  0.25559522 0.06576689 ... 0.07943479 0.01878218 0.06400054]\n",
      " [0.07413462 0.01378138 0.11948294 ... 0.15916503 0.177833   0.0827621 ]\n",
      " [0.11194536 0.1917473  0.08189561 ... 0.07698179 0.06648251 0.1387366 ]] & cached\n",
      "Re-used Cached Value, runNum =  93\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  93\n",
      "Provided input from cache for runNum = 93\n",
      "Provided input from cache for runNum = 94\n",
      "activation = [[4.43610757 5.10064258 4.91924813 ... 7.17866733 3.10469556 2.78946867]\n",
      " [0.         3.00094557 0.         ... 0.         0.         0.        ]\n",
      " [0.07192892 3.44616356 1.7129097  ... 1.3272227  3.39925163 1.3055572 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0447618  0.         0.49158532 ... 0.32536756 4.30034191 0.        ]\n",
      " [1.24379526 3.61007084 0.         ... 1.94965679 0.3385615  0.35698554]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74392348 0.         0.         ... 0.         0.5535181  0.        ]\n",
      " [0.71864761 3.33804892 1.24457785 ... 0.85194849 0.         1.60537458]\n",
      " ...\n",
      " [2.30149409 0.62782434 1.0025293  ... 0.71174467 1.90580418 1.16220537]\n",
      " [3.15735411 0.         2.44811513 ... 1.93498308 1.27394308 1.79271583]\n",
      " [0.         0.         0.         ... 0.         0.         0.01514078]] & cached\n",
      "activation = [[0.0095684  0.02090134 0.12031963 ... 0.13037001 0.03346515 0.10652964]\n",
      " [0.39220665 0.00594963 0.16006548 ... 0.13390747 0.12125757 0.12353634]\n",
      " [0.04231583 0.01310449 0.10345609 ... 0.08769205 0.2303974  0.11513432]\n",
      " ...\n",
      " [0.1423779  0.25642741 0.06503296 ... 0.0791506  0.0183185  0.06380376]\n",
      " [0.07352993 0.01329683 0.12008079 ... 0.15870236 0.17674999 0.08235271]\n",
      " [0.1129258  0.19129976 0.08071996 ... 0.07638255 0.06572731 0.13870734]] & cached\n",
      "Re-used Cached Value, runNum =  94\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  94\n",
      "Provided input from cache for runNum = 94\n",
      "Provided input from cache for runNum = 95\n",
      "activation = [[4.44099016 5.10079876 4.93252275 ... 7.18956688 3.10919963 2.79168415]\n",
      " [0.         3.00599762 0.         ... 0.         0.         0.        ]\n",
      " [0.07242254 3.45214493 1.70703066 ... 1.33486571 3.40530812 1.31017322]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.04165213 0.         0.50136222 ... 0.32192708 4.31266947 0.        ]\n",
      " [1.26799215 3.64328445 0.         ... 1.96059778 0.35138618 0.37067573]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74371374 0.         0.         ... 0.         0.56119042 0.        ]\n",
      " [0.72965094 3.36737157 1.2491841  ... 0.8698443  0.         1.62172985]\n",
      " ...\n",
      " [2.3166152  0.63801615 1.00999218 ... 0.70225706 1.93503952 1.16722257]\n",
      " [3.15920065 0.         2.47389114 ... 1.94765032 1.28108814 1.79725199]\n",
      " [0.         0.         0.         ... 0.         0.         0.01037217]] & cached\n",
      "activation = [[0.00938869 0.02027984 0.1205027  ... 0.13219944 0.03255591 0.10663076]\n",
      " [0.39162152 0.00572241 0.16106631 ... 0.13294323 0.12078729 0.12298511]\n",
      " [0.04127353 0.01265776 0.10311502 ... 0.08700697 0.23185951 0.11461183]\n",
      " ...\n",
      " [0.14428284 0.25720882 0.06429152 ... 0.07884254 0.01786308 0.06359909]\n",
      " [0.07295998 0.01283401 0.12071845 ... 0.15829377 0.17565383 0.0819583 ]\n",
      " [0.11390529 0.19086312 0.07954119 ... 0.07575295 0.06495398 0.138672  ]] & cached\n",
      "Re-used Cached Value, runNum =  95\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  95\n",
      "Provided input from cache for runNum = 95\n",
      "Provided input from cache for runNum = 96\n",
      "activation = [[4.44589018 5.10102481 4.94576627 ... 7.20070805 3.11386406 2.79392808]\n",
      " [0.         3.01095254 0.         ... 0.         0.         0.        ]\n",
      " [0.07291007 3.45807392 1.70116983 ... 1.34235459 3.41132154 1.31474731]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.03842695 0.         0.51096271 ... 0.31810498 4.32465671 0.        ]\n",
      " [1.29200022 3.6762265  0.         ... 1.97133725 0.36410663 0.38427234]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74345618 0.         0.         ... 0.         0.56860502 0.        ]\n",
      " [0.74086015 3.39691404 1.254058   ... 0.88832615 0.         1.63837263]\n",
      " ...\n",
      " [2.33167973 0.64836452 1.0173859  ... 0.69263188 1.96405086 1.17237418]\n",
      " [3.16117568 0.         2.49980506 ... 1.96073727 1.28841585 1.80197754]\n",
      " [0.         0.         0.         ... 0.         0.         0.00570201]] & cached\n",
      "activation = [[0.00921107 0.01967048 0.12069156 ... 0.1340734  0.03167545 0.10673277]\n",
      " [0.39094968 0.0055038  0.16204988 ... 0.13195481 0.12028439 0.1224176 ]\n",
      " [0.04024874 0.01219777 0.10275291 ... 0.08630031 0.23330938 0.11408607]\n",
      " ...\n",
      " [0.14622522 0.25844606 0.06355567 ... 0.07853786 0.0174174  0.06339036]\n",
      " [0.07239973 0.01239386 0.12136252 ... 0.15786489 0.17454547 0.08156641]\n",
      " [0.11491695 0.19046988 0.07838147 ... 0.0751247  0.06416847 0.13864787]] & cached\n",
      "Re-used Cached Value, runNum =  96\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  96\n",
      "Provided input from cache for runNum = 96\n",
      "Provided input from cache for runNum = 97\n",
      "activation = [[4.45093062 5.10149708 4.95905632 ... 7.21232215 3.11885243 2.79629057]\n",
      " [0.         3.01584575 0.         ... 0.         0.         0.        ]\n",
      " [0.07339501 3.46389688 1.69533543 ... 1.34966658 3.41728801 1.31927417]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.03531497 0.         0.52052613 ... 0.31423968 4.33660398 0.        ]\n",
      " [1.31582283 3.70888561 0.         ... 1.98181648 0.37674594 0.39775454]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.43252350e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.75861035e-01 0.00000000e+00]\n",
      " [7.51863480e-01 3.42633928e+00 1.25886331e+00 ... 9.06869554e-01\n",
      "  0.00000000e+00 1.65513627e+00]\n",
      " ...\n",
      " [2.34686034e+00 6.59020747e-01 1.02486690e+00 ... 6.83110467e-01\n",
      "  1.99309063e+00 1.17771925e+00]\n",
      " [3.16351372e+00 0.00000000e+00 2.52608426e+00 ... 1.97469282e+00\n",
      "  1.29622259e+00 1.80704668e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.08690027e-03]] & cached\n",
      "activation = [[0.0090339  0.01907624 0.12087129 ... 0.13597338 0.03081558 0.10682998]\n",
      " [0.39028133 0.00529364 0.16304404 ... 0.13098033 0.11975689 0.12184736]\n",
      " [0.03925078 0.01174839 0.10239132 ... 0.08560529 0.23476931 0.11357002]\n",
      " ...\n",
      " [0.14815019 0.25976444 0.06280972 ... 0.07820307 0.01697631 0.06316924]\n",
      " [0.07186382 0.01197406 0.12203824 ... 0.15747296 0.17342007 0.08118594]\n",
      " [0.11592848 0.19011827 0.07722336 ... 0.07446793 0.06335935 0.13861686]] & cached\n",
      "Re-used Cached Value, runNum =  97\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  97\n",
      "Provided input from cache for runNum = 97\n",
      "Provided input from cache for runNum = 98\n",
      "activation = [[4.45596017 5.1019829  4.97231313 ... 7.22417023 3.12398939 2.7986658 ]\n",
      " [0.         3.0207304  0.         ... 0.         0.         0.        ]\n",
      " [0.07406177 3.46991037 1.68965751 ... 1.3571185  3.42344666 1.32389202]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.03219611 0.         0.53001944 ... 0.31024173 4.34841423 0.        ]\n",
      " [1.33978276 3.74169648 0.         ... 1.99245505 0.3896194  0.41132993]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74282074 0.         0.         ... 0.         0.58266894 0.        ]\n",
      " [0.76296454 3.4559102  1.26377664 ... 0.92576054 0.         1.67206975]\n",
      " ...\n",
      " [2.36189828 0.66966251 1.03222053 ... 0.67335187 2.02187205 1.18306648]\n",
      " [3.16557433 0.         2.55232594 ... 1.98859412 1.30381758 1.81199462]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00885904 0.01849632 0.12105605 ... 0.13791846 0.02998624 0.10707985]\n",
      " [0.38945779 0.00508882 0.16401661 ... 0.12996598 0.11918506 0.12119376]\n",
      " [0.03826734 0.01131239 0.1020113  ... 0.08489186 0.23619752 0.11303985]\n",
      " ...\n",
      " [0.1501554  0.26103803 0.06207014 ... 0.07787396 0.01654915 0.06298025]\n",
      " [0.07134299 0.01156724 0.12272824 ... 0.15707451 0.17229699 0.08082797]\n",
      " [0.11699531 0.18976977 0.07608634 ... 0.07382153 0.06255538 0.13862625]] & cached\n",
      "Re-used Cached Value, runNum =  98\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  98\n",
      "Provided input from cache for runNum = 98\n",
      "Provided input from cache for runNum = 99\n",
      "activation = [[4.4611194  5.10269893 4.98561231 ... 7.23644876 3.12942793 2.80114526]\n",
      " [0.         3.02545439 0.         ... 0.         0.         0.        ]\n",
      " [0.07469543 3.47580007 1.68398619 ... 1.36437706 3.42951865 1.3284554 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02902561 0.         0.53942854 ... 0.30601805 4.36002896 0.        ]\n",
      " [1.36364395 3.77433595 0.         ... 2.00299712 0.4024687  0.42485881]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7423069  0.         0.         ... 0.         0.58923434 0.        ]\n",
      " [0.77437845 3.48580907 1.26886933 ... 0.94522731 0.         1.68932319]\n",
      " ...\n",
      " [2.3769024  0.68047379 1.03941717 ... 0.66350325 2.05051767 1.18855173]\n",
      " [3.16791595 0.         2.57893017 ... 2.00312737 1.31183459 1.81721353]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00868711 0.01793468 0.12123556 ... 0.13991268 0.02918138 0.1073767 ]\n",
      " [0.38854884 0.00489038 0.16501715 ... 0.12893052 0.11858994 0.12050906]\n",
      " [0.03730317 0.01089073 0.10157411 ... 0.08416088 0.23762901 0.11250859]\n",
      " ...\n",
      " [0.15219657 0.26228643 0.06135018 ... 0.07753994 0.01612887 0.06279315]\n",
      " [0.07083595 0.01117516 0.1234107  ... 0.15666916 0.17116611 0.08047798]\n",
      " [0.1180864  0.18946181 0.07496875 ... 0.07316451 0.06173281 0.13864476]] & cached\n",
      "Re-used Cached Value, runNum =  99\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  99\n",
      "Provided input from cache for runNum = 99\n",
      "Provided input from cache for runNum = 100\n",
      "activation = [[4.46631194 5.10347072 4.99889529 ... 7.24897716 3.13501228 2.80366294]\n",
      " [0.         3.03027622 0.         ... 0.         0.         0.        ]\n",
      " [0.07538037 3.48171655 1.67837867 ... 1.37157136 3.43565636 1.33302313]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02590513 0.         0.54875594 ... 0.30166643 4.37154379 0.        ]\n",
      " [1.38753983 3.80702864 0.         ... 2.01362636 0.41541737 0.43844325]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74167539 0.         0.         ... 0.         0.59549431 0.        ]\n",
      " [0.78560534 3.51565443 1.27352578 ... 0.96482136 0.         1.70666552]\n",
      " ...\n",
      " [2.39184    0.69134996 1.0461283  ... 0.65352788 2.07895732 1.19407921]\n",
      " [3.17024616 0.         2.60580982 ... 2.01789833 1.31994545 1.82245765]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00851645 0.01738732 0.12137862 ... 0.14194557 0.02840216 0.10767262]\n",
      " [0.38756233 0.00469771 0.16614176 ... 0.12787521 0.1179647  0.11980861]\n",
      " [0.03635965 0.01048387 0.10100664 ... 0.08342319 0.23904704 0.11197633]\n",
      " ...\n",
      " [0.15427776 0.26347967 0.06068037 ... 0.07719652 0.01571872 0.06260082]\n",
      " [0.07035478 0.01079815 0.12406183 ... 0.15628608 0.17003278 0.08013827]\n",
      " [0.11920063 0.18914955 0.07388467 ... 0.0725     0.06090532 0.13866938]] & cached\n",
      "Re-used Cached Value, runNum =  100\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  100\n",
      "Provided input from cache for runNum = 100\n",
      "Provided input from cache for runNum = 101\n",
      "activation = [[4.47158336 5.10436439 5.01218165 ... 7.26185759 3.14081454 2.80623175]\n",
      " [0.         3.03498808 0.         ... 0.         0.         0.        ]\n",
      " [0.07615313 3.4876977  1.67286158 ... 1.37877597 3.44188228 1.33762192]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02293684 0.         0.55807987 ... 0.29737009 4.38309007 0.        ]\n",
      " [1.41140305 3.8396271  0.         ... 2.02420343 0.42840188 0.45203976]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74099291 0.         0.         ... 0.         0.60153608 0.        ]\n",
      " [0.79651318 3.54526356 1.27800254 ... 0.9843154  0.         1.72400526]\n",
      " ...\n",
      " [2.40669823 0.70224225 1.05273478 ... 0.64340765 2.10720468 1.19958553]\n",
      " [3.17269629 0.         2.63289778 ... 2.03317422 1.32827509 1.82778501]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00834732 0.01685668 0.121522   ... 0.14402005 0.02764632 0.10797071]\n",
      " [0.38654684 0.00451201 0.16727023 ... 0.12681762 0.11731612 0.11909805]\n",
      " [0.03544135 0.01009262 0.1004395  ... 0.08269312 0.24045917 0.11144533]\n",
      " ...\n",
      " [0.15636503 0.26462459 0.06000146 ... 0.07682824 0.01531716 0.06240175]\n",
      " [0.06990116 0.01043706 0.12474922 ... 0.15594226 0.16889329 0.07981142]\n",
      " [0.12032216 0.18885041 0.07280112 ... 0.07181226 0.06006918 0.13869099]] & cached\n",
      "Re-used Cached Value, runNum =  101\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  101\n",
      "Provided input from cache for runNum = 101\n",
      "Provided input from cache for runNum = 102\n",
      "activation = [[4.47690108 5.10534334 5.02544361 ... 7.27499112 3.14679641 2.80883827]\n",
      " [0.         3.03964438 0.         ... 0.         0.         0.        ]\n",
      " [0.07695278 3.4936506  1.66740797 ... 1.38589584 3.44810343 1.34221715]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02009859 0.         0.56739431 ... 0.29311263 4.39464296 0.        ]\n",
      " [1.43512918 3.87200114 0.         ... 2.03462673 0.44130938 0.465574  ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74033444 0.         0.         ... 0.         0.60741636 0.        ]\n",
      " [0.8072429  3.57477595 1.28242836 ... 1.00387105 0.         1.74143114]\n",
      " ...\n",
      " [2.42154705 0.71329339 1.05931152 ... 0.63327491 2.13532547 1.20515803]\n",
      " [3.17527418 0.         2.6601348  ... 2.04891964 1.33687225 1.83322599]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0081801  0.01634539 0.1216628  ... 0.14612886 0.02691098 0.10826653]\n",
      " [0.38551602 0.00433371 0.16839491 ... 0.12575487 0.11664724 0.11837761]\n",
      " [0.03454885 0.00971836 0.09987207 ... 0.08196891 0.24187035 0.11091514]\n",
      " ...\n",
      " [0.15843925 0.26571129 0.05931615 ... 0.07643834 0.01492304 0.06219596]\n",
      " [0.06946966 0.01009219 0.12546684 ... 0.1556261  0.16774509 0.07949392]\n",
      " [0.12145545 0.18859468 0.0717254  ... 0.07111047 0.05922345 0.13871517]] & cached\n",
      "Re-used Cached Value, runNum =  102\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  102\n",
      "Provided input from cache for runNum = 102\n",
      "Provided input from cache for runNum = 103\n",
      "activation = [[4.48232553 5.10651011 5.03871513 ... 7.2885361  3.15306783 2.81153066]\n",
      " [0.         3.04421248 0.         ... 0.         0.         0.        ]\n",
      " [0.07770122 3.49949212 1.66195636 ... 1.39280526 3.45423934 1.34674783]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.01719768 0.         0.5765696  ... 0.28861259 4.40596606 0.        ]\n",
      " [1.45870672 3.9041677  0.         ... 2.04487404 0.45411475 0.47904348]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73974022 0.         0.         ... 0.         0.61316339 0.        ]\n",
      " [0.81814709 3.60454819 1.28707769 ... 1.0238495  0.         1.75910032]\n",
      " ...\n",
      " [2.43621197 0.72428374 1.06571093 ... 0.62284342 2.16312637 1.21074896]\n",
      " [3.17828273 0.         2.68777758 ... 2.06551243 1.34609342 1.83898576]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00801528 0.01584989 0.12181734 ... 0.1483057  0.02620019 0.10856917]\n",
      " [0.38444361 0.00416082 0.16951398 ... 0.12468116 0.11597113 0.11764785]\n",
      " [0.03367409 0.00935515 0.09928639 ... 0.08122742 0.2432929  0.11038391]\n",
      " ...\n",
      " [0.16054028 0.26678603 0.05863135 ... 0.07603693 0.01453555 0.06198308]\n",
      " [0.06904621 0.00975851 0.1261952  ... 0.15530238 0.1665917  0.07918013]\n",
      " [0.12259729 0.18836102 0.07065445 ... 0.07038583 0.05835784 0.13873372]] & cached\n",
      "Re-used Cached Value, runNum =  103\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  103\n",
      "Provided input from cache for runNum = 103\n",
      "iterations = 100\n",
      "Accuracy = 0.40075609756097563\n",
      "Provided input from cache for runNum = 104\n",
      "activation = [[4.48776262 5.10771664 5.05196784 ... 7.30232514 3.15945704 2.81424726]\n",
      " [0.         3.04876064 0.         ... 0.         0.         0.        ]\n",
      " [0.07852088 3.50538638 1.65657221 ... 1.39969938 3.46043628 1.35129584]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.01432847 0.         0.58569452 ... 0.28403366 4.4172103  0.        ]\n",
      " [1.48231827 3.93636958 0.         ... 2.05516876 0.46704006 0.49258118]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73922057 0.         0.         ... 0.         0.6188201  0.        ]\n",
      " [0.82908512 3.6344551  1.29183101 ... 1.04414427 0.         1.77699553]\n",
      " ...\n",
      " [2.45076221 0.73528435 1.07203589 ... 0.61226762 2.19069349 1.21640254]\n",
      " [3.18124091 0.         2.71552754 ... 2.08232269 1.3553402  1.84475819]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00785022 0.0153638  0.1219689  ... 0.1505264  0.02550933 0.1088686 ]\n",
      " [0.38328456 0.00399268 0.17062949 ... 0.12358434 0.11527513 0.11690172]\n",
      " [0.03281122 0.00900156 0.09868092 ... 0.0804705  0.24470206 0.10984271]\n",
      " ...\n",
      " [0.16270815 0.26783434 0.05795163 ... 0.07563291 0.01415767 0.06176951]\n",
      " [0.06862576 0.00943415 0.12693786 ... 0.15497483 0.16542944 0.07886773]\n",
      " [0.12378109 0.18813018 0.06960152 ... 0.06966381 0.05749426 0.13877141]] & cached\n",
      "Re-used Cached Value, runNum =  104\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  104\n",
      "Provided input from cache for runNum = 104\n",
      "Provided input from cache for runNum = 105\n",
      "activation = [[4.49322518 5.10897705 5.06518734 ... 7.31638002 3.16601069 2.81699772]\n",
      " [0.         3.05336537 0.         ... 0.         0.         0.        ]\n",
      " [0.07936065 3.5112606  1.6512418  ... 1.40649454 3.46662905 1.35582745]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0115724  0.         0.59479632 ... 0.27953168 4.42838597 0.        ]\n",
      " [1.50586004 3.96840783 0.         ... 2.06530106 0.47996219 0.50606015]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73869272 0.         0.         ... 0.         0.62427083 0.        ]\n",
      " [0.83974558 3.6642071  1.29646477 ... 1.06437821 0.         1.7949495 ]\n",
      " ...\n",
      " [2.46539873 0.74648003 1.07844396 ... 0.60186625 2.21810691 1.22219065]\n",
      " [3.18437241 0.         2.74351261 ... 2.09975094 1.36485544 1.8507114 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00768486 0.01488852 0.12210415 ... 0.15276727 0.02483841 0.10916069]\n",
      " [0.38211207 0.00383038 0.17175951 ... 0.12249795 0.11456236 0.11615237]\n",
      " [0.03196943 0.00866096 0.09807579 ... 0.07972924 0.24611225 0.10930616]\n",
      " ...\n",
      " [0.16488878 0.26882716 0.05726477 ... 0.07519767 0.01378756 0.06154664]\n",
      " [0.06822433 0.00912229 0.12771339 ... 0.15468888 0.16426687 0.07856605]\n",
      " [0.12497869 0.18791089 0.06855657 ... 0.06892434 0.05662454 0.13880905]] & cached\n",
      "Re-used Cached Value, runNum =  105\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  105\n",
      "Provided input from cache for runNum = 105\n",
      "Provided input from cache for runNum = 106\n",
      "activation = [[4.49870024 5.11028081 5.07835913 ... 7.33066265 3.17268492 2.81976932]\n",
      " [0.         3.05785437 0.         ... 0.         0.         0.        ]\n",
      " [0.08027198 3.51719596 1.64599581 ... 1.41331133 3.47293431 1.36039129]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00867164 0.         0.60368837 ... 0.27465335 4.43917816 0.        ]\n",
      " [1.52942547 4.00043804 0.         ... 2.07549454 0.49302333 0.51959181]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73798794 0.         0.         ... 0.         0.62928587 0.        ]\n",
      " [0.85063844 3.69414576 1.30140651 ... 1.08522059 0.         1.81315238]\n",
      " ...\n",
      " [2.47982892 0.75763617 1.08464963 ... 0.59111476 2.24506026 1.2279733 ]\n",
      " [3.18736793 0.         2.77150462 ... 2.11727204 1.37429589 1.85663596]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00752129 0.014424   0.1222467  ... 0.15506944 0.02419575 0.10945343]\n",
      " [0.38080606 0.00367298 0.17287197 ... 0.12137416 0.11382624 0.11538406]\n",
      " [0.03113538 0.00832921 0.09743707 ... 0.07895015 0.24749929 0.10875461]\n",
      " ...\n",
      " [0.16716501 0.26980934 0.05659204 ... 0.07477695 0.0134307  0.06132566]\n",
      " [0.06782138 0.00881898 0.12848438 ... 0.15435957 0.16312205 0.07826322]\n",
      " [0.12623197 0.18770648 0.06753822 ... 0.06819905 0.05576388 0.13886853]] & cached\n",
      "Re-used Cached Value, runNum =  106\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  106\n",
      "Provided input from cache for runNum = 106\n",
      "Provided input from cache for runNum = 107\n",
      "activation = [[4.50431213 5.11182385 5.09157729 ... 7.34538808 3.17963198 2.82265853]\n",
      " [0.         3.06226301 0.         ... 0.         0.         0.        ]\n",
      " [0.08119973 3.52307792 1.64081641 ... 1.41999996 3.47926082 1.36492517]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00597823 0.         0.61265428 ... 0.26999449 4.45006767 0.        ]\n",
      " [1.55294236 4.03231472 0.         ... 2.08558667 0.50608462 0.5330827 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73721515 0.         0.         ... 0.         0.63403374 0.        ]\n",
      " [0.86105208 3.72366247 1.30600713 ... 1.10574913 0.         1.83132422]\n",
      " ...\n",
      " [2.49439964 0.7690596  1.09100912 ... 0.5806459  2.27198879 1.23391943]\n",
      " [3.19059258 0.         2.79974554 ... 2.1354714  1.38406324 1.86275864]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00735778 0.01397202 0.12236259 ... 0.15737849 0.02356992 0.10973533]\n",
      " [0.37952739 0.00352242 0.1740114  ... 0.12027318 0.11307433 0.11461797]\n",
      " [0.03032435 0.00801121 0.09680637 ... 0.07819435 0.24889577 0.10821254]\n",
      " ...\n",
      " [0.16942932 0.27075296 0.05590793 ... 0.07431759 0.01308016 0.06109275]\n",
      " [0.067447   0.00853009 0.12930688 ... 0.15409911 0.16198208 0.07797488]\n",
      " [0.12748218 0.18753093 0.06652137 ... 0.06744886 0.05489594 0.13892309]] & cached\n",
      "Re-used Cached Value, runNum =  107\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  107\n",
      "Provided input from cache for runNum = 107\n",
      "Provided input from cache for runNum = 108\n",
      "activation = [[4.50999344 5.11349836 5.10479214 ... 7.36045826 3.18681671 2.82559099]\n",
      " [0.         3.06658839 0.         ... 0.         0.         0.        ]\n",
      " [0.08216212 3.52891349 1.63568571 ... 1.42655692 3.48557718 1.36942036]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00333887 0.         0.62156474 ... 0.26529605 4.46087064 0.        ]\n",
      " [1.57623849 4.06385262 0.         ... 2.09536547 0.51897425 0.54644515]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73649998 0.         0.         ... 0.         0.63871208 0.        ]\n",
      " [0.87142948 3.7532705  1.31254059 ... 1.12654687 0.         1.84969096]\n",
      " ...\n",
      " [2.50891477 0.78058208 1.09938416 ... 0.57014177 2.29870785 1.23994138]\n",
      " [3.1941264  0.         2.83005638 ... 2.15450391 1.394322   1.86916963]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00719622 0.01353413 0.12237913 ... 0.15974287 0.02296192 0.11001987]\n",
      " [0.37822594 0.00337709 0.17524348 ... 0.11916241 0.11231849 0.11384273]\n",
      " [0.02953543 0.00770523 0.09623743 ... 0.0774438  0.25030629 0.1076742 ]\n",
      " ...\n",
      " [0.17169991 0.27165086 0.05517332 ... 0.07382991 0.01273566 0.06085042]\n",
      " [0.06708744 0.00825215 0.13001957 ... 0.15385706 0.16083969 0.07769356]\n",
      " [0.1287414  0.18737829 0.06554059 ... 0.06667475 0.05401756 0.13897114]] & cached\n",
      "Re-used Cached Value, runNum =  108\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  108\n",
      "Provided input from cache for runNum = 108\n",
      "Provided input from cache for runNum = 109\n",
      "activation = [[4.51576287 5.11533296 5.11801476 ... 7.3758908  3.19425361 2.82859651]\n",
      " [0.         3.07086496 0.         ... 0.         0.         0.        ]\n",
      " [0.08313797 3.53472741 1.63060463 ... 1.43300976 3.49190392 1.37389877]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00087455 0.         0.63049632 ... 0.26081715 4.47173514 0.        ]\n",
      " [1.59942552 4.09526484 0.         ... 2.10505669 0.53185184 0.55978237]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7357358  0.         0.         ... 0.         0.64315905 0.        ]\n",
      " [0.88170995 3.78287104 1.32149157 ... 1.14746082 0.         1.86820021]\n",
      " ...\n",
      " [2.5234328  0.79214355 1.1104974  ... 0.55970817 2.32526299 1.24602972]\n",
      " [3.19780237 0.         2.86285427 ... 2.17405673 1.40485396 1.87569264]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00703612 0.01310765 0.12225325 ... 0.16214226 0.02237211 0.11030212]\n",
      " [0.37688125 0.0032368  0.17660543 ... 0.11804244 0.11155065 0.11305632]\n",
      " [0.02876518 0.00740913 0.09574673 ... 0.0766967  0.2517214  0.1071334 ]\n",
      " ...\n",
      " [0.17399632 0.27253022 0.05437818 ... 0.07332046 0.01239883 0.06060296]\n",
      " [0.06674395 0.00798399 0.13058158 ... 0.15364253 0.15970418 0.0774181 ]\n",
      " [0.13002005 0.1872398  0.06460896 ... 0.06588839 0.05313718 0.13902553]] & cached\n",
      "Re-used Cached Value, runNum =  109\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  109\n",
      "Provided input from cache for runNum = 109\n",
      "Provided input from cache for runNum = 110\n",
      "activation = [[4.52163674 5.11735552 5.13125647 ... 7.39173421 3.20197996 2.83168735]\n",
      " [0.         3.0750331  0.         ... 0.         0.         0.        ]\n",
      " [0.08409254 3.54047246 1.62553163 ... 1.43927264 3.49818616 1.37832684]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99856453 0.         0.63941875 ... 0.25641947 4.48251942 0.        ]\n",
      " [1.62257883 4.12657535 0.         ... 2.11463033 0.54472365 0.57311238]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73504725 0.         0.         ... 0.         0.64745724 0.        ]\n",
      " [0.89197717 3.81259374 1.33055958 ... 1.16865435 0.         1.88693465]\n",
      " ...\n",
      " [2.53816128 0.80404542 1.1218337  ... 0.54958676 2.35177078 1.25232428]\n",
      " [3.20175152 0.         2.89595476 ... 2.19434668 1.41586026 1.88244765]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0068763  0.0126912  0.12210786 ... 0.16456443 0.02179717 0.11057604]\n",
      " [0.37552313 0.00310161 0.17797254 ... 0.11692101 0.11077605 0.11226231]\n",
      " [0.02801537 0.00712357 0.09525234 ... 0.07595927 0.25315465 0.10659463]\n",
      " ...\n",
      " [0.17629583 0.27337012 0.05358138 ... 0.0727831  0.01206733 0.06034683]\n",
      " [0.06641085 0.00772525 0.13116459 ... 0.15345009 0.15857075 0.07714703]\n",
      " [0.13131898 0.1871397  0.06368692 ... 0.06509083 0.05224834 0.1390852 ]] & cached\n",
      "Re-used Cached Value, runNum =  110\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  110\n",
      "Provided input from cache for runNum = 110\n",
      "Provided input from cache for runNum = 111\n",
      "activation = [[4.52750644 5.11938454 5.14444352 ... 7.40776996 3.20983466 2.83477392]\n",
      " [0.         3.07921423 0.         ... 0.         0.         0.        ]\n",
      " [0.08512758 3.54627504 1.62054497 ... 1.44552374 3.50452956 1.38277142]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99649309 0.         0.64840968 ... 0.25226825 4.49341281 0.        ]\n",
      " [1.64567925 4.15783262 0.         ... 2.12419409 0.5576506  0.58645369]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73440099 0.         0.         ... 0.         0.65162014 0.        ]\n",
      " [0.90198058 3.84221294 1.33952562 ... 1.18983758 0.         1.90572358]\n",
      " ...\n",
      " [2.55294268 0.81603828 1.13326916 ... 0.53963829 2.37818988 1.25868094]\n",
      " [3.20563551 0.         2.92904348 ... 2.21484454 1.42692637 1.88914597]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00671748 0.01228577 0.1219471  ... 0.1670093  0.02123663 0.11084491]\n",
      " [0.37412158 0.00297107 0.17932925 ... 0.11578363 0.10998754 0.11145253]\n",
      " [0.02728744 0.00684891 0.09475899 ... 0.07523367 0.25458512 0.10605349]\n",
      " ...\n",
      " [0.17861026 0.27415532 0.05278529 ... 0.07222438 0.01174375 0.06008728]\n",
      " [0.06609716 0.00747633 0.13178258 ... 0.15329565 0.15743838 0.07688484]\n",
      " [0.13264384 0.18703962 0.06277666 ... 0.06429055 0.05136559 0.13915682]] & cached\n",
      "Re-used Cached Value, runNum =  111\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  111\n",
      "Provided input from cache for runNum = 111\n",
      "Provided input from cache for runNum = 112\n",
      "activation = [[4.53353395 5.1216387  5.15763571 ... 7.42422639 3.21803187 2.83794702]\n",
      " [0.         3.08331978 0.         ... 0.         0.         0.        ]\n",
      " [0.08617221 3.55206215 1.61561869 ... 1.4516934  3.51088304 1.38720004]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99456755 0.         0.65740531 ... 0.24813752 4.50423799 0.        ]\n",
      " [1.66854959 4.18880675 0.         ... 2.13350641 0.57043504 0.59969731]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73367109 0.         0.         ... 0.         0.65549312 0.        ]\n",
      " [0.91172105 3.87165325 1.34841701 ... 1.2111188  0.         1.92457608]\n",
      " ...\n",
      " [2.56791862 0.82841029 1.14493107 ... 0.53002468 2.40458888 1.26523768]\n",
      " [3.20974657 0.         2.96224714 ... 2.23596561 1.43835645 1.89601007]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00656034 0.01189355 0.12176419 ... 0.16947141 0.02069152 0.11110332]\n",
      " [0.37273126 0.00284628 0.18068365 ... 0.11464554 0.10918881 0.11063766]\n",
      " [0.02658217 0.00658698 0.09427149 ... 0.0745229  0.25602938 0.10551935]\n",
      " ...\n",
      " [0.18090528 0.27488647 0.05198664 ... 0.07163623 0.01142652 0.05981777]\n",
      " [0.06580652 0.00723925 0.13243672 ... 0.15317774 0.1563173  0.07663311]\n",
      " [0.13397357 0.18698391 0.06187553 ... 0.06347888 0.05047981 0.13922885]] & cached\n",
      "Re-used Cached Value, runNum =  112\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  112\n",
      "Provided input from cache for runNum = 112\n",
      "Provided input from cache for runNum = 113\n",
      "activation = [[4.53962355 5.12400311 5.17081212 ... 7.44100504 3.22643541 2.84117556]\n",
      " [0.         3.08744153 0.         ... 0.         0.         0.        ]\n",
      " [0.08723803 3.55782236 1.61075738 ... 1.45778938 3.51726413 1.3916049 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99272358 0.         0.66639986 ... 0.24401274 4.51500097 0.        ]\n",
      " [1.69127263 4.21954942 0.         ... 2.14261574 0.58314092 0.61286639]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73283233 0.         0.         ... 0.         0.65911746 0.        ]\n",
      " [0.92132667 3.90109374 1.35735382 ... 1.2326612  0.         1.94357728]\n",
      " ...\n",
      " [2.58279607 0.8407045  1.15656976 ... 0.52035655 2.43068295 1.27183518]\n",
      " [3.21392509 0.         2.99551016 ... 2.25757093 1.44997496 1.9030094 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00640545 0.011514   0.12157608 ... 0.17198043 0.02016571 0.11136168]\n",
      " [0.37129133 0.002726   0.18202503 ... 0.11349146 0.10838378 0.10981271]\n",
      " [0.02589157 0.0063341  0.09377022 ... 0.07380064 0.25747308 0.10497978]\n",
      " ...\n",
      " [0.18323888 0.27560386 0.05119561 ... 0.07103726 0.01111838 0.05954457]\n",
      " [0.06552717 0.00701075 0.13310959 ... 0.15306175 0.15521034 0.07638473]\n",
      " [0.13532238 0.18693903 0.06098747 ... 0.06266119 0.04959958 0.13930595]] & cached\n",
      "Re-used Cached Value, runNum =  113\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  113\n",
      "Provided input from cache for runNum = 113\n",
      "Provided input from cache for runNum = 114\n",
      "activation = [[4.54570047 5.12638395 5.18393483 ... 7.45795569 3.23495747 2.84440012]\n",
      " [0.         3.09150032 0.         ... 0.         0.         0.        ]\n",
      " [0.08837946 3.56365472 1.60598501 ... 1.46390344 3.52374459 1.3960456 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99084302 0.         0.67530499 ... 0.23976629 4.52561505 0.        ]\n",
      " [1.71405471 4.25033173 0.         ... 2.15178206 0.59594357 0.62609402]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73188397 0.         0.         ... 0.         0.66245639 0.        ]\n",
      " [0.93107218 3.93067168 1.36650386 ... 1.25465495 0.         1.96279399]\n",
      " ...\n",
      " [2.59760009 0.85307698 1.16820542 ... 0.51063881 2.4565095  1.27850138]\n",
      " [3.21789859 0.         3.02872448 ... 2.27927547 1.46154376 1.90994205]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00625165 0.01114175 0.12138296 ... 0.17453275 0.01965787 0.11161496]\n",
      " [0.36971563 0.00260928 0.18334333 ... 0.1123049  0.10756751 0.10896952]\n",
      " [0.02520975 0.00608849 0.09324905 ... 0.07305815 0.25890597 0.10442762]\n",
      " ...\n",
      " [0.1856744  0.27629444 0.05041741 ... 0.07044077 0.0108202  0.05927257]\n",
      " [0.06524449 0.00678842 0.1337886  ... 0.15292503 0.15411821 0.07613532]\n",
      " [0.13672869 0.18688787 0.06012128 ... 0.06185611 0.04873254 0.13940649]] & cached\n",
      "Re-used Cached Value, runNum =  114\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  114\n",
      "Provided input from cache for runNum = 114\n",
      "Provided input from cache for runNum = 115\n",
      "activation = [[4.55181953 5.12885719 5.19702805 ... 7.47520676 3.2436636  2.84766655]\n",
      " [0.         3.09547655 0.         ... 0.         0.         0.        ]\n",
      " [0.08947637 3.56936004 1.60122037 ... 1.46983148 3.5301653  1.40042863]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98895917 0.         0.68413118 ... 0.23535107 4.53610279 0.        ]\n",
      " [1.73685974 4.28115255 0.         ... 2.16099665 0.60886243 0.6393842 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73093689 0.         0.         ... 0.         0.66563813 0.        ]\n",
      " [0.94087369 3.96036417 1.37581422 ... 1.27704396 0.         1.98225445]\n",
      " ...\n",
      " [2.61250774 0.86574163 1.18001775 ... 0.50106235 2.48223223 1.28536206]\n",
      " [3.22204094 0.         3.06218123 ... 2.30151031 1.47339139 1.91703434]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.006097   0.0107719  0.121169   ... 0.17711422 0.0191637  0.11185796]\n",
      " [0.36807178 0.00249603 0.18467127 ... 0.1111102  0.10674802 0.10811945]\n",
      " [0.02453555 0.0058481  0.09271076 ... 0.07230182 0.26034935 0.10387065]\n",
      " ...\n",
      " [0.18819655 0.27698459 0.04964574 ... 0.06983515 0.01052852 0.05899545]\n",
      " [0.06495478 0.00657091 0.13447678 ... 0.15278086 0.15303301 0.07588567]\n",
      " [0.13816932 0.18683423 0.05926962 ... 0.06105148 0.04786739 0.13951978]] & cached\n",
      "Re-used Cached Value, runNum =  115\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  115\n",
      "Provided input from cache for runNum = 115\n",
      "Provided input from cache for runNum = 116\n",
      "activation = [[4.55795409 5.1313433  5.21007027 ... 7.4926411  3.25249767 2.85093311]\n",
      " [0.         3.09957499 0.         ... 0.         0.         0.        ]\n",
      " [0.09065985 3.5751653  1.59655079 ... 1.47583256 3.53674078 1.40486292]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98706783 0.         0.69292459 ... 0.23083501 4.5464185  0.        ]\n",
      " [1.75980844 4.31221125 0.         ... 2.1705011  0.62202674 0.65283961]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72991335 0.         0.         ... 0.         0.66854575 0.        ]\n",
      " [0.95071958 3.99024713 1.38519366 ... 1.29976984 0.         2.00190707]\n",
      " ...\n",
      " [2.6274697  0.87860261 1.19192973 ... 0.49155259 2.50775589 1.29234376]\n",
      " [3.22585155 0.         3.09547929 ... 2.32350185 1.48500293 1.92392746]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0059432  0.01041036 0.12093867 ... 0.17972142 0.01868484 0.11208955]\n",
      " [0.36631739 0.00238644 0.18598211 ... 0.10987965 0.10591503 0.10724933]\n",
      " [0.02387227 0.00561587 0.09216077 ... 0.07153083 0.261776   0.10330126]\n",
      " ...\n",
      " [0.19079507 0.2776274  0.04888407 ... 0.06923406 0.01024668 0.0587198 ]\n",
      " [0.06466352 0.00635987 0.13518243 ... 0.1526264  0.15196077 0.07563558]\n",
      " [0.13967099 0.18678635 0.05844003 ... 0.06026876 0.04702074 0.13966464]] & cached\n",
      "Re-used Cached Value, runNum =  116\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  116\n",
      "Provided input from cache for runNum = 116\n",
      "Provided input from cache for runNum = 117\n",
      "activation = [[4.56410887 5.13384637 5.22306361 ... 7.51024374 3.26143329 2.85421678]\n",
      " [0.         3.10363968 0.         ... 0.         0.         0.        ]\n",
      " [0.09191598 3.58104857 1.59197215 ... 1.48187868 3.5434473  1.4093264 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98526802 0.         0.70171979 ... 0.22631567 4.55668084 0.        ]\n",
      " [1.78297131 4.3435766  0.         ... 2.18036108 0.63551331 0.66649879]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72883127 0.         0.         ... 0.         0.6711992  0.        ]\n",
      " [0.96049178 4.02014681 1.39460689 ... 1.3227932  0.         2.02174068]\n",
      " ...\n",
      " [2.64247029 0.89157111 1.20392094 ... 0.48209543 2.53309584 1.29942449]\n",
      " [3.22937118 0.         3.12868503 ... 2.34531913 1.49635359 1.9306445 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00578943 0.01005515 0.12068953 ... 0.18235311 0.01822028 0.11231017]\n",
      " [0.36446833 0.00228051 0.18728986 ... 0.10862141 0.10507019 0.10636305]\n",
      " [0.02321817 0.00539026 0.09159568 ... 0.07074519 0.2631863  0.10271855]\n",
      " ...\n",
      " [0.19347998 0.27825243 0.04813241 ... 0.06863634 0.0099743  0.05844568]\n",
      " [0.06436815 0.00615459 0.13590223 ... 0.15246053 0.15090022 0.07538419]\n",
      " [0.14122539 0.18673232 0.05763014 ... 0.05950503 0.04619275 0.13983897]] & cached\n",
      "Re-used Cached Value, runNum =  117\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  117\n",
      "Provided input from cache for runNum = 117\n",
      "Provided input from cache for runNum = 118\n",
      "activation = [[4.57044321 5.13663422 5.23609807 ... 7.52831664 3.27072342 2.85762786]\n",
      " [0.         3.10756343 0.         ... 0.         0.         0.        ]\n",
      " [0.09306585 3.58672648 1.58735536 ... 1.48765762 3.55001999 1.41369167]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98350008 0.         0.71046292 ... 0.22177346 4.56688072 0.        ]\n",
      " [1.80587958 4.37460171 0.         ... 2.18989616 0.64879162 0.68004637]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72778911 0.         0.         ... 0.         0.67374473 0.        ]\n",
      " [0.97015053 4.04989294 1.40408991 ... 1.34599717 0.         2.04170982]\n",
      " ...\n",
      " [2.6577082  0.90498716 1.21621118 ... 0.47301442 2.55849187 1.30676742]\n",
      " [3.23339161 0.         3.1622731  ... 2.36814233 1.50838419 1.93773798]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00563727 0.00971223 0.12041329 ... 0.18500153 0.01776585 0.1125189 ]\n",
      " [0.36269371 0.00217993 0.18861709 ... 0.10737993 0.10423163 0.10548306]\n",
      " [0.02258281 0.00517501 0.09102963 ... 0.06996908 0.26463426 0.10214525]\n",
      " ...\n",
      " [0.19612259 0.27885356 0.04737815 ... 0.06800568 0.00970481 0.05815736]\n",
      " [0.06408211 0.00595918 0.13664422 ... 0.15232097 0.14984486 0.07513965]\n",
      " [0.1427671  0.18674547 0.05682494 ... 0.05872176 0.04535415 0.14000341]] & cached\n",
      "Re-used Cached Value, runNum =  118\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  118\n",
      "Provided input from cache for runNum = 118\n",
      "Provided input from cache for runNum = 119\n",
      "activation = [[4.57688479 5.13959866 5.24913102 ... 7.54676761 3.2802474  2.86112521]\n",
      " [0.         3.11144935 0.         ... 0.         0.         0.        ]\n",
      " [0.09420714 3.59231635 1.58278874 ... 1.49330153 3.55658201 1.41800551]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98189104 0.         0.71924178 ... 0.21735024 4.57713474 0.        ]\n",
      " [1.82867278 4.40544908 0.         ... 2.19926903 0.66202707 0.69352958]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72675744 0.         0.         ... 0.         0.6761417  0.        ]\n",
      " [0.97973917 4.0796588  1.41365231 ... 1.36941803 0.         2.06186366]\n",
      " ...\n",
      " [2.67318272 0.91870945 1.22881854 ... 0.46433354 2.58390108 1.31433713]\n",
      " [3.23747561 0.         3.1959381  ... 2.3914573  1.52060984 1.94497475]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00548671 0.00938025 0.12011146 ... 0.18766346 0.01732176 0.11271746]\n",
      " [0.3609018  0.00208352 0.18993559 ... 0.10612765 0.10338907 0.10459374]\n",
      " [0.02196492 0.00496859 0.09046485 ... 0.06920155 0.26609387 0.10157607]\n",
      " ...\n",
      " [0.19877052 0.27942949 0.04662646 ... 0.06735246 0.00944095 0.05786005]\n",
      " [0.06380752 0.00577208 0.13741526 ... 0.15221216 0.14879504 0.07490041]\n",
      " [0.14432781 0.18678766 0.05603252 ... 0.05793277 0.04452025 0.14017141]] & cached\n",
      "Re-used Cached Value, runNum =  119\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  119\n",
      "Provided input from cache for runNum = 119\n",
      "Provided input from cache for runNum = 120\n",
      "activation = [[4.58345127 5.1428043  5.26217665 ... 7.56560785 3.29005817 2.86473306]\n",
      " [0.         3.11521606 0.         ... 0.         0.         0.        ]\n",
      " [0.09525559 3.59771641 1.57820346 ... 1.49870682 3.5630346  1.42221638]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98041174 0.         0.72801213 ... 0.2130053  4.58735577 0.        ]\n",
      " [1.85124142 4.43596251 0.         ... 2.20838938 0.675083   0.70689522]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72578636 0.         0.         ... 0.         0.67841926 0.        ]\n",
      " [0.98917789 4.1093118  1.42324893 ... 1.39301425 0.         2.08213432]\n",
      " ...\n",
      " [2.68899281 0.93297623 1.24182145 ... 0.45620942 2.6094558  1.32221256]\n",
      " [3.24189924 0.         3.22980911 ... 2.41551872 1.53333955 1.95249711]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00533743 0.00905861 0.11977648 ... 0.19032452 0.01688609 0.11290067]\n",
      " [0.35915354 0.00199167 0.19126267 ... 0.1048828  0.10254709 0.10370595]\n",
      " [0.02136605 0.00477165 0.08990364 ... 0.06844904 0.26758353 0.10101669]\n",
      " ...\n",
      " [0.20138971 0.27996874 0.04587475 ... 0.06667009 0.00918053 0.05754992]\n",
      " [0.06354389 0.00559368 0.13821348 ... 0.15213627 0.14774861 0.0746687 ]\n",
      " [0.14588677 0.18688229 0.05525023 ... 0.05713488 0.04368197 0.14033423]] & cached\n",
      "Re-used Cached Value, runNum =  120\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  120\n",
      "Provided input from cache for runNum = 120\n",
      "Provided input from cache for runNum = 121\n",
      "activation = [[4.59003617 5.14608468 5.27516974 ... 7.58471753 3.3000125  2.86837916]\n",
      " [0.         3.11893167 0.         ... 0.         0.         0.        ]\n",
      " [0.09628398 3.60299985 1.57362682 ... 1.50392874 3.56948753 1.42635401]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97903106 0.         0.73677528 ... 0.20872626 4.59753941 0.        ]\n",
      " [1.87368794 4.46620637 0.         ... 2.2172771  0.68805392 0.72016084]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72490133 0.         0.         ... 0.         0.68061092 0.        ]\n",
      " [0.99865819 4.13912449 1.43299546 ... 1.41697872 0.         2.10259554]\n",
      " ...\n",
      " [2.70500446 0.94761344 1.25507625 ... 0.44845722 2.63502177 1.33028237]\n",
      " [3.24644595 0.         3.26376998 ... 2.44022229 1.54629638 1.96018196]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00518941 0.00874493 0.11942353 ... 0.19301031 0.01646009 0.11307683]\n",
      " [0.35736476 0.00190321 0.19257416 ... 0.10362527 0.10170392 0.10280843]\n",
      " [0.02078197 0.0045819  0.08933981 ... 0.06770379 0.26908612 0.10045893]\n",
      " ...\n",
      " [0.20404141 0.28047495 0.04512846 ... 0.06596477 0.00892555 0.05723223]\n",
      " [0.06328123 0.00542122 0.13903164 ... 0.15207363 0.14670513 0.07444075]\n",
      " [0.14746916 0.18699374 0.05448139 ... 0.05633082 0.04284882 0.14049871]] & cached\n",
      "Re-used Cached Value, runNum =  121\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  121\n",
      "Provided input from cache for runNum = 121\n",
      "Provided input from cache for runNum = 122\n",
      "activation = [[4.59664389 5.14945195 5.28810504 ... 7.60404406 3.31010897 2.87205721]\n",
      " [0.         3.12244829 0.         ... 0.         0.         0.        ]\n",
      " [0.09730023 3.60819904 1.56910484 ... 1.50906022 3.57594013 1.43045661]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97777159 0.         0.74557555 ... 0.20454054 4.60773245 0.        ]\n",
      " [1.89610906 4.49635992 0.         ... 2.22612769 0.70103122 0.73341628]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72404269 0.         0.         ... 0.         0.68270365 0.        ]\n",
      " [1.0080794  4.16889324 1.44279119 ... 1.44122345 0.         2.12319783]\n",
      " ...\n",
      " [2.72110879 0.96240085 1.26843105 ... 0.44086882 2.6604735  1.33843296]\n",
      " [3.25113168 0.         3.2978653  ... 2.46542746 1.55955346 1.96799604]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00504262 0.00843894 0.11905725 ... 0.19572776 0.01604573 0.11324974]\n",
      " [0.35554951 0.0018183  0.19388365 ... 0.10235916 0.10086456 0.10190471]\n",
      " [0.02020944 0.00439816 0.08876763 ... 0.06695557 0.27060359 0.09989519]\n",
      " ...\n",
      " [0.20673544 0.28098481 0.04438853 ... 0.06524533 0.00867642 0.05691065]\n",
      " [0.06301392 0.00525406 0.13986335 ... 0.15200845 0.14566963 0.0742141 ]\n",
      " [0.14906973 0.18712136 0.05372345 ... 0.05552551 0.04202043 0.14066708]] & cached\n",
      "Re-used Cached Value, runNum =  122\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  122\n",
      "Provided input from cache for runNum = 122\n",
      "Provided input from cache for runNum = 123\n",
      "activation = [[4.60329893 5.15292161 5.30100381 ... 7.62364065 3.32040072 2.87577254]\n",
      " [0.         3.12589526 0.         ... 0.         0.         0.        ]\n",
      " [0.09840649 3.61348672 1.56466743 ... 1.51425962 3.58257762 1.43460507]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97662878 0.         0.754403   ... 0.20043587 4.61794007 0.        ]\n",
      " [1.91847151 4.5264664  0.         ... 2.23497692 0.71409771 0.74668324]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72326078 0.         0.         ... 0.         0.68462742 0.        ]\n",
      " [1.01765377 4.19891067 1.45284825 ... 1.46597088 0.         2.1440823 ]\n",
      " ...\n",
      " [2.73722825 0.97719785 1.28187236 ... 0.43333634 2.68573436 1.34665968]\n",
      " [3.25588769 0.         3.33204777 ... 2.49096441 1.57291405 1.97587211]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0048978  0.00814136 0.11868588 ... 0.19850048 0.01564415 0.11342153]\n",
      " [0.35367105 0.00173625 0.19517141 ... 0.10106206 0.10002627 0.1009858 ]\n",
      " [0.01964982 0.00422041 0.0881888  ... 0.06620538 0.27212466 0.09931992]\n",
      " ...\n",
      " [0.2094785  0.28146388 0.04365631 ... 0.06451362 0.00843449 0.05658783]\n",
      " [0.06273753 0.00509102 0.14069472 ... 0.15192264 0.14464691 0.07398382]\n",
      " [0.15070208 0.18724688 0.05298122 ... 0.05472363 0.04120316 0.1408483 ]] & cached\n",
      "Re-used Cached Value, runNum =  123\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  123\n",
      "Provided input from cache for runNum = 123\n",
      "Provided input from cache for runNum = 124\n",
      "activation = [[4.6099059  5.15632739 5.3137956  ... 7.64331447 3.33072921 2.87944524]\n",
      " [0.         3.12930968 0.         ... 0.         0.         0.        ]\n",
      " [0.09956496 3.61881876 1.56030011 ... 1.51942074 3.58927956 1.43876401]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.9755177  0.         0.76320447 ... 0.1962714  4.62800676 0.        ]\n",
      " [1.94084879 4.5566342  0.         ... 2.24391887 0.72727921 0.75999991]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.722499   0.         0.         ... 0.         0.68355087 0.        ]\n",
      " [1.02751524 4.22928792 1.46327441 ... 1.49144082 0.         2.16528327]\n",
      " ...\n",
      " [2.75363005 0.99241958 1.29568416 ... 0.4262942  2.71539265 1.35515586]\n",
      " [3.26041412 0.         3.36609431 ... 2.51650699 1.58974357 1.98365524]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0047542  0.00784969 0.11829248 ... 0.2012807  0.0152226  0.1135762 ]\n",
      " [0.35168288 0.00165685 0.19642526 ... 0.09973074 0.09909684 0.10004873]\n",
      " [0.01910241 0.00404879 0.08760843 ... 0.06545201 0.27389477 0.09873516]\n",
      " ...\n",
      " [0.21228702 0.28188779 0.04293397 ... 0.06377948 0.00816994 0.0562638 ]\n",
      " [0.0624511  0.00493207 0.14153172 ... 0.15181183 0.14370599 0.07374965]\n",
      " [0.15239756 0.18737644 0.05226446 ... 0.05394553 0.04028041 0.14105841]] & cached\n",
      "Re-used Cached Value, runNum =  124\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  124\n",
      "Provided input from cache for runNum = 124\n",
      "Provided input from cache for runNum = 125\n",
      "activation = [[4.61659151 5.15989787 5.32656975 ... 7.66330467 3.34128997 2.88319858]\n",
      " [0.         3.13263252 0.         ... 0.         0.         0.        ]\n",
      " [0.10066023 3.62400516 1.55593147 ... 1.52437812 3.59590207 1.44285496]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97469069 0.         0.77213889 ... 0.1924842  4.63826018 0.        ]\n",
      " [1.96310264 4.58664546 0.         ... 2.25273708 0.74037902 0.77327913]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72193061 0.         0.         ... 0.         0.68112436 0.        ]\n",
      " [1.03727958 4.25973087 1.4737769  ... 1.51712384 0.         2.18670287]\n",
      " ...\n",
      " [2.77039957 1.00804384 1.30988857 ... 0.41983264 2.74739948 1.36390223]\n",
      " [3.26539545 0.         3.40052254 ... 2.54298978 1.60892955 1.9917617 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00461171 0.0075654  0.11786953 ... 0.20406946 0.01479393 0.11372207]\n",
      " [0.3497614  0.00158077 0.19769645 ... 0.09840815 0.09813132 0.09911203]\n",
      " [0.01857172 0.00388344 0.08703211 ... 0.06472009 0.27582335 0.098154  ]\n",
      " ...\n",
      " [0.2150646  0.28229823 0.04221019 ... 0.0630103  0.0078952  0.05592909]\n",
      " [0.06216829 0.00477863 0.14239253 ... 0.15173631 0.14279368 0.07351892]\n",
      " [0.1540808  0.18753022 0.05155135 ... 0.0531521  0.03929817 0.1412615 ]] & cached\n",
      "Re-used Cached Value, runNum =  125\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  125\n",
      "Provided input from cache for runNum = 125\n",
      "Provided input from cache for runNum = 126\n",
      "activation = [[4.62325464 5.16344882 5.3392371  ... 7.68342427 3.35193147 2.88692905]\n",
      " [0.         3.13595337 0.         ... 0.         0.         0.        ]\n",
      " [0.10185602 3.62929138 1.55170148 ... 1.52939447 3.60266806 1.4469939 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97405934 0.         0.78112794 ... 0.18884448 4.64861514 0.        ]\n",
      " [1.98534359 4.61662767 0.         ... 2.26156484 0.75354891 0.78660249]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72151901 0.         0.         ... 0.         0.67867754 0.        ]\n",
      " [1.04702901 4.29030378 1.48439082 ... 1.5431239  0.         2.20832901]\n",
      " ...\n",
      " [2.78747097 1.02400308 1.32441806 ... 0.41381569 2.77944395 1.37286355]\n",
      " [3.27022788 0.         3.4348498  ... 2.5696062  1.62802316 1.99979929]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00447049 0.00728874 0.11742336 ... 0.20686889 0.01437443 0.11380014]\n",
      " [0.34779295 0.00150751 0.19894828 ... 0.09706467 0.09716028 0.09813812]\n",
      " [0.01805632 0.00372476 0.08646048 ... 0.06399933 0.27774445 0.09756103]\n",
      " ...\n",
      " [0.21786693 0.28265278 0.04149288 ... 0.06222764 0.00762874 0.05564606]\n",
      " [0.0618847  0.00463027 0.14327347 ... 0.15167141 0.14187383 0.07329416]\n",
      " [0.15580017 0.18768182 0.05085566 ... 0.05236815 0.03833599 0.141531  ]] & cached\n",
      "Re-used Cached Value, runNum =  126\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  126\n",
      "Provided input from cache for runNum = 126\n",
      "Provided input from cache for runNum = 127\n",
      "activation = [[4.6299345  5.16701747 5.35182217 ... 7.70369003 3.36271228 2.89065404]\n",
      " [0.         3.13931104 0.         ... 0.         0.         0.        ]\n",
      " [0.10307656 3.63461379 1.54754044 ... 1.53443441 3.60952178 1.4511483 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97350421 0.         0.790132   ... 0.18515888 4.65883509 0.        ]\n",
      " [2.00752093 4.64660097 0.         ... 2.27043173 0.76676625 0.79995409]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72120394 0.         0.         ... 0.         0.67613728 0.        ]\n",
      " [1.05686443 4.32110133 1.49516594 ... 1.56957037 0.         2.23018577]\n",
      " ...\n",
      " [2.80460025 1.04001595 1.33904871 ... 0.40786348 2.8112406  1.38191359]\n",
      " [3.27499472 0.         3.46908069 ... 2.59627915 1.64706582 2.00777716]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0043313  0.00702028 0.11696686 ... 0.20970724 0.0139685  0.1136674 ]\n",
      " [0.34577333 0.00143689 0.20018155 ... 0.09569289 0.09619218 0.0970639 ]\n",
      " [0.01755059 0.0035713  0.08587421 ... 0.06326239 0.27965683 0.09693538]\n",
      " ...\n",
      " [0.22072227 0.28300354 0.04078846 ... 0.06144693 0.00737184 0.05556745]\n",
      " [0.06159331 0.00448625 0.14416431 ... 0.1515861  0.14095966 0.07308754]\n",
      " [0.15754515 0.18782432 0.05017685 ... 0.05159489 0.03739522 0.14199919]] & cached\n",
      "Re-used Cached Value, runNum =  127\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  127\n",
      "Provided input from cache for runNum = 127\n",
      "Provided input from cache for runNum = 128\n",
      "activation = [[4.63668522 5.17070277 5.3643735  ... 7.72423641 3.37367826 2.89443363]\n",
      " [0.         3.1426103  0.         ... 0.         0.         0.        ]\n",
      " [0.10422834 3.63979831 1.54336465 ... 1.53927689 3.61633541 1.45522987]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97302129 0.         0.79913238 ... 0.18140382 4.66888337 0.        ]\n",
      " [2.0295799  4.67643845 0.         ... 2.27920832 0.77995456 0.81326247]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72101    0.         0.         ... 0.         0.67354237 0.        ]\n",
      " [1.06671688 4.35197817 1.50605718 ... 1.59643035 0.         2.25222276]\n",
      " ...\n",
      " [2.8219927  1.05641844 1.35398506 ... 0.40231001 2.84300617 1.39119654]\n",
      " [3.28004936 0.         3.50348402 ... 2.62357722 1.66641079 2.01594439]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00419399 0.00675931 0.11648967 ... 0.21256632 0.01357309 0.11352161]\n",
      " [0.34376507 0.00136918 0.2014112  ... 0.09431285 0.09523012 0.09598666]\n",
      " [0.01705842 0.00342382 0.08528742 ... 0.0625267  0.28158361 0.09631301]\n",
      " ...\n",
      " [0.22358677 0.28333623 0.04008817 ... 0.06064903 0.0071218  0.05548351]\n",
      " [0.06130423 0.00434755 0.1450805  ... 0.1515089  0.1400467  0.0728854 ]\n",
      " [0.1592861  0.18797702 0.04950525 ... 0.0508172  0.03646551 0.14247236]] & cached\n",
      "Re-used Cached Value, runNum =  128\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  128\n",
      "Provided input from cache for runNum = 128\n",
      "Provided input from cache for runNum = 129\n",
      "activation = [[4.64339744 5.17435773 5.37683303 ... 7.74486318 3.38469767 2.8981861 ]\n",
      " [0.         3.14590562 0.         ... 0.         0.         0.        ]\n",
      " [0.10537496 3.64494259 1.5392162  ... 1.54405403 3.62317479 1.4592946 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97276953 0.         0.80823499 ... 0.17787553 4.6790211  0.        ]\n",
      " [2.05154474 4.70611468 0.         ... 2.28791492 0.79304687 0.82656427]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72102308 0.         0.         ... 0.         0.67105943 0.        ]\n",
      " [1.076362   4.38274209 1.51689214 ... 1.62340469 0.         2.27434458]\n",
      " ...\n",
      " [2.83962903 1.07300619 1.36920226 ... 0.39714961 2.87473173 1.40061937]\n",
      " [3.28509352 0.         3.53788864 ... 2.65116738 1.68583162 2.02411692]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0040581  0.00650633 0.11598473 ... 0.21542991 0.0131864  0.11336161]\n",
      " [0.34178379 0.00130449 0.20264979 ... 0.09293299 0.09427327 0.09490672]\n",
      " [0.01657999 0.00328222 0.08470277 ... 0.06180082 0.28351166 0.09568589]\n",
      " ...\n",
      " [0.22644492 0.28365112 0.03939324 ... 0.05983733 0.00687892 0.05539968]\n",
      " [0.06101285 0.00421384 0.14601857 ... 0.15144705 0.13912384 0.07268582]\n",
      " [0.16103581 0.1881375  0.04884512 ... 0.05004357 0.03555217 0.14296231]] & cached\n",
      "Re-used Cached Value, runNum =  129\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  129\n",
      "Provided input from cache for runNum = 129\n",
      "Provided input from cache for runNum = 130\n",
      "activation = [[4.65014438 5.17803888 5.38923137 ... 7.76566014 3.39584396 2.90195654]\n",
      " [0.         3.14930868 0.         ... 0.         0.         0.        ]\n",
      " [0.10656147 3.65012974 1.53514699 ... 1.54887451 3.63011583 1.4633893 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97258478 0.         0.81733193 ... 0.17431457 4.68905851 0.        ]\n",
      " [2.07348506 4.73584936 0.         ... 2.29672019 0.80623932 0.83992837]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72104911 0.         0.         ... 0.         0.66840315 0.        ]\n",
      " [1.08607257 4.41374708 1.52790049 ... 1.65087476 0.         2.29670838]\n",
      " ...\n",
      " [2.85739666 1.08973651 1.38461468 ... 0.3921961  2.90633707 1.41018913]\n",
      " [3.29008179 0.         3.5722321  ... 2.67885765 1.70518299 2.03224514]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00392396 0.00625967 0.11546129 ... 0.2183177  0.01281144 0.11318499]\n",
      " [0.33975481 0.00124213 0.20388396 ... 0.09153487 0.09331718 0.0938166 ]\n",
      " [0.01610928 0.0031451  0.08410674 ... 0.06106402 0.28543228 0.09504542]\n",
      " ...\n",
      " [0.22936499 0.28395261 0.03870995 ... 0.05902666 0.00664459 0.05532208]\n",
      " [0.06070696 0.00408328 0.14695719 ... 0.15135539 0.13820843 0.07248186]\n",
      " [0.16281242 0.18828907 0.04820303 ... 0.04928462 0.03465978 0.14348325]] & cached\n",
      "Re-used Cached Value, runNum =  130\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  130\n",
      "Provided input from cache for runNum = 130\n",
      "Provided input from cache for runNum = 131\n",
      "activation = [[4.65694672 5.18181488 5.40158173 ... 7.78666201 3.40714184 2.90577167]\n",
      " [0.         3.15260793 0.         ... 0.         0.         0.        ]\n",
      " [0.10777638 3.6553498  1.53113953 ... 1.55370792 3.63715332 1.46748609]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97250617 0.         0.82644367 ... 0.17079509 4.69908327 0.        ]\n",
      " [2.09531487 4.76543754 0.         ... 2.30544435 0.81940278 0.85324643]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7210204  0.         0.         ... 0.         0.66558794 0.        ]\n",
      " [1.09571848 4.44470662 1.53898481 ... 1.68014879 0.         2.31920372]\n",
      " ...\n",
      " [2.87529765 1.10655822 1.4001595  ... 0.3884465  2.93777218 1.41984223]\n",
      " [3.29511182 0.         3.60651247 ... 2.70751291 1.72454734 2.04039539]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00379241 0.00602197 0.11492256 ... 0.22116255 0.01244842 0.11300246]\n",
      " [0.33772749 0.00118274 0.20511326 ... 0.09012333 0.09236574 0.09272348]\n",
      " [0.01564898 0.00301336 0.08349996 ... 0.06030058 0.28735059 0.0943982 ]\n",
      " ...\n",
      " [0.23230415 0.2842688  0.03803767 ... 0.05821543 0.00641843 0.05524242]\n",
      " [0.06039866 0.0039577  0.1479086  ... 0.15117836 0.1373026  0.07227849]\n",
      " [0.16459255 0.18846379 0.04757443 ... 0.04855178 0.03378646 0.14401643]] & cached\n",
      "Re-used Cached Value, runNum =  131\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  131\n",
      "Provided input from cache for runNum = 131\n",
      "Provided input from cache for runNum = 132\n",
      "activation = [[4.66378183 5.18563124 5.4138673  ... 7.80780333 3.41854249 2.90960196]\n",
      " [0.         3.15583718 0.         ... 0.         0.         0.        ]\n",
      " [0.1089603  3.66049166 1.52714839 ... 1.55846339 3.64420099 1.47155093]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97250879 0.         0.83557057 ... 0.16735479 4.70898876 0.        ]\n",
      " [2.11706536 4.79498122 0.         ... 2.31421004 0.83259625 0.86656865]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72102168 0.         0.         ... 0.         0.66264744 0.        ]\n",
      " [1.10526313 4.47560944 1.55010086 ... 1.71248276 0.         2.34183796]\n",
      " ...\n",
      " [2.89344481 1.12370542 1.41599917 ... 0.38705329 2.96919279 1.42970024]\n",
      " [3.30019465 0.         3.64078433 ... 2.73767841 1.74394366 2.04855957]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00366255 0.00579085 0.11435156 ... 0.2238604  0.01209488 0.11279888]\n",
      " [0.33572006 0.00112612 0.20636368 ... 0.0887102  0.09141641 0.09163197]\n",
      " [0.01519801 0.00288658 0.0828847  ... 0.05949821 0.28926973 0.09374285]\n",
      " ...\n",
      " [0.23525926 0.28458311 0.03737482 ... 0.0574089  0.00619936 0.0551639 ]\n",
      " [0.06008072 0.00383628 0.1488671  ... 0.15085064 0.1363999  0.07207319]\n",
      " [0.1663797  0.18866339 0.04696112 ... 0.0478715  0.03293029 0.14457438]] & cached\n",
      "Re-used Cached Value, runNum =  132\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  132\n",
      "Provided input from cache for runNum = 132\n",
      "Provided input from cache for runNum = 133\n",
      "activation = [[4.6706497  5.1895166  5.42608371 ... 7.82912272 3.43006402 2.91346022]\n",
      " [0.         3.15896193 0.         ... 0.         0.         0.        ]\n",
      " [0.11009352 3.66553738 1.52315869 ... 1.56309607 3.65124905 1.4755603 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.9726275  0.         0.84473374 ... 0.16402457 4.71895092 0.        ]\n",
      " [2.13863398 4.82431409 0.         ... 2.32279972 0.84567735 0.87982428]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72102025 0.         0.         ... 0.         0.65959045 0.        ]\n",
      " [1.1144991  4.50620038 1.56108988 ... 1.74479524 0.         2.36450819]\n",
      " ...\n",
      " [2.91188209 1.14119728 1.43219869 ... 0.38616559 3.00071638 1.43980193]\n",
      " [3.30537046 0.         3.67505323 ... 2.76822716 1.7634676  2.05681088]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00353506 0.00556785 0.11374647 ... 0.22653785 0.01174936 0.11257703]\n",
      " [0.33375619 0.00107237 0.20762931 ... 0.08730592 0.09046792 0.09054496]\n",
      " [0.01476083 0.00276576 0.0822746  ... 0.05870856 0.29119689 0.0930919 ]\n",
      " ...\n",
      " [0.23819898 0.2848845  0.03671709 ... 0.05658723 0.00598641 0.05507939]\n",
      " [0.05977228 0.00372063 0.14985733 ... 0.15056007 0.13549757 0.07187362]\n",
      " [0.16814986 0.18887865 0.04635646 ... 0.04718907 0.03208813 0.14514002]] & cached\n",
      "Re-used Cached Value, runNum =  133\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  133\n",
      "Provided input from cache for runNum = 133\n",
      "Provided input from cache for runNum = 134\n",
      "activation = [[4.67756826 5.19351119 5.4382491  ... 7.85064273 3.44174963 2.91737149]\n",
      " [0.         3.16196198 0.         ... 0.         0.         0.        ]\n",
      " [0.11119399 3.670482   1.51918543 ... 1.5676357  3.65831307 1.47951945]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97308298 0.         0.85406688 ... 0.16117147 4.7291915  0.        ]\n",
      " [2.16006561 4.85349114 0.         ... 2.33128201 0.85870115 0.89301861]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72139483 0.         0.         ... 0.         0.65677166 0.        ]\n",
      " [1.12342074 4.53659855 1.57194085 ... 1.77702941 0.         2.3872626 ]\n",
      " ...\n",
      " [2.93068788 1.15896532 1.44876149 ... 0.38588405 3.03243107 1.45010286]\n",
      " [3.31077862 0.         3.70944651 ... 2.79933499 1.78329592 2.06523151]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00340892 0.00535079 0.11310637 ... 0.22918764 0.01140855 0.11233841]\n",
      " [0.33187816 0.00102109 0.20892291 ... 0.08592026 0.08952635 0.08946292]\n",
      " [0.01433765 0.00264939 0.08167038 ... 0.05794041 0.29314187 0.09244094]\n",
      " ...\n",
      " [0.24109985 0.28519279 0.03606286 ... 0.05574492 0.00577791 0.0549926 ]\n",
      " [0.05946207 0.00360889 0.15087088 ... 0.15030974 0.13457515 0.07167652]\n",
      " [0.16989964 0.18909925 0.04575822 ... 0.04650201 0.03125522 0.14571382]] & cached\n",
      "Re-used Cached Value, runNum =  134\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  134\n",
      "Provided input from cache for runNum = 134\n",
      "Provided input from cache for runNum = 135\n",
      "activation = [[4.68452539 5.19761313 5.45036269 ... 7.87238125 3.45359256 2.9213295 ]\n",
      " [0.         3.16490289 0.         ... 0.         0.         0.        ]\n",
      " [0.11230069 3.67536546 1.51524875 ... 1.57211    3.66541164 1.48345023]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97368125 0.         0.8634637  ... 0.15844623 4.73947569 0.        ]\n",
      " [2.18137995 4.88248437 0.         ... 2.33963005 0.87164622 0.9061733 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72187483 0.         0.         ... 0.         0.65393671 0.        ]\n",
      " [1.13230801 4.56704606 1.58288916 ... 1.80961371 0.         2.41021391]\n",
      " ...\n",
      " [2.94969383 1.17691436 1.46557515 ... 0.38597797 3.06415938 1.46058558]\n",
      " [3.31627234 0.         3.74384205 ... 2.83085105 1.80329914 2.0737735 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00328516 0.00514081 0.11244876 ... 0.23184802 0.01107619 0.11208876]\n",
      " [0.32997802 0.00097196 0.21020629 ... 0.08452166 0.08858999 0.0883754 ]\n",
      " [0.013925   0.0025375  0.08106379 ... 0.05717535 0.29509568 0.09178793]\n",
      " ...\n",
      " [0.2440347  0.28549846 0.03541764 ... 0.05489208 0.0055755  0.05490412]\n",
      " [0.05914696 0.00350094 0.15189879 ... 0.15006489 0.13365141 0.07147967]\n",
      " [0.17165056 0.18931558 0.04517136 ... 0.04581429 0.03043661 0.14629839]] & cached\n",
      "Re-used Cached Value, runNum =  135\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  135\n",
      "Provided input from cache for runNum = 135\n",
      "Provided input from cache for runNum = 136\n",
      "activation = [[4.69146197 5.20172428 5.46237514 ... 7.89420948 3.46550771 2.92530031]\n",
      " [0.         3.1677661  0.         ... 0.         0.         0.        ]\n",
      " [0.11339937 3.68019133 1.51133947 ... 1.57655837 3.67255775 1.48734455]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97437518 0.         0.87288585 ... 0.15574004 4.74970898 0.        ]\n",
      " [2.20261793 4.91145054 0.         ... 2.34802517 0.88460489 0.91931887]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72259164 0.         0.         ... 0.         0.65120309 0.        ]\n",
      " [1.14111413 4.59743433 1.59387541 ... 1.84251267 0.02178691 2.433291  ]\n",
      " ...\n",
      " [2.9688197  1.19498483 1.48253096 ... 0.38628635 3.09575976 1.47118805]\n",
      " [3.32176758 0.         3.77814696 ... 2.86247669 1.82324701 2.08232328]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00316351 0.00493709 0.11177522 ... 0.23451627 0.01085323 0.11182562]\n",
      " [0.32807105 0.00092495 0.21148711 ... 0.08311192 0.08716374 0.08728463]\n",
      " [0.01352135 0.00242959 0.08044842 ... 0.05640214 0.29652059 0.09112719]\n",
      " ...\n",
      " [0.24700038 0.28581462 0.03478426 ... 0.05404118 0.00542581 0.0548183 ]\n",
      " [0.05881899 0.00339631 0.152934   ... 0.14980565 0.13221086 0.07128148]\n",
      " [0.17340649 0.18952685 0.04459823 ... 0.04513589 0.02993734 0.14690303]] & cached\n",
      "Re-used Cached Value, runNum =  136\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  136\n",
      "Provided input from cache for runNum = 136\n",
      "Provided input from cache for runNum = 137\n",
      "activation = [[4.69844063 5.20591168 5.47431937 ... 7.91622025 3.47756961 2.92929437]\n",
      " [0.         3.17052016 0.         ... 0.         0.         0.        ]\n",
      " [0.11452099 3.68499955 1.50749416 ... 1.5809872  3.67976998 1.4912295 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97539385 0.         0.88248476 ... 0.15344354 4.76021226 0.        ]\n",
      " [2.2237742  4.94031144 0.         ... 2.35637138 0.89755282 0.93244981]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72339449 0.         0.         ... 0.         0.64848048 0.        ]\n",
      " [1.14940593 4.6274139  1.60454469 ... 1.87517541 0.04758852 2.45633681]\n",
      " ...\n",
      " [2.98824455 1.21322766 1.49976842 ... 0.38708133 3.12746521 1.48191855]\n",
      " [3.3273484  0.         3.81245802 ... 2.89449146 1.84335841 2.09092527]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0030443  0.00474048 0.11107055 ... 0.23716596 0.01065132 0.11155128]\n",
      " [0.32623498 0.00088031 0.21279164 ... 0.08171528 0.08565861 0.08619758]\n",
      " [0.01313193 0.00232647 0.07984438 ... 0.05565228 0.29783972 0.09046934]\n",
      " ...\n",
      " [0.24992908 0.286132   0.0341544  ... 0.05317304 0.00528682 0.05472844]\n",
      " [0.05850082 0.0032962  0.15400528 ... 0.14959809 0.13069377 0.07108944]\n",
      " [0.17513374 0.18974    0.04402924 ... 0.04445208 0.0294947  0.14751228]] & cached\n",
      "Re-used Cached Value, runNum =  137\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  137\n",
      "Provided input from cache for runNum = 137\n",
      "Provided input from cache for runNum = 138\n",
      "activation = [[4.70538456 5.21002532 5.48614533 ... 7.93825994 3.48964105 2.93326361]\n",
      " [0.         3.17326142 0.         ... 0.         0.         0.        ]\n",
      " [0.11568999 3.68983889 1.50371592 ... 1.58548308 3.68709896 1.49512883]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97649661 0.         0.89211424 ... 0.15121558 4.77069441 0.        ]\n",
      " [2.24482555 4.96905455 0.         ... 2.36468386 0.91045909 0.94555801]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72435078 0.         0.         ... 0.         0.64578337 0.        ]\n",
      " [1.15760545 4.65732608 1.61525122 ... 1.90809131 0.07351455 2.47955016]\n",
      " ...\n",
      " [3.00782135 1.23161824 1.51724056 ... 0.38815499 3.15920514 1.49283744]\n",
      " [3.33285274 0.         3.84663501 ... 2.92654271 1.86345637 2.09951653]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00292712 0.00454931 0.11033983 ... 0.23980901 0.01045258 0.11125577]\n",
      " [0.32437947 0.0008376  0.21410751 ... 0.08031365 0.08416341 0.08510853]\n",
      " [0.01274943 0.00222686 0.07922911 ... 0.05489724 0.29913243 0.08980033]\n",
      " ...\n",
      " [0.25290117 0.28646209 0.03353769 ... 0.05230921 0.00515082 0.05464313]\n",
      " [0.05816393 0.00319877 0.15507254 ... 0.14937437 0.12919241 0.07089177]\n",
      " [0.17687461 0.18995073 0.04347825 ... 0.04378154 0.0290584  0.14815207]] & cached\n",
      "Re-used Cached Value, runNum =  138\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  138\n",
      "Provided input from cache for runNum = 138\n",
      "Provided input from cache for runNum = 139\n",
      "activation = [[4.71233864 5.21414464 5.49787196 ... 7.96038447 3.50178857 2.93723559]\n",
      " [0.         3.17604669 0.         ... 0.         0.         0.        ]\n",
      " [0.11687052 3.69468002 1.49998967 ... 1.59001078 3.69452158 1.49903289]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.9777274  0.         0.9017868  ... 0.14905188 4.78117388 0.        ]\n",
      " [2.26573892 4.99766603 0.         ... 2.37292809 0.92330948 0.95862139]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72548422 0.         0.         ... 0.         0.64313671 0.        ]\n",
      " [1.16595179 4.68752108 1.6262096  ... 1.94156243 0.09977941 2.50298515]\n",
      " ...\n",
      " [3.02757554 1.25015415 1.53487839 ... 0.3895018  3.19088872 1.503862  ]\n",
      " [3.33844505 0.         3.88074085 ... 2.9588393  1.88364138 2.10813829]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00281333 0.00436589 0.1096082  ... 0.24248349 0.01025882 0.11095725]\n",
      " [0.32250684 0.00079667 0.21538068 ... 0.07888601 0.08267794 0.08400979]\n",
      " [0.01237957 0.00213167 0.07861977 ... 0.05414537 0.3003996  0.08912731]\n",
      " ...\n",
      " [0.25586881 0.28676036 0.03292965 ... 0.05143867 0.0050182  0.05455519]\n",
      " [0.05782314 0.00310466 0.156146   ... 0.14913245 0.12770541 0.07069264]\n",
      " [0.17861929 0.19016509 0.04293896 ... 0.04311463 0.02862994 0.14880273]] & cached\n",
      "Re-used Cached Value, runNum =  139\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  139\n",
      "Provided input from cache for runNum = 139\n",
      "Provided input from cache for runNum = 140\n",
      "activation = [[4.71940313 5.21847177 5.50957008 ... 7.98282741 3.51413833 2.94129424]\n",
      " [0.         3.17866068 0.         ... 0.         0.         0.        ]\n",
      " [0.11798873 3.69935622 1.49627969 ... 1.59438932 3.70192353 1.50286086]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.97907264 0.         0.91151424 ... 0.14700642 4.7916553  0.        ]\n",
      " [2.28649079 5.02602827 0.         ... 2.38098998 0.93608158 0.97160114]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72668005 0.         0.         ... 0.         0.64047263 0.        ]\n",
      " [1.17418026 4.71757318 1.63716596 ... 1.97526849 0.12611975 2.52652851]\n",
      " ...\n",
      " [3.04757502 1.268916   1.5527833  ... 0.39125988 3.22258221 1.51505171]\n",
      " [3.34431663 0.         3.91491388 ... 2.99175085 1.90409951 2.11694775]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00270208 0.00418879 0.10884774 ... 0.24514144 0.01006874 0.1106482 ]\n",
      " [0.32069205 0.00075789 0.21668217 ... 0.07747287 0.08121021 0.08291991]\n",
      " [0.01201785 0.00204003 0.07800107 ... 0.05339099 0.30165864 0.08845168]\n",
      " ...\n",
      " [0.25882924 0.28711263 0.03233123 ... 0.05056285 0.00488785 0.05446166]\n",
      " [0.0574745  0.00301401 0.15722414 ... 0.1488911  0.12624296 0.07049302]\n",
      " [0.18033981 0.1904207  0.04241016 ... 0.0424477  0.02820213 0.14945839]] & cached\n",
      "Re-used Cached Value, runNum =  140\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  140\n",
      "Provided input from cache for runNum = 140\n",
      "Provided input from cache for runNum = 141\n",
      "activation = [[4.72642416 5.22277049 5.52114847 ... 8.00528973 3.52649822 2.94534264]\n",
      " [0.         3.181158   0.         ... 0.         0.         0.        ]\n",
      " [0.11910612 3.70399732 1.49260807 ... 1.59875721 3.70938521 1.50667436]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98048394 0.         0.92126756 ... 0.14511697 4.80216677 0.        ]\n",
      " [2.30721257 5.05436384 0.         ... 2.38908472 0.94890026 0.98459492]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72801918 0.         0.         ... 0.         0.63790202 0.        ]\n",
      " [1.1823305  4.74749918 1.64811666 ... 2.00914087 0.15244452 2.55018138]\n",
      " ...\n",
      " [3.06793263 1.28814114 1.5711372  ... 0.39368318 3.25450047 1.52654034]\n",
      " [3.34997354 0.         3.94888679 ... 3.02461865 1.92439572 2.12567868]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0025927  0.00401631 0.10804726 ... 0.24774105 0.00987851 0.11030848]\n",
      " [0.31884723 0.00072092 0.21800073 ... 0.07606709 0.0797504  0.08183048]\n",
      " [0.01166383 0.00195197 0.07738216 ... 0.05264459 0.302885   0.08777133]\n",
      " ...\n",
      " [0.26182524 0.28746145 0.03174389 ... 0.04969133 0.00475968 0.05437098]\n",
      " [0.05711089 0.00292623 0.15830944 ... 0.14865801 0.12478845 0.07029114]\n",
      " [0.1820839  0.19068756 0.04190004 ... 0.04179946 0.02777978 0.15014847]] & cached\n",
      "Re-used Cached Value, runNum =  141\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  141\n",
      "Provided input from cache for runNum = 141\n",
      "Provided input from cache for runNum = 142\n",
      "activation = [[4.73348588 5.22711432 5.53265181 ... 8.02787436 3.53896853 2.94940669]\n",
      " [0.         3.18358773 0.         ... 0.         0.         0.        ]\n",
      " [0.1202071  3.70862154 1.48896417 ... 1.60311943 3.71690759 1.51047374]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98193019 0.         0.93099225 ... 0.14320988 4.81263947 0.        ]\n",
      " [2.32785836 5.08267592 0.         ... 2.39719836 0.96172853 0.99760534]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.72938081 0.         0.         ... 0.         0.63528625 0.        ]\n",
      " [1.19038532 4.77728201 1.65906393 ... 2.0432583  0.17877516 2.57389776]\n",
      " ...\n",
      " [3.08834523 1.30736208 1.58956705 ... 0.3962349  3.28624994 1.53807936]\n",
      " [3.35554513 0.         3.9826521  ... 3.05749479 1.94457351 2.13432211]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00248634 0.00385019 0.10716498 ... 0.25034757 0.00969315 0.10995988]\n",
      " [0.31697568 0.00068569 0.21926188 ... 0.07465224 0.07830757 0.08073851]\n",
      " [0.01131732 0.00186724 0.07676372 ... 0.05188809 0.30408173 0.08708427]\n",
      " ...\n",
      " [0.26486469 0.28784712 0.03123575 ... 0.04882759 0.0046352  0.0542828 ]\n",
      " [0.05674116 0.00284156 0.15946341 ... 0.14841407 0.12336277 0.07008897]\n",
      " [0.18382183 0.19095166 0.04144023 ... 0.04115972 0.02736564 0.15085734]] & cached\n",
      "Re-used Cached Value, runNum =  142\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  142\n",
      "Provided input from cache for runNum = 142\n",
      "Provided input from cache for runNum = 143\n",
      "activation = [[4.7405547  5.23149136 5.54405769 ... 8.05052944 3.55150645 2.95347526]\n",
      " [0.         3.18602342 0.         ... 0.         0.         0.        ]\n",
      " [0.12132888 3.71322805 1.48539222 ... 1.60749035 3.72450148 1.51426289]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98363169 0.         0.94082081 ... 0.1416653  4.8233433  0.        ]\n",
      " [2.34832461 5.11072317 0.         ... 2.40511678 0.97440786 1.01050687]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73087202 0.         0.         ... 0.         0.63277226 0.        ]\n",
      " [1.19812426 4.80683268 1.66982185 ... 2.07723617 0.20487442 2.59758965]\n",
      " ...\n",
      " [3.1089585  1.32665411 1.60814286 ... 0.39913747 3.3180347  1.549682  ]\n",
      " [3.36119983 0.         4.01631367 ... 3.09068933 1.9648995  2.14300505]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00238327 0.00369128 0.10611798 ... 0.25294992 0.009509   0.10960734]\n",
      " [0.31516993 0.00065225 0.2204184  ... 0.07324739 0.07688777 0.07965173]\n",
      " [0.01098461 0.0017867  0.07618031 ... 0.05115236 0.30527279 0.08640018]\n",
      " ...\n",
      " [0.26784891 0.28821677 0.03087192 ... 0.04795345 0.00451236 0.05418903]\n",
      " [0.05637925 0.00276062 0.16076883 ... 0.14820394 0.12195871 0.06989169]\n",
      " [0.18552853 0.19121876 0.04106594 ... 0.04051688 0.02695004 0.15156628]] & cached\n",
      "Re-used Cached Value, runNum =  143\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  143\n",
      "Provided input from cache for runNum = 143\n",
      "Provided input from cache for runNum = 144\n",
      "activation = [[4.74759668e+00 5.23585545e+00 5.55535099e+00 ... 8.07324720e+00\n",
      "  3.56411586e+00 2.95752587e+00]\n",
      " [0.00000000e+00 3.18845423e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.22520829e-01 3.71789473e+00 1.48189970e+00 ... 1.61195329e+00\n",
      "  3.73223418e+00 1.51809027e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.98547962e+00 3.90577680e-03 9.50721689e-01 ... 1.40251980e-01\n",
      "  4.83411026e+00 0.00000000e+00]\n",
      " [2.36875002e+00 5.13868881e+00 0.00000000e+00 ... 2.41303064e+00\n",
      "  9.87062476e-01 1.02342309e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73237821 0.         0.         ... 0.         0.63016936 0.        ]\n",
      " [1.20572313 4.83457646 1.68053457 ... 2.11134723 0.23094384 2.62135256]\n",
      " ...\n",
      " [3.12961697 1.34810205 1.62680616 ... 0.40214417 3.34966083 1.56131174]\n",
      " [3.36663722 0.         4.04974917 ... 3.1238399  1.98511281 2.15156757]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00228327 0.00353867 0.10506243 ... 0.25556852 0.0093296  0.1092464 ]\n",
      " [0.31333004 0.00062137 0.2215626  ... 0.07183525 0.07548576 0.07856159]\n",
      " [0.01066112 0.00171557 0.07560102 ... 0.05041932 0.30643562 0.08570896]\n",
      " ...\n",
      " [0.27086336 0.28826309 0.03051688 ... 0.04708315 0.00439309 0.05409887]\n",
      " [0.05601124 0.00268869 0.16208992 ... 0.14798689 0.12058444 0.0696929 ]\n",
      " [0.18723539 0.19151156 0.04070374 ... 0.03988136 0.02654233 0.15229605]] & cached\n",
      "Re-used Cached Value, runNum =  144\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  144\n",
      "Provided input from cache for runNum = 144\n",
      "Provided input from cache for runNum = 145\n",
      "activation = [[4.754596   5.2401991  5.56651786 ... 8.09599466 3.57675591 2.96155597]\n",
      " [0.         3.19082672 0.         ... 0.         0.         0.        ]\n",
      " [0.12377312 3.72262    1.47850492 ... 1.61650271 3.74010266 1.52195613]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98750556 0.01504387 0.96070871 ... 0.13904456 4.84507871 0.        ]\n",
      " [2.38915785 5.16670538 0.         ... 2.42108038 0.99980116 1.03640116]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73397482 0.         0.         ... 0.         0.6275522  0.        ]\n",
      " [1.21307159 4.85893313 1.69112205 ... 2.14552333 0.25688217 2.64517293]\n",
      " ...\n",
      " [3.15052228 1.37393907 1.64576032 ... 0.40557276 3.38144351 1.57310377]\n",
      " [3.37182298 0.         4.08291084 ... 3.15679367 2.00510994 2.15995782]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00218555 0.00339016 0.10397489 ... 0.25815102 0.00915086 0.10886064]\n",
      " [0.3114692  0.00059369 0.22271693 ... 0.07042844 0.07409609 0.07747128]\n",
      " [0.01034654 0.00165766 0.07502925 ... 0.04969474 0.30756561 0.08501136]\n",
      " ...\n",
      " [0.27390594 0.28771151 0.03017351 ... 0.04622041 0.00427622 0.05401535]\n",
      " [0.05563437 0.00262997 0.1634307  ... 0.14777427 0.11922545 0.06949156]\n",
      " [0.18894456 0.19186096 0.04035835 ... 0.03926195 0.02614023 0.15306066]] & cached\n",
      "Re-used Cached Value, runNum =  145\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  145\n",
      "Provided input from cache for runNum = 145\n",
      "Provided input from cache for runNum = 146\n",
      "activation = [[4.76156046 5.24454196 5.57755712 ... 8.11879501 3.58944428 2.96557356]\n",
      " [0.         3.19321564 0.         ... 0.         0.         0.        ]\n",
      " [0.12504477 3.72735682 1.47516817 ... 1.62108426 3.74804722 1.52583649]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.98966637 0.0263341  0.97075767 ... 0.13798989 4.85605972 0.        ]\n",
      " [2.40943433 5.19456405 0.         ... 2.42906603 1.01246991 1.04935006]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73581888 0.         0.         ... 0.         0.62506618 0.        ]\n",
      " [1.22037687 4.88323808 1.70174799 ... 2.18000034 0.28290526 2.66908368]\n",
      " ...\n",
      " [3.17155701 1.39991412 1.66483456 ... 0.4092362  3.41307955 1.5849279 ]\n",
      " [3.37696911 0.         4.11587637 ... 3.18984712 2.02503412 2.16826606]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00209065 0.00324739 0.1028762  ... 0.26073756 0.00897589 0.10846747]\n",
      " [0.30961734 0.00056719 0.22386209 ... 0.06901884 0.07272612 0.0763801 ]\n",
      " [0.0100414  0.00160194 0.0744584  ... 0.04897201 0.30866624 0.08430753]\n",
      " ...\n",
      " [0.27694523 0.28714129 0.02984059 ... 0.04536072 0.00416251 0.05393399]\n",
      " [0.05524724 0.00257296 0.1647812  ... 0.14754599 0.11788277 0.06928819]\n",
      " [0.19064491 0.19220357 0.04002608 ... 0.03864996 0.02574598 0.15384284]] & cached\n",
      "Re-used Cached Value, runNum =  146\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  146\n",
      "Provided input from cache for runNum = 146\n",
      "Provided input from cache for runNum = 147\n",
      "activation = [[4.76865485 5.24911027 5.58856373 ... 8.14182729 3.60235484 2.96965614]\n",
      " [0.         3.19545526 0.         ... 0.         0.         0.        ]\n",
      " [0.1262409  3.73194603 1.47182754 ... 1.62559195 3.75598926 1.52966236]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99198638 0.03780381 0.98084457 ... 0.13712871 4.86714341 0.        ]\n",
      " [2.42939701 5.22196564 0.         ... 2.43674166 1.02484979 1.06212778]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7377473  0.         0.         ... 0.         0.62257986 0.        ]\n",
      " [1.2274492  4.90721452 1.71229633 ... 2.21448999 0.30881774 2.69303025]\n",
      " ...\n",
      " [3.19282235 1.42619255 1.68416827 ... 0.41331615 3.44475899 1.59690314]\n",
      " [3.38268046 0.         4.14904574 ... 3.2237148  2.04552746 2.17688488]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00199901 0.00311035 0.10175701 ... 0.26331867 0.00880369 0.10806952]\n",
      " [0.30788539 0.00054207 0.22503398 ... 0.06763043 0.07138326 0.0753037 ]\n",
      " [0.00974818 0.00154864 0.0738967  ... 0.0482669  0.30978579 0.08361195]\n",
      " ...\n",
      " [0.27991262 0.28656949 0.02950927 ... 0.04448712 0.00404994 0.05384178]\n",
      " [0.05486829 0.0025184  0.16615182 ... 0.14734483 0.11656909 0.06908982]\n",
      " [0.19227421 0.19256866 0.03969512 ... 0.03802728 0.02534562 0.15461034]] & cached\n",
      "Re-used Cached Value, runNum =  147\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  147\n",
      "Provided input from cache for runNum = 147\n",
      "Provided input from cache for runNum = 148\n",
      "activation = [[4.77571185 5.25362865 5.59944259 ... 8.16484813 3.61527446 2.97371611]\n",
      " [0.         3.19771792 0.         ... 0.         0.         0.        ]\n",
      " [0.12746852 3.73654201 1.4685592  ... 1.63015337 3.76404248 1.53349344]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99456716 0.04961343 0.99107897 ... 0.13666031 4.87840316 0.        ]\n",
      " [2.44931569 5.24934336 0.         ... 2.44449645 1.03726093 1.07492663]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.73985001 0.         0.         ... 0.         0.62016836 0.        ]\n",
      " [1.23433049 4.930972   1.72275274 ... 2.24904071 0.33467769 2.71706046]\n",
      " ...\n",
      " [3.214389   1.45290935 1.70381445 ... 0.41794413 3.47657622 1.60903581]\n",
      " [3.38825136 0.         4.18198095 ... 3.25750838 2.06585893 2.18538037]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0019096  0.00297716 0.10060661 ... 0.26584828 0.00863228 0.1076488 ]\n",
      " [0.30614336 0.00051797 0.22621593 ... 0.06625154 0.07005352 0.07422729]\n",
      " [0.00946265 0.00149714 0.07333834 ... 0.04757281 0.3108711  0.08291044]\n",
      " ...\n",
      " [0.28290102 0.28597918 0.02919006 ... 0.04362031 0.0039397  0.0537555 ]\n",
      " [0.05448024 0.00246537 0.16754308 ... 0.14715889 0.11526599 0.06888854]\n",
      " [0.19389982 0.19291481 0.03938025 ... 0.03741853 0.02495128 0.15541015]] & cached\n",
      "Re-used Cached Value, runNum =  148\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  148\n",
      "Provided input from cache for runNum = 148\n",
      "Provided input from cache for runNum = 149\n",
      "activation = [[4.7827545  5.25810982 5.6102252  ... 8.18786059 3.62819001 2.97775402]\n",
      " [0.         3.1999735  0.         ... 0.         0.         0.        ]\n",
      " [0.12874438 3.74116686 1.46536619 ... 1.63477696 3.77220249 1.53734122]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.99744224 0.06183236 1.0014578  ... 0.13660548 4.88997638 0.        ]\n",
      " [2.46924584 5.27672045 0.         ... 2.45235564 1.04974103 1.08776514]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74207598 0.         0.         ... 0.         0.61785747 0.        ]\n",
      " [1.24107859 4.95453413 1.73318783 ... 2.28368133 0.36051709 2.7412142 ]\n",
      " ...\n",
      " [3.23643058 1.48031734 1.72396625 ... 0.42338704 3.50879344 1.62144397]\n",
      " [3.39359048 0.         4.21465591 ... 3.29113465 2.08597237 2.193717  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00182275 0.00284843 0.09942627 ... 0.26830358 0.00845938 0.10720186]\n",
      " [0.30440421 0.00049497 0.2274001  ... 0.06488257 0.06873005 0.07315029]\n",
      " [0.00918727 0.00144816 0.07279635 ... 0.04690254 0.31191712 0.08220655]\n",
      " ...\n",
      " [0.28586231 0.28531889 0.02887465 ... 0.04275731 0.00383114 0.0536702 ]\n",
      " [0.05408891 0.00241433 0.16895269 ... 0.14699681 0.11396188 0.06868393]\n",
      " [0.19553465 0.1932667  0.03907911 ... 0.03682723 0.02456203 0.15624381]] & cached\n",
      "Re-used Cached Value, runNum =  149\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  149\n",
      "Provided input from cache for runNum = 149\n",
      "Provided input from cache for runNum = 150\n",
      "activation = [[4.78975401 5.26259836 5.62087902 ... 8.21088968 3.64108848 2.98178244]\n",
      " [0.         3.202209   0.         ... 0.         0.         0.        ]\n",
      " [0.13002174 3.74574061 1.4622126  ... 1.63940237 3.78043282 1.54117827]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00054045 0.07431618 1.01199748 ... 0.13684034 4.90169996 0.        ]\n",
      " [2.48907556 5.30390224 0.         ... 2.46012587 1.06216024 1.10054941]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74452426 0.         0.         ... 0.         0.61568516 0.        ]\n",
      " [1.24753374 4.97774804 1.74338947 ... 2.31822949 0.38612115 2.76534842]\n",
      " ...\n",
      " [3.25869826 1.5080563  1.74438789 ... 0.42927141 3.54108341 1.63397574]\n",
      " [3.39895018 0.         4.24718675 ... 3.32492991 2.10607732 2.20204328]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00173862 0.00272471 0.09822428 ... 0.27072567 0.00828738 0.10674167]\n",
      " [0.30272091 0.0004731  0.22860537 ... 0.06353148 0.06742942 0.07208234]\n",
      " [0.00892123 0.00140135 0.07226573 ... 0.04624792 0.31294617 0.08150251]\n",
      " ...\n",
      " [0.28879161 0.28463878 0.02856317 ... 0.04189556 0.0037245  0.05358376]\n",
      " [0.05369455 0.0023653  0.17038702 ... 0.14685598 0.11267403 0.06848052]\n",
      " [0.19713645 0.19362024 0.03878397 ... 0.03624061 0.02417464 0.1570891 ]] & cached\n",
      "Re-used Cached Value, runNum =  150\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  150\n",
      "Provided input from cache for runNum = 150\n",
      "Provided input from cache for runNum = 151\n",
      "activation = [[4.79664335 5.26691005 5.63137581 ... 8.23374488 3.65386654 2.98572847]\n",
      " [0.         3.20444822 0.         ... 0.         0.         0.        ]\n",
      " [0.13135547 3.75039036 1.45912284 ... 1.64418419 3.78882767 1.54505019]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00354357 0.08660119 1.02245071 ... 0.1368884  4.91317777 0.        ]\n",
      " [2.50881025 5.33105061 0.         ... 2.46796176 1.07456964 1.11332521]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.74696922 0.         0.         ... 0.         0.61330823 0.        ]\n",
      " [1.25382769 5.00080859 1.75351301 ... 2.35289246 0.41171802 2.78950446]\n",
      " ...\n",
      " [3.28072527 1.53544663 1.76467343 ... 0.43486354 3.57296062 1.64647543]\n",
      " [3.40401774 0.         4.27937958 ... 3.35830506 2.12584837 2.21018321]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00165739 0.00260547 0.09701783 ... 0.27317001 0.00812275 0.10626877]\n",
      " [0.30098505 0.00045208 0.2298194  ... 0.06218008 0.06615082 0.07101785]\n",
      " [0.00865882 0.0013554  0.07172026 ... 0.04557445 0.31393301 0.08078359]\n",
      " ...\n",
      " [0.29180292 0.28400809 0.02826827 ... 0.0410568  0.00362253 0.05350747]\n",
      " [0.05328044 0.00231673 0.17181169 ... 0.14666575 0.11142691 0.06827086]\n",
      " [0.19872582 0.1939343  0.03850485 ... 0.03567016 0.02380133 0.15796735]] & cached\n",
      "Re-used Cached Value, runNum =  151\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  151\n",
      "Provided input from cache for runNum = 151\n",
      "Provided input from cache for runNum = 152\n",
      "activation = [[4.80351973 5.27125133 5.64176078 ... 8.25666458 3.66666976 2.98968338]\n",
      " [0.         3.20659303 0.         ... 0.         0.         0.        ]\n",
      " [0.13266579 3.75496556 1.45605712 ... 1.64895083 3.79726091 1.54889364]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00671201 0.09906911 1.03297433 ... 0.1371561  4.92479118 0.        ]\n",
      " [2.52844608 5.35806488 0.         ... 2.47573678 1.08695018 1.12606362]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7495314  0.         0.         ... 0.         0.61096479 0.        ]\n",
      " [1.25979062 5.02347184 1.76339246 ... 2.38743752 0.43703341 2.81361051]\n",
      " ...\n",
      " [3.30297171 1.56315817 1.78516068 ... 0.44086127 3.60489619 1.65909074]\n",
      " [3.40904383 0.         4.31139505 ... 3.39184179 2.1455739  2.21830404]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00157896 0.00249106 0.09579457 ... 0.27558108 0.00795928 0.1057842 ]\n",
      " [0.29929662 0.00043211 0.23104903 ... 0.06084493 0.06489402 0.06996184]\n",
      " [0.00840552 0.0013115  0.07118411 ... 0.04491581 0.31490392 0.08006944]\n",
      " ...\n",
      " [0.29478407 0.28337329 0.02797699 ... 0.04021985 0.0035225  0.05342773]\n",
      " [0.05287056 0.00227038 0.17326583 ... 0.14650514 0.11020054 0.06806513]\n",
      " [0.20027825 0.19425157 0.03823012 ... 0.03510221 0.02342995 0.15885189]] & cached\n",
      "Re-used Cached Value, runNum =  152\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  152\n",
      "Provided input from cache for runNum = 152\n",
      "Provided input from cache for runNum = 153\n",
      "activation = [[4.81036959 5.27555944 5.65204171 ... 8.27960835 3.67948329 2.9936291 ]\n",
      " [0.         3.20880155 0.         ... 0.         0.         0.        ]\n",
      " [0.13402487 3.75958925 1.4530568  ... 1.65381149 3.80584716 1.55275065]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.00993085 0.11158788 1.04352418 ... 0.13748227 4.93638719 0.        ]\n",
      " [2.54802346 5.3850715  0.         ... 2.48356349 1.09932235 1.13883089]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7507241  0.         0.         ... 0.         0.60870876 0.        ]\n",
      " [1.26801665 5.04615646 1.77331324 ... 2.42226532 0.46246059 2.83784507]\n",
      " ...\n",
      " [3.32788349 1.59100851 1.80579466 ... 0.44709148 3.63679198 1.67182826]\n",
      " [3.41597401 0.         4.3432537  ... 3.42540652 2.16523432 2.22638498]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00150316 0.00238063 0.09455934 ... 0.27797839 0.00779895 0.10528261]\n",
      " [0.29765692 0.0004129  0.23227867 ... 0.05951508 0.06365399 0.0689081 ]\n",
      " [0.00816435 0.00126891 0.07064822 ... 0.04425682 0.31584518 0.07934973]\n",
      " ...\n",
      " [0.29756341 0.2827262  0.0276962  ... 0.03939254 0.00342517 0.05335246]\n",
      " [0.05241532 0.00222479 0.17472171 ... 0.1463213  0.1089882  0.06785473]\n",
      " [0.20192981 0.19454243 0.03796745 ... 0.03454443 0.02306597 0.15976168]] & cached\n",
      "Re-used Cached Value, runNum =  153\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  153\n",
      "Provided input from cache for runNum = 153\n",
      "iterations = 150\n",
      "Accuracy = 0.49690243902439024\n",
      "Provided input from cache for runNum = 154\n",
      "activation = [[4.81723842 5.27992294 5.66222844 ... 8.30260315 3.69235337 2.99759671]\n",
      " [0.         3.21104329 0.         ... 0.         0.         0.        ]\n",
      " [0.13536836 3.7641442  1.45007795 ... 1.65864029 3.81448255 1.55659   ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.01345535 0.12450412 1.05425581 ... 0.13823361 4.94826256 0.        ]\n",
      " [2.56746031 5.41192397 0.         ... 2.49134905 1.1116088  1.15157851]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75121881 0.         0.         ... 0.         0.60660874 0.        ]\n",
      " [1.27737264 5.06856178 1.78311534 ... 2.45706898 0.48777407 2.86215161]\n",
      " ...\n",
      " [3.35454012 1.61915259 1.82663957 ... 0.4537118  3.66879554 1.68466062]\n",
      " [3.42422382 0.         4.37505834 ... 3.4592203  2.18507008 2.23451999]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.0014301  0.00227453 0.09330741 ... 0.28034314 0.00763967 0.10476865]\n",
      " [0.2961199  0.00039458 0.23352632 ... 0.05820343 0.06243533 0.06786216]\n",
      " [0.00793506 0.0012281  0.07011919 ... 0.04361038 0.31677972 0.07862813]\n",
      " ...\n",
      " [0.30016197 0.28206851 0.02742056 ... 0.03856788 0.00332932 0.05327787]\n",
      " [0.05194461 0.00218092 0.17619504 ... 0.14615515 0.1077892  0.0676428 ]\n",
      " [0.20360689 0.19482799 0.03771012 ... 0.03398948 0.02270205 0.16068465]] & cached\n",
      "Re-used Cached Value, runNum =  154\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  154\n",
      "Provided input from cache for runNum = 154\n",
      "Provided input from cache for runNum = 155\n",
      "activation = [[4.82411915 5.28430848 5.67231984 ... 8.32561887 3.70526818 3.00157865]\n",
      " [0.         3.21325432 0.         ... 0.         0.         0.        ]\n",
      " [0.13667057 3.76860497 1.44710086 ... 1.66341983 3.82311969 1.56038196]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.01712939 0.13754434 1.06506187 ... 0.13914752 4.96026409 0.        ]\n",
      " [2.58681492 5.43867859 0.         ... 2.49917975 1.12386193 1.16429239]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75191122 0.         0.         ... 0.         0.60462978 0.        ]\n",
      " [1.28673267 5.09094304 1.79292633 ... 2.49205723 0.51313223 2.88657924]\n",
      " ...\n",
      " [3.381505   1.64768527 1.84785556 ... 0.46088672 3.70101678 1.69773536]\n",
      " [3.43256078 0.         4.40681222 ... 3.49319983 2.20502927 2.24273204]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00135945 0.00217182 0.09202758 ... 0.28264468 0.00748128 0.10423043]\n",
      " [0.29458531 0.00037703 0.23479382 ... 0.05691067 0.06123132 0.06682456]\n",
      " [0.00771174 0.00118847 0.06959332 ... 0.04297032 0.31769687 0.07790549]\n",
      " ...\n",
      " [0.30276206 0.28141613 0.02715427 ... 0.03775166 0.00323503 0.05320457]\n",
      " [0.05146628 0.00213791 0.17767869 ... 0.14599025 0.10659818 0.06742772]\n",
      " [0.2052708  0.19511295 0.03746454 ... 0.03344556 0.02234017 0.16163067]] & cached\n",
      "Re-used Cached Value, runNum =  155\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  155\n",
      "Provided input from cache for runNum = 155\n",
      "Provided input from cache for runNum = 156\n",
      "activation = [[4.83101066 5.28873465 5.68231659 ... 8.3486838  3.71821345 3.00556796]\n",
      " [0.         3.21533962 0.         ... 0.         0.         0.        ]\n",
      " [0.13798078 3.77304294 1.44416461 ... 1.66822852 3.83187027 1.56416595]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02084898 0.15057758 1.07587483 ... 0.14008708 4.97220524 0.        ]\n",
      " [2.60593103 5.46514055 0.         ... 2.50681701 1.13596996 1.176908  ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75273723 0.         0.         ... 0.         0.60263596 0.        ]\n",
      " [1.29592388 5.1130971  1.80263309 ... 2.52707907 0.53844726 2.91103102]\n",
      " ...\n",
      " [3.40851361 1.6762196  1.86916636 ... 0.46819313 3.73313222 1.71090023]\n",
      " [3.44109673 0.         4.43853303 ... 3.52751888 2.22520932 2.25108785]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00129172 0.00207355 0.09074029 ... 0.28493969 0.00732693 0.10368371]\n",
      " [0.29311091 0.00036033 0.23608125 ... 0.05563604 0.06005068 0.06580003]\n",
      " [0.00749545 0.00115019 0.06906996 ... 0.04233262 0.31861028 0.07718264]\n",
      " ...\n",
      " [0.30532191 0.28079123 0.02689274 ... 0.03694099 0.00314302 0.05312804]\n",
      " [0.05098846 0.00209619 0.17916735 ... 0.14581929 0.1054359  0.06721249]\n",
      " [0.20689215 0.19540911 0.03722362 ... 0.03290311 0.02198088 0.1625811 ]] & cached\n",
      "Re-used Cached Value, runNum =  156\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  156\n",
      "Provided input from cache for runNum = 156\n",
      "Provided input from cache for runNum = 157\n",
      "activation = [[4.83781286 5.29305582 5.69218019 ... 8.37163199 3.73107343 3.00950787]\n",
      " [0.         3.21739128 0.         ... 0.         0.         0.        ]\n",
      " [0.13930266 3.77747156 1.4412499  ... 1.67307383 3.84070164 1.56794481]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02462226 0.16361234 1.08672322 ... 0.14104906 4.98410523 0.        ]\n",
      " [2.62498547 5.49153921 0.         ... 2.51449212 1.14807984 1.18951107]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75364722 0.         0.         ... 0.         0.60062916 0.        ]\n",
      " [1.30495765 5.13509564 1.81220996 ... 2.56216926 0.56372461 2.93549855]\n",
      " ...\n",
      " [3.435751   1.70509516 1.89081841 ... 0.47597618 3.76539655 1.72429011]\n",
      " [3.44935565 0.         4.46994631 ... 3.56157712 2.24512908 2.2592812 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00122654 0.00197916 0.08943191 ... 0.28717344 0.00717391 0.10311307]\n",
      " [0.29161501 0.00034438 0.23737758 ... 0.0543741  0.05888102 0.06478011]\n",
      " [0.0072857  0.00111351 0.06855718 ... 0.04170528 0.31948162 0.07646095]\n",
      " ...\n",
      " [0.3078789  0.28013445 0.02663721 ... 0.03614368 0.00305336 0.0530519 ]\n",
      " [0.05050892 0.00205584 0.18067349 ... 0.14565265 0.10428579 0.06699665]\n",
      " [0.2085153  0.1957002  0.03699423 ... 0.03237715 0.02162897 0.16356102]] & cached\n",
      "Re-used Cached Value, runNum =  157\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  157\n",
      "Provided input from cache for runNum = 157\n",
      "Provided input from cache for runNum = 158\n",
      "activation = [[4.84457592 5.29737429 5.70191826 ... 8.39454308 3.74393423 3.01343745]\n",
      " [0.         3.21941157 0.         ... 0.         0.         0.        ]\n",
      " [0.14067555 3.78193319 1.43840769 ... 1.67800555 3.84965524 1.57174102]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.02856787 0.17683694 1.09767088 ... 0.14222431 4.99612571 0.        ]\n",
      " [2.64384623 5.51768603 0.         ... 2.52205405 1.16005781 1.20200876]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75475404 0.         0.         ... 0.         0.59873487 0.        ]\n",
      " [1.31382561 5.15685814 1.82168735 ... 2.59726056 0.58893959 2.95999137]\n",
      " ...\n",
      " [3.46304936 1.73403851 1.91256623 ... 0.48392687 3.79758992 1.73771705]\n",
      " [3.45758412 0.         4.5011632  ... 3.59565479 2.2651049  2.26746609]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[0.00116412 0.00188887 0.0881202  ... 0.2893991  0.00702373 0.10253644]\n",
      " [0.29014335 0.00032916 0.2386725  ... 0.05312342 0.05773309 0.063767  ]\n",
      " [0.00708332 0.00107829 0.06805295 ... 0.0410871  0.32033724 0.07573814]\n",
      " ...\n",
      " [0.31040807 0.27947379 0.02638757 ... 0.03535399 0.00296591 0.05297537]\n",
      " [0.05003119 0.00201681 0.18219091 ... 0.14548601 0.10315516 0.06678069]\n",
      " [0.2101092  0.19597847 0.03677135 ... 0.03185632 0.02128113 0.16455057]] & cached\n",
      "Re-used Cached Value, runNum =  158\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  158\n",
      "Provided input from cache for runNum = 158\n",
      "Provided input from cache for runNum = 159\n",
      "activation = [[4.85135066 5.30175551 5.7115898  ... 8.4175287  3.75689971 3.01738922]\n",
      " [0.         3.22132851 0.         ... 0.         0.         0.        ]\n",
      " [0.1420551  3.78635551 1.43558171 ... 1.68295375 3.85866825 1.57552851]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.03260831 0.19015892 1.10866172 ... 0.14356879 5.00814744 0.        ]\n",
      " [2.66267106 5.54374873 0.         ... 2.52961304 1.17202168 1.21449846]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75588639 0.         0.         ... 0.         0.59674722 0.        ]\n",
      " [1.32248408 5.17834231 1.83101591 ... 2.63231464 0.61409509 2.98450906]\n",
      " ...\n",
      " [3.49046636 1.76318158 1.93450526 ... 0.49218221 3.82978441 1.75127324]\n",
      " [3.46575523 0.         4.53227058 ... 3.62981509 2.28513891 2.27564633]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.10406389e-03 1.80189250e-03 8.67892451e-02 ... 2.91573714e-01\n",
      "  6.87671112e-03 1.01941450e-01]\n",
      " [2.88677730e-01 3.14669544e-04 2.40007152e-01 ... 5.18974337e-02\n",
      "  5.66037348e-02 6.27650205e-02]\n",
      " [6.88546119e-03 1.04416053e-03 6.75455190e-02 ... 4.04728893e-02\n",
      "  3.21176692e-01 7.50117134e-02]\n",
      " ...\n",
      " [3.12951645e-01 2.78835861e-01 2.61460594e-02 ... 3.45765114e-02\n",
      "  2.88070703e-03 5.29013281e-02]\n",
      " [4.95473813e-02 1.97873765e-03 1.83711262e-01 ... 1.45319887e-01\n",
      "  1.02050507e-01 6.65619876e-02]\n",
      " [2.11676668e-01 1.96258506e-01 3.65570823e-02 ... 3.13450471e-02\n",
      "  2.09373557e-02 1.65562781e-01]] & cached\n",
      "Re-used Cached Value, runNum =  159\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  159\n",
      "Provided input from cache for runNum = 159\n",
      "Provided input from cache for runNum = 160\n",
      "activation = [[4.85805804 5.30607328 5.72112792 ... 8.4404387  3.7698069  3.02131003]\n",
      " [0.         3.2232545  0.         ... 0.         0.         0.        ]\n",
      " [0.14352654 3.79085976 1.43284264 ... 1.6880586  3.86785232 1.57935914]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.03675312 0.20366735 1.11969987 ... 0.1450527  5.02024572 0.        ]\n",
      " [2.68144765 5.5697396  0.         ... 2.53722677 1.18400662 1.22700274]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.75714326 0.         0.         ... 0.         0.59480937 0.        ]\n",
      " [1.33112668 5.19974671 1.84036532 ... 2.66755644 0.63927689 3.00909252]\n",
      " ...\n",
      " [3.5178661  1.79238315 1.95646839 ... 0.50052426 3.8618634  1.7648367 ]\n",
      " [3.47363528 0.         4.56304584 ... 3.66370719 2.30492466 2.28364329]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.04667976e-03 1.71890839e-03 8.54642857e-02 ... 2.93740781e-01\n",
      "  6.73292546e-03 1.01338148e-01]\n",
      " [2.87177878e-01 3.00819532e-04 2.41325431e-01 ... 5.06757755e-02\n",
      "  5.54905043e-02 6.17650329e-02]\n",
      " [6.69348836e-03 1.01143859e-03 6.70417716e-02 ... 3.98605326e-02\n",
      "  3.21975867e-01 7.42780744e-02]\n",
      " ...\n",
      " [3.15499062e-01 2.78176674e-01 2.59110173e-02 ... 3.38133890e-02\n",
      "  2.79828426e-03 5.28308456e-02]\n",
      " [4.90591570e-02 1.94179046e-03 1.85227401e-01 ... 1.45128387e-01\n",
      "  1.00963383e-01 6.63388000e-02]\n",
      " [2.13244979e-01 1.96531165e-01 3.63527506e-02 ... 3.08461579e-02\n",
      "  2.06029066e-02 1.66601351e-01]] & cached\n",
      "Re-used Cached Value, runNum =  160\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  160\n",
      "Provided input from cache for runNum = 160\n",
      "Provided input from cache for runNum = 161\n",
      "activation = [[4.86475345 5.31037496 5.73055911 ... 8.46334636 3.78267937 3.02523281]\n",
      " [0.         3.2251368  0.         ... 0.         0.         0.        ]\n",
      " [0.14501421 3.79536242 1.43014971 ... 1.69323674 3.87715932 1.58318801]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0410184  0.21729616 1.13081317 ... 0.14666726 5.03238131 0.        ]\n",
      " [2.70015989 5.59570494 0.         ... 2.54489498 1.19602471 1.23950253]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.7584705  0.         0.         ... 0.         0.59282202 0.        ]\n",
      " [1.33963721 5.22093369 1.84959738 ... 2.70282665 0.66439276 3.03369875]\n",
      " ...\n",
      " [3.5453382  1.82163303 1.97855753 ... 0.50904223 3.8938585  1.7784631 ]\n",
      " [3.48141472 0.         4.59360466 ... 3.6974707  2.3245268  2.29155696]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.91752599e-04 1.63942104e-03 8.41319304e-02 ... 2.95880541e-01\n",
      "  6.59223600e-03 1.00723394e-01]\n",
      " [2.85678387e-01 2.87600088e-04 2.42654625e-01 ... 4.94679974e-02\n",
      "  5.43950184e-02 6.07723428e-02]\n",
      " [6.50699298e-03 9.79846145e-04 6.65427872e-02 ... 3.92531454e-02\n",
      "  3.22743949e-01 7.35431021e-02]\n",
      " ...\n",
      " [3.18049205e-01 2.77534573e-01 2.56822248e-02 ... 3.30631093e-02\n",
      "  2.71840822e-03 5.27619858e-02]\n",
      " [4.85721386e-02 1.90596619e-03 1.86756643e-01 ... 1.44934987e-01\n",
      "  9.98995925e-02 6.61149761e-02]\n",
      " [2.14785615e-01 1.96790072e-01 3.61550568e-02 ... 3.03564055e-02\n",
      "  2.02756680e-02 1.67658244e-01]] & cached\n",
      "Re-used Cached Value, runNum =  161\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  161\n",
      "Provided input from cache for runNum = 161\n",
      "Provided input from cache for runNum = 162\n",
      "activation = [[4.87140659 5.31464304 5.739873   ... 8.4862217  3.79552635 3.029135  ]\n",
      " [0.         3.2269707  0.         ... 0.         0.         0.        ]\n",
      " [0.14650181 3.79981342 1.42748272 ... 1.69844188 3.88654761 1.58698732]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.04534752 0.23098189 1.14193048 ... 0.14834677 5.04456358 0.        ]\n",
      " [2.71867538 5.62136921 0.         ... 2.55239078 1.20785102 1.25188921]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.76002115 0.         0.         ... 0.         0.59101471 0.        ]\n",
      " [1.34784847 5.2417214  1.85862326 ... 2.73800033 0.68925086 3.05824696]\n",
      " ...\n",
      " [3.57274468 1.85078894 2.00060254 ... 0.5175177  3.92565596 1.79207781]\n",
      " [3.48930076 0.         4.62405108 ... 3.7314306  2.34423979 2.29953466]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.39143041e-04 1.56327019e-03 8.27942198e-02 ... 2.98021648e-01\n",
      "  6.45416488e-03 1.00103420e-01]\n",
      " [2.84230105e-01 2.75018255e-04 2.44011437e-01 ... 4.82804059e-02\n",
      "  5.33277526e-02 5.97930686e-02]\n",
      " [6.32609845e-03 9.49295280e-04 6.60440777e-02 ... 3.86489436e-02\n",
      "  3.23510381e-01 7.28098335e-02]\n",
      " ...\n",
      " [3.20584578e-01 2.76918299e-01 2.54618291e-02 ... 3.23216743e-02\n",
      "  2.64043473e-03 5.26929783e-02]\n",
      " [4.80848955e-02 1.87115733e-03 1.88294532e-01 ... 1.44739010e-01\n",
      "  9.88596012e-02 6.58938861e-02]\n",
      " [2.16269687e-01 1.97031091e-01 3.59620016e-02 ... 2.98676704e-02\n",
      "  1.99503006e-02 1.68717933e-01]] & cached\n",
      "Re-used Cached Value, runNum =  162\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  162\n",
      "Provided input from cache for runNum = 162\n",
      "Provided input from cache for runNum = 163\n",
      "activation = [[4.87797155 5.3187912  5.74903271 ... 8.50894345 3.8082709  3.03298225]\n",
      " [0.         3.22878476 0.         ... 0.         0.         0.        ]\n",
      " [0.14798495 3.80425902 1.42482991 ... 1.70370363 3.89601555 1.59077889]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.04978924 0.24477017 1.15311003 ... 0.15014882 5.05676572 0.        ]\n",
      " [2.73712803 5.64701386 0.         ... 2.55998268 1.21973329 1.26427536]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.76183161 0.         0.         ... 0.         0.58931321 0.        ]\n",
      " [1.35581508 5.26226854 1.8674622  ... 2.77314926 0.71399846 3.08276999]\n",
      " ...\n",
      " [3.60038977 1.88030906 2.02296544 ... 0.52648867 3.95753613 1.80586945]\n",
      " [3.49685642 0.         4.65411454 ... 3.76498389 2.36352777 2.30727716]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.88522331e-04 1.48977001e-03 8.14353404e-02 ... 3.00081917e-01\n",
      "  6.31677715e-03 9.94581423e-02]\n",
      " [2.82766237e-01 2.62969414e-04 2.45375423e-01 ... 4.71084617e-02\n",
      "  5.22726354e-02 5.88178027e-02]\n",
      " [6.15050827e-03 9.19804424e-04 6.55550616e-02 ... 3.80574908e-02\n",
      "  3.24225293e-01 7.20795848e-02]\n",
      " ...\n",
      " [3.23122044e-01 2.76291584e-01 2.52500348e-02 ... 3.15958667e-02\n",
      "  2.56461296e-03 5.26281933e-02]\n",
      " [4.75971822e-02 1.83733161e-03 1.89862807e-01 ... 1.44562773e-01\n",
      "  9.78259530e-02 6.56737044e-02]\n",
      " [2.17747869e-01 1.97250123e-01 3.57805413e-02 ... 2.93952627e-02\n",
      "  1.96330182e-02 1.69811699e-01]] & cached\n",
      "Re-used Cached Value, runNum =  163\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  163\n",
      "Provided input from cache for runNum = 163\n",
      "Provided input from cache for runNum = 164\n",
      "activation = [[4.88448397 5.32286551 5.75804841 ... 8.5315685  3.82094254 3.03680134]\n",
      " [0.         3.2306066  0.         ... 0.         0.         0.        ]\n",
      " [0.14949343 3.80871954 1.42222327 ... 1.70907627 3.90559858 1.59457969]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.05442306 0.25882053 1.16441633 ... 0.15218574 5.06918576 0.        ]\n",
      " [2.75552063 5.67256905 0.         ... 2.56762129 1.23162298 1.27663311]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.76388288 0.         0.         ... 0.         0.58777443 0.        ]\n",
      " [1.36368454 5.28263434 1.87622292 ... 2.80832598 0.73861922 3.10731006]\n",
      " ...\n",
      " [3.62822882 1.91014374 2.04558127 ... 0.53586572 3.98953565 1.81977682]\n",
      " [3.50415592 0.         4.68382528 ... 3.79821965 2.38248876 2.31483277]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.40224185e-04 1.41975017e-03 8.00727089e-02 ... 3.02089037e-01\n",
      "  6.17978666e-03 9.88001050e-02]\n",
      " [2.81302107e-01 2.51482343e-04 2.46725131e-01 ... 4.59487673e-02\n",
      "  5.12312364e-02 5.78466266e-02]\n",
      " [5.98203140e-03 8.91854867e-04 6.50825826e-02 ... 3.74807874e-02\n",
      "  3.24898675e-01 7.13524753e-02]\n",
      " ...\n",
      " [3.25620403e-01 2.75619063e-01 2.50405160e-02 ... 3.08819459e-02\n",
      "  2.49062811e-03 5.25620231e-02]\n",
      " [4.71155203e-02 1.80505966e-03 1.91450277e-01 ... 1.44397639e-01\n",
      "  9.67963340e-02 6.54535742e-02]\n",
      " [2.19219366e-01 1.97462766e-01 3.56056651e-02 ... 2.89349894e-02\n",
      "  1.93218272e-02 1.70925801e-01]] & cached\n",
      "Re-used Cached Value, runNum =  164\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  164\n",
      "Provided input from cache for runNum = 164\n",
      "Provided input from cache for runNum = 165\n",
      "activation = [[4.89106343 5.32707124 5.76699389 ... 8.55426455 3.83368005 3.04067556]\n",
      " [0.         3.23234542 0.         ... 0.         0.         0.        ]\n",
      " [0.15096934 3.81307706 1.41963867 ... 1.71441869 3.91521186 1.5983431 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.05930064 0.27314532 1.17588945 ... 0.15454437 5.081814   0.        ]\n",
      " [2.77376426 5.69797785 0.         ... 2.57521919 1.24342636 1.2889415 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.76597736 0.         0.         ... 0.         0.58624066 0.        ]\n",
      " [1.37129707 5.30271731 1.88480137 ... 2.84343698 0.76306751 3.13183977]\n",
      " ...\n",
      " [3.65619064 1.94025103 2.06837833 ... 0.54561162 4.02157763 1.83375277]\n",
      " [3.5117155  0.         4.71355161 ... 3.83181907 2.40167558 2.32249525]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.94228992e-04 1.35266779e-03 7.87019224e-02 ... 3.04051556e-01\n",
      "  6.04497531e-03 9.81348106e-02]\n",
      " [2.79904732e-01 2.40561494e-04 2.48102361e-01 ... 4.48140283e-02\n",
      "  5.02118165e-02 5.68887839e-02]\n",
      " [5.81957500e-03 8.64959599e-04 6.46177891e-02 ... 3.69159469e-02\n",
      "  3.25577423e-01 7.06301929e-02]\n",
      " ...\n",
      " [3.28085152e-01 2.74970562e-01 2.48334828e-02 ... 3.01733131e-02\n",
      "  2.41810089e-03 5.24938440e-02]\n",
      " [4.66461435e-02 1.77404334e-03 1.93059261e-01 ... 1.44256232e-01\n",
      "  9.57895579e-02 6.52358245e-02]\n",
      " [2.20619490e-01 1.97679533e-01 3.54298969e-02 ... 2.84747476e-02\n",
      "  1.90105761e-02 1.72040591e-01]] & cached\n",
      "Re-used Cached Value, runNum =  165\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  165\n",
      "Provided input from cache for runNum = 165\n",
      "Provided input from cache for runNum = 166\n",
      "activation = [[4.89763694 5.33126027 5.77585156 ... 8.57694676 3.84640835 3.0445431 ]\n",
      " [0.         3.23412463 0.         ... 0.         0.         0.        ]\n",
      " [0.15244269 3.81742203 1.41707873 ... 1.71980501 3.92491155 1.60209652]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.06426269 0.2875359  1.18737505 ... 0.15698613 5.09444243 0.        ]\n",
      " [2.79188371 5.72322922 0.         ... 2.58278212 1.25515964 1.30120363]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.76812425 0.         0.         ... 0.         0.5846567  0.        ]\n",
      " [1.37871786 5.3225623  1.89322775 ... 2.87851534 0.78735676 3.15633353]\n",
      " ...\n",
      " [3.68402463 1.97019119 2.09111378 ... 0.55530226 4.05335633 1.84768293]\n",
      " [3.5191544  0.         4.74304185 ... 3.86535321 2.42073232 2.33007358]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.50416481e-04 1.28855167e-03 7.73338923e-02 ... 3.06007175e-01\n",
      "  5.91419952e-03 9.74645258e-02]\n",
      " [2.78486715e-01 2.30105365e-04 2.49484589e-01 ... 4.36932915e-02\n",
      "  4.92153055e-02 5.59386228e-02]\n",
      " [5.66097077e-03 8.38821060e-04 6.41500798e-02 ... 3.63494668e-02\n",
      "  3.26236842e-01 6.99073684e-02]\n",
      " ...\n",
      " [3.30587161e-01 2.74366175e-01 2.46359822e-02 ... 2.94783943e-02\n",
      "  2.34807831e-03 5.24291986e-02]\n",
      " [4.61784534e-02 1.74390313e-03 1.94681495e-01 ... 1.44109709e-01\n",
      "  9.48117799e-02 6.50179816e-02]\n",
      " [2.21979368e-01 1.97866641e-01 3.52601650e-02 ... 2.80192198e-02\n",
      "  1.87061570e-02 1.73169033e-01]] & cached\n",
      "Re-used Cached Value, runNum =  166\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  166\n",
      "Provided input from cache for runNum = 166\n",
      "Provided input from cache for runNum = 167\n",
      "activation = [[4.90418158 5.33544421 5.78459198 ... 8.59961988 3.85908252 3.04840198]\n",
      " [0.         3.23579736 0.         ... 0.         0.         0.        ]\n",
      " [0.15393228 3.82174207 1.41455335 ... 1.72521725 3.93469602 1.60583241]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.06938696 0.30215413 1.19896345 ... 0.15974178 5.10722923 0.        ]\n",
      " [2.80984315 5.74826656 0.         ... 2.59022851 1.26679922 1.31339476]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.77035773 0.         0.         ... 0.         0.58311704 0.        ]\n",
      " [1.38582989 5.34196653 1.90139623 ... 2.91332561 0.81125958 3.18070611]\n",
      " ...\n",
      " [3.71191099 2.00026979 2.11401552 ... 0.56536054 4.0851148  1.86165375]\n",
      " [3.52648808 0.         4.77227976 ... 3.89894747 2.4396171  2.33758833]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.08789271e-04 1.22761002e-03 7.59612397e-02 ... 3.07906847e-01\n",
      "  5.78465919e-03 9.67859222e-02]\n",
      " [2.77111741e-01 2.20205129e-04 2.50885570e-01 ... 4.26007664e-02\n",
      "  4.82399559e-02 5.50014668e-02]\n",
      " [5.50896381e-03 8.14043762e-04 6.36978447e-02 ... 3.58022858e-02\n",
      "  3.26878298e-01 6.91905343e-02]\n",
      " ...\n",
      " [3.33034147e-01 2.73737831e-01 2.44391710e-02 ... 2.87914078e-02\n",
      "  2.27975120e-03 5.23614528e-02]\n",
      " [4.57199565e-02 1.71525679e-03 1.96315794e-01 ... 1.43989130e-01\n",
      "  9.38506472e-02 6.48026392e-02]\n",
      " [2.23305368e-01 1.98059659e-01 3.50932272e-02 ... 2.75695083e-02\n",
      "  1.84049213e-02 1.74305518e-01]] & cached\n",
      "Re-used Cached Value, runNum =  167\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  167\n",
      "Provided input from cache for runNum = 167\n",
      "Provided input from cache for runNum = 168\n",
      "activation = [[4.91069043 5.3396062  5.79321136 ... 8.62225038 3.87172701 3.05225005]\n",
      " [0.         3.23742631 0.         ... 0.         0.         0.        ]\n",
      " [0.15540504 3.82600443 1.41203737 ... 1.73062595 3.94452958 1.60953009]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.07465994 0.31685831 1.21058931 ... 0.16260109 5.12006361 0.        ]\n",
      " [2.82772675 5.7732108  0.         ... 2.59770861 1.2784164  1.32558164]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.77284598 0.         0.         ... 0.         0.58172462 0.        ]\n",
      " [1.39277615 5.36071042 1.90942217 ... 2.94810916 0.83506722 3.20507163]\n",
      " ...\n",
      " [3.73975336 2.03000333 2.13686282 ... 0.57538073 4.11666748 1.87559455]\n",
      " [3.53374809 0.         4.80129343 ... 3.93245578 2.45838771 2.34502785]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.69147996e-04 1.16728636e-03 7.45941804e-02 ... 3.09792154e-01\n",
      "  5.65808194e-03 9.61019087e-02]\n",
      " [2.75754319e-01 2.10666572e-04 2.52293403e-01 ... 4.15231911e-02\n",
      "  4.72859177e-02 5.40729474e-02]\n",
      " [5.36144580e-03 7.88443334e-04 6.32430209e-02 ... 3.52534770e-02\n",
      "  3.27496366e-01 6.84715113e-02]\n",
      " ...\n",
      " [3.35468978e-01 2.73298200e-01 2.42492420e-02 ... 2.81187852e-02\n",
      "  2.21355021e-03 5.22955416e-02]\n",
      " [4.52617411e-02 1.68545650e-03 1.97956178e-01 ... 1.43860830e-01\n",
      "  9.29059976e-02 6.45844090e-02]\n",
      " [2.24600497e-01 1.98186943e-01 3.49310770e-02 ... 2.71258091e-02\n",
      "  1.81100566e-02 1.75458212e-01]] & cached\n",
      "Re-used Cached Value, runNum =  168\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  168\n",
      "Provided input from cache for runNum = 168\n",
      "Provided input from cache for runNum = 169\n",
      "activation = [[4.91719169 5.3437624  5.80172256 ... 8.64485153 3.88434539 3.05610379]\n",
      " [0.         3.23903776 0.         ... 0.         0.         0.        ]\n",
      " [0.15684614 3.83019188 1.40952381 ... 1.73600644 3.95437724 1.61319549]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.08020699 0.33195823 1.22238183 ... 0.16588735 5.13319068 0.        ]\n",
      " [2.84547082 5.79797991 0.         ... 2.6051341  1.28995791 1.33771345]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.77571162 0.         0.         ... 0.         0.58065445 0.        ]\n",
      " [1.39956973 5.37853642 1.91730294 ... 2.98276826 0.85874823 3.22947219]\n",
      " ...\n",
      " [3.76793647 2.05993181 2.1600517  ... 0.5860165  4.14853695 1.88972343]\n",
      " [3.54106002 0.         4.83013922 ... 3.96600666 2.47721396 2.35247221]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.31250663e-04 1.10665508e-03 7.32126248e-02 ... 3.11578078e-01\n",
      "  5.53055244e-03 9.54003314e-02]\n",
      " [2.74474523e-01 2.01501043e-04 2.53737919e-01 ... 4.04752367e-02\n",
      "  4.63458414e-02 5.31569903e-02]\n",
      " [5.21938011e-03 7.61889799e-04 6.27961431e-02 ... 3.47222585e-02\n",
      "  3.28096264e-01 6.77552593e-02]\n",
      " ...\n",
      " [3.37820221e-01 2.73047649e-01 2.40614865e-02 ... 2.74567259e-02\n",
      "  2.14811841e-03 5.22261893e-02]\n",
      " [4.48034188e-02 1.65410936e-03 1.99596389e-01 ... 1.43759101e-01\n",
      "  9.19545917e-02 6.43628071e-02]\n",
      " [2.25877344e-01 1.98254711e-01 3.47750439e-02 ... 2.66926291e-02\n",
      "  1.78152230e-02 1.76630616e-01]] & cached\n",
      "Re-used Cached Value, runNum =  169\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  169\n",
      "Provided input from cache for runNum = 169\n",
      "Provided input from cache for runNum = 170\n",
      "activation = [[4.92368212 5.34790447 5.81013469 ... 8.66740203 3.89692406 3.05995236]\n",
      " [0.         3.24060432 0.         ... 0.         0.         0.        ]\n",
      " [0.15828299 3.83434509 1.40703933 ... 1.74140736 3.96427108 1.6168473 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.08592959 0.34727063 1.23423701 ... 0.16942403 5.14645514 0.        ]\n",
      " [2.8630192  5.82249344 0.         ... 2.61243366 1.30137177 1.34972369]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.77870879 0.         0.         ... 0.         0.57968097 0.        ]\n",
      " [1.40622264 5.39614863 1.92508964 ... 3.01736291 0.88229319 3.2538337 ]\n",
      " ...\n",
      " [3.79620513 2.09008041 2.18336049 ... 0.59699179 4.18044444 1.90391178]\n",
      " [3.54840255 0.         4.85879495 ... 3.99967435 2.49605944 2.35993579]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.95448835e-04 1.04938556e-03 7.18416542e-02 ... 3.13329846e-01\n",
      "  5.40477941e-03 9.46974856e-02]\n",
      " [2.73234632e-01 1.92791339e-04 2.55164166e-01 ... 3.94448306e-02\n",
      "  4.54228509e-02 5.22509018e-02]\n",
      " [5.08414702e-03 7.36821653e-04 6.23653976e-02 ... 3.42077757e-02\n",
      "  3.28683024e-01 6.70470939e-02]\n",
      " ...\n",
      " [3.40103107e-01 2.72753746e-01 2.38731295e-02 ... 2.68012807e-02\n",
      "  2.08421922e-03 5.21500668e-02]\n",
      " [4.43594783e-02 1.62425939e-03 2.01250617e-01 ... 1.43675451e-01\n",
      "  9.10162561e-02 6.41442053e-02]\n",
      " [2.27120094e-01 1.98326161e-01 3.46194180e-02 ... 2.62626466e-02\n",
      "  1.75232631e-02 1.77799858e-01]] & cached\n",
      "Re-used Cached Value, runNum =  170\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  170\n",
      "Provided input from cache for runNum = 170\n",
      "Provided input from cache for runNum = 171\n",
      "activation = [[4.93010696 5.3519865  5.8184207  ... 8.68987132 3.90942032 3.06377485]\n",
      " [0.         3.24215614 0.         ... 0.         0.         0.        ]\n",
      " [0.15971058 3.83844081 1.40456534 ... 1.74679961 3.97421401 1.62046749]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.09166855 0.3626136  1.2460957  ... 0.17300527 5.15966214 0.        ]\n",
      " [2.88057696 5.84696042 0.         ... 2.61978384 1.31279907 1.36173214]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.78177784 0.         0.         ... 0.         0.57871354 0.        ]\n",
      " [1.41281011 5.41356816 1.93278005 ... 3.05198077 0.90581763 3.27821079]\n",
      " ...\n",
      " [3.82451741 2.12032659 2.20679155 ... 0.6081601  4.21230766 1.91821047]\n",
      " [3.55546118 0.         4.8871769  ... 4.03313832 2.51470371 2.36729346]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.61258186e-04 9.94634550e-04 7.04662314e-02 ... 3.15026446e-01\n",
      "  5.28165031e-03 9.39774762e-02]\n",
      " [2.71935112e-01 1.84461186e-04 2.56608941e-01 ... 3.84339230e-02\n",
      "  4.45147775e-02 5.13540145e-02]\n",
      " [4.95118019e-03 7.12524431e-04 6.19326082e-02 ... 3.36947047e-02\n",
      "  3.29234730e-01 6.63373739e-02]\n",
      " ...\n",
      " [3.42442733e-01 2.72488340e-01 2.36946335e-02 ... 2.61629567e-02\n",
      "  2.02237771e-03 5.20782916e-02]\n",
      " [4.39053905e-02 1.59501642e-03 2.02900444e-01 ... 1.43582834e-01\n",
      "  9.00928218e-02 6.39212524e-02]\n",
      " [2.28356201e-01 1.98382938e-01 3.44742105e-02 ... 2.58448302e-02\n",
      "  1.72384649e-02 1.78998799e-01]] & cached\n",
      "Re-used Cached Value, runNum =  171\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  171\n",
      "Provided input from cache for runNum = 171\n",
      "Provided input from cache for runNum = 172\n",
      "activation = [[4.93651999 5.35602168 5.82658552 ... 8.71226752 3.92188157 3.06756342]\n",
      " [0.         3.24366242 0.         ... 0.         0.         0.        ]\n",
      " [0.16107116 3.84244246 1.40205835 ... 1.75214513 3.98413147 1.62403574]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.0973562  0.37786623 1.25789659 ... 0.17649886 5.17275093 0.        ]\n",
      " [2.8980186  5.87131058 0.         ... 2.62715404 1.32418868 1.37371631]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.78494083 0.         0.         ... 0.         0.57773991 0.        ]\n",
      " [1.41933635 5.430858   1.94036797 ... 3.08660773 0.92920822 3.30252616]\n",
      " ...\n",
      " [3.85270448 2.15048935 2.23020514 ... 0.61936146 4.24391594 1.93252963]\n",
      " [3.56233428 0.         4.9152366  ... 4.06639823 2.53311014 2.37451302]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.28910716e-04 9.42875667e-04 6.91030023e-02 ... 3.16696929e-01\n",
      "  5.16206540e-03 9.32520792e-02]\n",
      " [2.70608344e-01 1.76512128e-04 2.58037853e-01 ... 3.74357607e-02\n",
      "  4.36261870e-02 5.04649995e-02]\n",
      " [4.82179420e-03 6.89186666e-04 6.15003017e-02 ... 3.31804543e-02\n",
      "  3.29759591e-01 6.56320652e-02]\n",
      " ...\n",
      " [3.44803739e-01 2.72257284e-01 2.35219022e-02 ... 2.55403254e-02\n",
      "  1.96280246e-03 5.20063596e-02]\n",
      " [4.34569220e-02 1.56695337e-03 2.04568317e-01 ... 1.43490228e-01\n",
      "  8.91924272e-02 6.36993281e-02]\n",
      " [2.29565403e-01 1.98433582e-01 3.43333274e-02 ... 2.54344005e-02\n",
      "  1.69613382e-02 1.80210456e-01]] & cached\n",
      "Re-used Cached Value, runNum =  172\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  172\n",
      "Provided input from cache for runNum = 172\n",
      "Provided input from cache for runNum = 173\n",
      "activation = [[4.94288622 5.35998418 5.83463713 ... 8.73454416 3.93421765 3.07131751]\n",
      " [0.         3.24510987 0.         ... 0.         0.         0.        ]\n",
      " [0.16244832 3.84643744 1.39957308 ... 1.75753419 3.9941372  1.62758511]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.10297434 0.39298162 1.2696432  ... 0.17988736 5.18567684 0.        ]\n",
      " [2.91537335 5.8955717  0.         ... 2.63457832 1.33557687 1.38568049]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.78796581 0.         0.         ... 0.         0.57659935 0.        ]\n",
      " [1.42577336 5.44798847 1.94784144 ... 3.12127385 0.95253444 3.3267899 ]\n",
      " ...\n",
      " [3.88076589 2.18054348 2.25358042 ... 0.63054325 4.27522836 1.94685409]\n",
      " [3.56896272 0.         4.94295545 ... 4.09928905 2.55111774 2.38155485]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.98260330e-04 8.93637713e-04 6.77482182e-02 ... 3.18344433e-01\n",
      "  5.04662884e-03 9.25184190e-02]\n",
      " [2.69212447e-01 1.68896746e-04 2.59459415e-01 ... 3.64504469e-02\n",
      "  4.27543484e-02 4.95834920e-02]\n",
      " [4.69537466e-03 6.66601039e-04 6.10704921e-02 ... 3.26655244e-02\n",
      "  3.30246014e-01 6.49298012e-02]\n",
      " ...\n",
      " [3.47229285e-01 2.72056523e-01 2.33582231e-02 ... 2.49350117e-02\n",
      "  1.90576642e-03 5.19377484e-02]\n",
      " [4.30095462e-02 1.53959810e-03 2.06245603e-01 ... 1.43384508e-01\n",
      "  8.83173515e-02 6.34762812e-02]\n",
      " [2.30748658e-01 1.98457710e-01 3.41998727e-02 ... 2.50345327e-02\n",
      "  1.66943958e-02 1.81443143e-01]] & cached\n",
      "Re-used Cached Value, runNum =  173\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  173\n",
      "Provided input from cache for runNum = 173\n",
      "Provided input from cache for runNum = 174\n",
      "activation = [[4.94917514 5.36377948 5.84254433 ... 8.75663552 3.94638819 3.07499781]\n",
      " [0.         3.24662236 0.         ... 0.         0.         0.        ]\n",
      " [0.16382499 3.85039614 1.39710651 ... 1.76293698 4.00417471 1.63113245]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.10878646 0.40836061 1.28148406 ... 0.18358863 5.19879569 0.        ]\n",
      " [2.93268942 5.9199076  0.         ... 2.6421439  1.34704618 1.39770202]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.791375   0.         0.         ... 0.         0.57581587 0.        ]\n",
      " [1.43195002 5.46474228 1.95504222 ... 3.15570563 0.97557111 3.35098953]\n",
      " ...\n",
      " [3.90901306 2.21090906 2.27718003 ... 0.64219097 4.30670961 1.96130221]\n",
      " [3.57544082 0.         4.97042176 ... 4.13191257 2.56883949 2.38840879]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.68923849e-04 8.46116876e-04 6.63790889e-02 ... 3.19895619e-01\n",
      "  4.93038259e-03 9.17584723e-02]\n",
      " [2.67834278e-01 1.61588755e-04 2.60925209e-01 ... 3.54954599e-02\n",
      "  4.18980697e-02 4.87124272e-02]\n",
      " [4.57291497e-03 6.44706761e-04 6.06500751e-02 ... 3.21673847e-02\n",
      "  3.30700164e-01 6.42296930e-02]\n",
      " ...\n",
      " [3.49641270e-01 2.71837341e-01 2.32036548e-02 ... 2.43442307e-02\n",
      "  1.84977251e-03 5.18795358e-02]\n",
      " [4.25551383e-02 1.51247527e-03 2.07927056e-01 ... 1.43298414e-01\n",
      "  8.74337545e-02 6.32497191e-02]\n",
      " [2.31910347e-01 1.98434218e-01 3.40753701e-02 ... 2.46484238e-02\n",
      "  1.64301396e-02 1.82718695e-01]] & cached\n",
      "Re-used Cached Value, runNum =  174\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  174\n",
      "Provided input from cache for runNum = 174\n",
      "Provided input from cache for runNum = 175\n",
      "activation = [[4.95548613 5.36767102 5.85037134 ... 8.77877293 3.95857447 3.07872272]\n",
      " [0.         3.24804371 0.         ... 0.         0.         0.        ]\n",
      " [0.16513924 3.85422418 1.39462148 ... 1.7682834  4.01417639 1.63461536]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.11483075 0.42405036 1.29342957 ... 0.18755745 5.21213636 0.        ]\n",
      " [2.94975766 5.94388434 0.         ... 2.64948539 1.35828345 1.40958129]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.79501221 0.         0.         ... 0.         0.575216   0.        ]\n",
      " [1.43784626 5.48102118 1.96197983 ... 3.18983189 0.99824222 3.37509879]\n",
      " ...\n",
      " [3.93745043 2.24156537 2.30094465 ... 0.65413754 4.33832428 1.97585046]\n",
      " [3.58219316 0.         4.99784067 ... 4.1648688  2.58681645 2.39540861]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.41169676e-04 8.01225029e-04 6.50094716e-02 ... 3.21390514e-01\n",
      "  4.81470581e-03 9.09957943e-02]\n",
      " [2.66573123e-01 1.54717420e-04 2.62430003e-01 ... 3.45703266e-02\n",
      "  4.10622787e-02 4.78597532e-02]\n",
      " [4.45592207e-03 6.23994843e-04 6.02366392e-02 ... 3.16823772e-02\n",
      "  3.31172072e-01 6.35397320e-02]\n",
      " ...\n",
      " [3.51950612e-01 2.71627665e-01 2.30489612e-02 ... 2.37606897e-02\n",
      "  1.79448860e-03 5.18113527e-02]\n",
      " [4.21135805e-02 1.48693892e-03 2.09617015e-01 ... 1.43244945e-01\n",
      "  8.65598904e-02 6.30255776e-02]\n",
      " [2.33016454e-01 1.98451557e-01 3.39489539e-02 ... 2.42628231e-02\n",
      "  1.61631335e-02 1.83985653e-01]] & cached\n",
      "Re-used Cached Value, runNum =  175\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  175\n",
      "Provided input from cache for runNum = 175\n",
      "Provided input from cache for runNum = 176\n",
      "activation = [[4.9617877  5.37156476 5.85809242 ... 8.80086061 3.97068627 3.08244903]\n",
      " [0.         3.24940331 0.         ... 0.         0.         0.        ]\n",
      " [0.16643426 3.85800085 1.39215335 ... 1.77363622 4.02421979 1.6380688 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.12101271 0.43987835 1.30543525 ... 0.19165553 5.22553126 0.        ]\n",
      " [2.96661972 5.96757705 0.         ... 2.65666438 1.36937888 1.42133638]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.79878341 0.         0.         ... 0.         0.57467662 0.        ]\n",
      " [1.44355648 5.4969486  1.96871199 ... 3.22376985 1.02065999 3.39913658]\n",
      " ...\n",
      " [3.9659585  2.27230134 2.32480681 ... 0.66625161 4.36991954 1.9904653 ]\n",
      " [3.58904053 0.         5.02507861 ... 4.19789038 2.60479342 2.4024505 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.14997364e-04 7.58863142e-04 6.36487478e-02 ... 3.22848063e-01\n",
      "  4.70092933e-03 9.02320673e-02]\n",
      " [2.65375581e-01 1.48218522e-04 2.63951097e-01 ... 3.36673938e-02\n",
      "  4.02440038e-02 4.70213890e-02]\n",
      " [4.34391260e-03 6.04302426e-04 5.98320546e-02 ... 3.12070666e-02\n",
      "  3.31637728e-01 6.28564339e-02]\n",
      " ...\n",
      " [3.54187297e-01 2.71425112e-01 2.28936840e-02 ... 2.31873787e-02\n",
      "  1.74056429e-03 5.17348756e-02]\n",
      " [4.16821028e-02 1.46262844e-03 2.11302577e-01 ... 1.43200808e-01\n",
      "  8.57002213e-02 6.28013766e-02]\n",
      " [2.34086319e-01 1.98490523e-01 3.38227798e-02 ... 2.38815563e-02\n",
      "  1.58984028e-02 1.85250706e-01]] & cached\n",
      "Re-used Cached Value, runNum =  176\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  176\n",
      "Provided input from cache for runNum = 176\n",
      "Provided input from cache for runNum = 177\n",
      "activation = [[4.96804934 5.37541181 5.86571181 ... 8.82285261 3.98275154 3.08615598]\n",
      " [0.         3.25079571 0.         ... 0.         0.         0.        ]\n",
      " [0.16776875 3.86181049 1.38972807 ... 1.77908218 4.03434345 1.64153297]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.12733668 0.45586342 1.31748534 ... 0.19595331 5.23902808 0.        ]\n",
      " [2.98338948 5.99111744 0.         ... 2.66380833 1.38037882 1.433037  ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.80263868 0.         0.         ... 0.         0.57417985 0.        ]\n",
      " [1.44909047 5.51263107 1.97526699 ... 3.26228055 1.04278355 3.42308431]\n",
      " ...\n",
      " [3.99425283 2.30280021 2.34847399 ... 0.68363692 4.40115991 2.00493695]\n",
      " [3.59568973 0.         5.05204416 ... 4.23465633 2.62261433 2.40939659]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.90385395e-04 7.18956971e-04 6.23137652e-02 ... 3.23796326e-01\n",
      "  4.59064283e-03 8.94764804e-02]\n",
      " [2.64148539e-01 1.41999013e-04 2.65436866e-01 ... 3.28236124e-02\n",
      "  3.94475082e-02 4.61897649e-02]\n",
      " [4.23648854e-03 5.85537971e-04 5.94381790e-02 ... 3.07968100e-02\n",
      "  3.32092017e-01 6.21765903e-02]\n",
      " ...\n",
      " [3.56427900e-01 2.71206269e-01 2.27420953e-02 ... 2.25829712e-02\n",
      "  1.68858472e-03 5.16586083e-02]\n",
      " [4.12604030e-02 1.43925035e-03 2.13001097e-01 ... 1.42899132e-01\n",
      "  8.48648078e-02 6.25781713e-02]\n",
      " [2.35125974e-01 1.98493487e-01 3.36967279e-02 ... 2.35438859e-02\n",
      "  1.56394279e-02 1.86515945e-01]] & cached\n",
      "Re-used Cached Value, runNum =  177\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  177\n",
      "Provided input from cache for runNum = 177\n",
      "Provided input from cache for runNum = 178\n",
      "activation = [[4.97425075 5.37917364 5.87321077 ... 8.84468674 3.99468291 3.08983323]\n",
      " [0.         3.25221741 0.         ... 0.         0.         0.        ]\n",
      " [0.16910225 3.86557559 1.38731946 ... 1.78454966 4.04451644 1.64496365]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.13365789 0.47182535 1.32952487 ... 0.20027705 5.25247866 0.        ]\n",
      " [3.00011394 6.01463743 0.         ... 2.67105097 1.39142878 1.44473014]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.80659255 0.         0.         ... 0.         0.57370514 0.        ]\n",
      " [1.45466929 5.52824951 1.98178112 ... 3.30266694 1.06490685 3.44705252]\n",
      " ...\n",
      " [4.02258194 2.3333685  2.37224744 ... 0.70331012 4.43237831 2.01952326]\n",
      " [3.6020634  0.         5.07864393 ... 4.27245696 2.64004323 2.41616814]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.67041865e-04 6.80935082e-04 6.09843453e-02 ... 3.24461973e-01\n",
      "  4.48240494e-03 8.87050727e-02]\n",
      " [2.62869316e-01 1.36039425e-04 2.66923817e-01 ... 3.20118799e-02\n",
      "  3.86616337e-02 4.53664374e-02]\n",
      " [4.13126505e-03 5.67333135e-04 5.90439404e-02 ... 3.04071898e-02\n",
      "  3.32493159e-01 6.14958820e-02]\n",
      " ...\n",
      " [3.58696888e-01 2.71015071e-01 2.25981402e-02 ... 2.19830203e-02\n",
      "  1.63838797e-03 5.15848588e-02]\n",
      " [4.08325144e-02 1.41632527e-03 2.14688372e-01 ... 1.42488271e-01\n",
      "  8.40356795e-02 6.23483186e-02]\n",
      " [2.36171496e-01 1.98493110e-01 3.35809790e-02 ... 2.32356828e-02\n",
      "  1.53885818e-02 1.87815475e-01]] & cached\n",
      "Re-used Cached Value, runNum =  178\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  178\n",
      "Provided input from cache for runNum = 178\n",
      "Provided input from cache for runNum = 179\n",
      "activation = [[4.98046879 5.38296457 5.88063033 ... 8.86651617 4.00658913 3.09353331]\n",
      " [0.         3.25368925 0.         ... 0.         0.         0.        ]\n",
      " [0.17043486 3.86928972 1.38493406 ... 1.7900213  4.05471124 1.64837599]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.13994126 0.48764651 1.34150101 ... 0.20449287 5.26582511 0.        ]\n",
      " [3.01659492 6.03785592 0.         ... 2.67808731 1.40230951 1.45627842]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.8105112  0.         0.         ... 0.         0.57312588 0.        ]\n",
      " [1.46013525 5.54363116 1.98813435 ... 3.3430125  1.08684879 3.47094535]\n",
      " ...\n",
      " [4.0509221  2.36387617 2.3961043  ... 0.72318986 4.46354771 2.0342278 ]\n",
      " [3.60859083 0.         5.10512197 ... 4.31045789 2.65759976 2.4230803 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.45048525e-04 6.44937963e-04 5.96621588e-02 ... 3.25074825e-01\n",
      "  4.37681970e-03 8.79302597e-02]\n",
      " [2.61634749e-01 1.30371244e-04 2.68435778e-01 ... 3.12202980e-02\n",
      "  3.78929767e-02 4.45602909e-02]\n",
      " [4.02976940e-03 5.49824161e-04 5.86561036e-02 ... 3.00224896e-02\n",
      "  3.32898371e-01 6.08254801e-02]\n",
      " ...\n",
      " [3.60927921e-01 2.70852318e-01 2.24559714e-02 ... 2.13949763e-02\n",
      "  1.58951552e-03 5.15023845e-02]\n",
      " [4.04120634e-02 1.39417161e-03 2.16367237e-01 ... 1.42077190e-01\n",
      "  8.32283554e-02 6.21207077e-02]\n",
      " [2.37172605e-01 1.98497930e-01 3.34663141e-02 ... 2.29301630e-02\n",
      "  1.51395996e-02 1.89107455e-01]] & cached\n",
      "Re-used Cached Value, runNum =  179\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  179\n",
      "Provided input from cache for runNum = 179\n",
      "Provided input from cache for runNum = 180\n",
      "activation = [[4.98665726 5.38672533 5.88794107 ... 8.88825928 4.018451   3.09722158]\n",
      " [0.         3.25510707 0.         ... 0.         0.         0.        ]\n",
      " [0.17175725 3.87295047 1.38256938 ... 1.79550524 4.06492832 1.65176052]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.14620669 0.50343636 1.3534486  ... 0.20869192 5.27909606 0.        ]\n",
      " [3.03287913 6.06080364 0.         ... 2.6849946  1.41301844 1.46769746]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.81454208 0.         0.         ... 0.         0.57259935 0.        ]\n",
      " [1.46560777 5.55887403 1.99446236 ... 3.38344858 1.10882996 3.4948258 ]\n",
      " ...\n",
      " [4.07922764 2.39435762 2.41998479 ... 0.74321622 4.49463134 2.04899772]\n",
      " [3.61510184 0.         5.13136287 ... 4.34842286 2.67516861 2.43001132]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.24316218e-04 6.10921298e-04 5.83550972e-02 ... 3.25637782e-01\n",
      "  4.27390842e-03 8.71528560e-02]\n",
      " [2.60423860e-01 1.24992564e-04 2.69951374e-01 ... 3.04444104e-02\n",
      "  3.71382236e-02 4.37676726e-02]\n",
      " [3.93148867e-03 5.33005443e-04 5.82721639e-02 ... 2.96392595e-02\n",
      "  3.33285535e-01 6.01582546e-02]\n",
      " ...\n",
      " [3.63122552e-01 2.70712797e-01 2.23159535e-02 ... 2.08215588e-02\n",
      "  1.54210186e-03 5.14139251e-02]\n",
      " [3.99924625e-02 1.37272292e-03 2.18023069e-01 ... 1.41646798e-01\n",
      "  8.24342101e-02 6.18903341e-02]\n",
      " [2.38156940e-01 1.98530464e-01 3.33559997e-02 ... 2.26315417e-02\n",
      "  1.48951837e-02 1.90405280e-01]] & cached\n",
      "Re-used Cached Value, runNum =  180\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  180\n",
      "Provided input from cache for runNum = 180\n",
      "Provided input from cache for runNum = 181\n",
      "activation = [[4.99279996 5.39047837 5.89513098 ... 8.90991063 4.03022594 3.10089719]\n",
      " [0.         3.25645966 0.         ... 0.         0.         0.        ]\n",
      " [0.17307941 3.87655906 1.38020635 ... 1.80101019 4.07519277 1.65511728]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.15254967 0.51933909 1.36546026 ... 0.21300966 5.29236572 0.        ]\n",
      " [3.04899954 6.08350601 0.         ... 2.69180291 1.42362142 1.47901058]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.81894288 0.         0.         ... 0.         0.57231557 0.        ]\n",
      " [1.47095791 5.57387197 2.00064824 ... 3.42376641 1.1306792  3.51864172]\n",
      " ...\n",
      " [4.10742438 2.42478128 2.44385206 ... 0.76325005 4.52549545 2.06373934]\n",
      " [3.62150235 0.         5.15732053 ... 4.3861929  2.69251854 2.43687763]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.04712964e-04 5.78720031e-04 5.70607575e-02 ... 3.26164891e-01\n",
      "  4.17313732e-03 8.63761248e-02]\n",
      " [2.59222676e-01 1.19867356e-04 2.71465127e-01 ... 2.96817805e-02\n",
      "  3.64002643e-02 4.29856301e-02]\n",
      " [3.83625780e-03 5.16830325e-04 5.78918687e-02 ... 2.92577014e-02\n",
      "  3.33644674e-01 5.94953451e-02]\n",
      " ...\n",
      " [3.65310537e-01 2.70602712e-01 2.21813194e-02 ... 2.02637013e-02\n",
      "  1.49623346e-03 5.13241895e-02]\n",
      " [3.95752342e-02 1.35208909e-03 2.19684608e-01 ... 1.41222128e-01\n",
      "  8.16492711e-02 6.16604089e-02]\n",
      " [2.39110605e-01 1.98555339e-01 3.32496073e-02 ... 2.23375129e-02\n",
      "  1.46564376e-02 1.91708264e-01]] & cached\n",
      "Re-used Cached Value, runNum =  181\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  181\n",
      "Provided input from cache for runNum = 181\n",
      "Provided input from cache for runNum = 182\n",
      "activation = [[4.99885679 5.39409602 5.90218549 ... 8.93137514 4.04184813 3.10450928]\n",
      " [0.         3.25778433 0.         ... 0.         0.         0.        ]\n",
      " [0.17442389 3.88017333 1.3778705  ... 1.80656198 4.08551404 1.65847668]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.15897322 0.53531061 1.37751808 ... 0.2173939  5.30565959 0.        ]\n",
      " [3.06504808 6.10614032 0.         ... 2.69866275 1.43423864 1.4903058 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.82342732 0.         0.         ... 0.         0.57206635 0.        ]\n",
      " [1.47610993 5.58858021 2.00661262 ... 3.46393261 1.1522933  3.5423714 ]\n",
      " ...\n",
      " [4.13565308 2.45529645 2.46783756 ... 0.78352119 4.55630631 2.07856055]\n",
      " [3.62765551 0.         5.1829396  ... 4.42358463 2.7095138  2.44357402]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.86191465e-04 5.48112627e-04 5.57735633e-02 ... 3.26620111e-01\n",
      "  4.07388199e-03 8.55875840e-02]\n",
      " [2.58006235e-01 1.14966959e-04 2.72982401e-01 ... 2.89355213e-02\n",
      "  3.56754254e-02 4.22133124e-02]\n",
      " [3.74446943e-03 5.01334864e-04 5.75247343e-02 ... 2.88847011e-02\n",
      "  3.33967462e-01 5.88380372e-02]\n",
      " ...\n",
      " [3.67494381e-01 2.70475904e-01 2.20502538e-02 ... 1.97220102e-02\n",
      "  1.45180216e-03 5.12368912e-02]\n",
      " [3.91636656e-02 1.33211133e-03 2.21355050e-01 ... 1.40809132e-01\n",
      "  8.08737098e-02 6.14302982e-02]\n",
      " [2.40044144e-01 1.98552158e-01 3.31473862e-02 ... 2.20529560e-02\n",
      "  1.44230026e-02 1.93034523e-01]] & cached\n",
      "Re-used Cached Value, runNum =  182\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  182\n",
      "Provided input from cache for runNum = 182\n",
      "Provided input from cache for runNum = 183\n",
      "activation = [[5.00490636 5.39771719 5.90914892 ... 8.95278635 4.05340719 3.108127  ]\n",
      " [0.         3.25911875 0.         ... 0.         0.         0.        ]\n",
      " [0.17571393 3.88365576 1.37552538 ... 1.81202031 4.09578956 1.66177482]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.1655703  0.55151444 1.38967346 ... 0.2221361  5.31910188 0.        ]\n",
      " [3.0809676  6.12858057 0.         ... 2.70543358 1.44477132 1.50151134]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.82818716 0.         0.         ... 0.         0.5720454  0.        ]\n",
      " [1.48108616 5.60298386 2.01236825 ... 3.50382779 1.17364904 3.56602226]\n",
      " ...\n",
      " [4.16414274 2.48627415 2.49215943 ... 0.80453646 4.58737548 2.09359037]\n",
      " [3.63378939 0.         5.20834653 ... 4.46101146 2.72644376 2.4502663 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.68718036e-04 5.19107247e-04 5.44899752e-02 ... 3.26946443e-01\n",
      "  3.97443603e-03 8.47898914e-02]\n",
      " [2.56852691e-01 1.10321602e-04 2.74511822e-01 ... 2.82129192e-02\n",
      "  3.49625587e-02 4.14546366e-02]\n",
      " [3.65709069e-03 4.86678258e-04 5.71728246e-02 ... 2.85309483e-02\n",
      "  3.34270805e-01 5.81929068e-02]\n",
      " ...\n",
      " [3.69595989e-01 2.70329054e-01 2.19169775e-02 ... 1.91913109e-02\n",
      "  1.40808227e-03 5.11416286e-02]\n",
      " [3.87655686e-02 1.31328608e-03 2.23043088e-01 ... 1.40454450e-01\n",
      "  8.00949122e-02 6.12030957e-02]\n",
      " [2.40951812e-01 1.98565007e-01 3.30443805e-02 ... 2.17737853e-02\n",
      "  1.41898911e-02 1.94363228e-01]] & cached\n",
      "Re-used Cached Value, runNum =  183\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  183\n",
      "Provided input from cache for runNum = 183\n",
      "Provided input from cache for runNum = 184\n",
      "activation = [[5.0109553  5.40135324 5.91603185 ... 8.97415663 4.06492588 3.11174909]\n",
      " [0.         3.26040494 0.         ... 0.         0.         0.        ]\n",
      " [0.17699782 3.88706175 1.37319404 ... 1.81747648 4.10608085 1.66503834]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.17224299 0.56784054 1.40183285 ... 0.22701945 5.33261225 0.        ]\n",
      " [3.09668426 6.15073054 0.         ... 2.71203803 1.45513596 1.51258854]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.83300583 0.         0.         ... 0.         0.57211892 0.        ]\n",
      " [1.4858417  5.61701283 2.01792449 ... 3.5435022  1.19475141 3.58956346]\n",
      " ...\n",
      " [4.19245037 2.5171461  2.51636331 ... 0.82551098 4.61823823 2.10855837]\n",
      " [3.64001391 0.         5.23358264 ... 4.49850572 2.74341182 2.45700016]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.52307581e-04 4.91660780e-04 5.32268377e-02 ... 3.27265607e-01\n",
      "  3.87762912e-03 8.40004664e-02]\n",
      " [2.55722409e-01 1.05899803e-04 2.76041478e-01 ... 2.75046382e-02\n",
      "  3.42690602e-02 4.07084785e-02]\n",
      " [3.57281586e-03 4.72593103e-04 5.68260669e-02 ... 2.81821611e-02\n",
      "  3.34576687e-01 5.75543309e-02]\n",
      " ...\n",
      " [3.71696552e-01 2.70211489e-01 2.17873805e-02 ... 1.86716418e-02\n",
      "  1.36568494e-03 5.10432060e-02]\n",
      " [3.83761856e-02 1.29515871e-03 2.24728515e-01 ... 1.40106764e-01\n",
      "  7.93351293e-02 6.09781441e-02]\n",
      " [2.41797820e-01 1.98562315e-01 3.29407471e-02 ... 2.14935121e-02\n",
      "  1.39596503e-02 1.95681016e-01]] & cached\n",
      "Re-used Cached Value, runNum =  184\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  184\n",
      "Provided input from cache for runNum = 184\n",
      "Provided input from cache for runNum = 185\n",
      "activation = [[5.0169355  5.40490108 5.92279511 ... 8.99536957 4.07631202 3.11533235]\n",
      " [0.         3.26170369 0.         ... 0.         0.         0.        ]\n",
      " [0.17823958 3.89040259 1.37084995 ... 1.82291094 4.11635582 1.66827613]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.178906   0.58412069 1.4139487  ... 0.23186532 5.34604559 0.        ]\n",
      " [3.11233043 6.17279752 0.         ... 2.71869087 1.46551827 1.52364416]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.83789724 0.         0.         ... 0.         0.57223438 0.        ]\n",
      " [1.49050871 5.63089985 2.02335201 ... 3.58305213 1.21566574 3.61301505]\n",
      " ...\n",
      " [4.22071104 2.5480848  2.54062552 ... 0.84660107 4.64897486 2.12358229]\n",
      " [3.64592807 0.         5.25842906 ... 4.53551866 2.75996899 2.46351742]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.36832899e-04 4.65647027e-04 5.19778012e-02 ... 3.27521874e-01\n",
      "  3.78284101e-03 8.32040841e-02]\n",
      " [2.54553896e-01 1.01671771e-04 2.77565730e-01 ... 2.68094393e-02\n",
      "  3.35886945e-02 3.99711029e-02]\n",
      " [3.49073591e-03 4.59048880e-04 5.64827593e-02 ... 2.78348055e-02\n",
      "  3.34838159e-01 5.69190145e-02]\n",
      " ...\n",
      " [3.73815063e-01 2.70099379e-01 2.16625727e-02 ... 1.81694848e-02\n",
      "  1.32481147e-03 5.09455759e-02]\n",
      " [3.79873165e-02 1.27758101e-03 2.26407676e-01 ... 1.39760585e-01\n",
      "  7.85851828e-02 6.07507236e-02]\n",
      " [2.42637675e-01 1.98555721e-01 3.28434218e-02 ... 2.12230714e-02\n",
      "  1.37362081e-02 1.97021498e-01]] & cached\n",
      "Re-used Cached Value, runNum =  185\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  185\n",
      "Provided input from cache for runNum = 185\n",
      "Provided input from cache for runNum = 186\n",
      "activation = [[5.0228739  5.40838587 5.92945309 ... 9.01643686 4.08760188 3.11888967]\n",
      " [0.         3.26294378 0.         ... 0.         0.         0.        ]\n",
      " [0.17946676 3.89366589 1.36851599 ... 1.828304   4.12661338 1.67147963]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.18553629 0.60028487 1.42601981 ... 0.23659745 5.35935786 0.        ]\n",
      " [3.12795015 6.19482605 0.         ... 2.72543898 1.4759394  1.5346831 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.84280148 0.         0.         ... 0.         0.57233151 0.        ]\n",
      " [1.4951665  5.64464952 2.02870986 ... 3.62265944 1.23655365 3.63643312]\n",
      " ...\n",
      " [4.24884271 2.578908   2.56483235 ... 0.86763673 4.67950299 2.13861509]\n",
      " [3.65164863 0.         5.28298282 ... 4.57215281 2.77625273 2.46989957]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.22199607e-04 4.40822028e-04 5.07407996e-02 ... 3.27717498e-01\n",
      "  3.69097793e-03 8.23994412e-02]\n",
      " [2.53335099e-01 9.76244409e-05 2.79116185e-01 ... 2.61298514e-02\n",
      "  3.29215366e-02 3.92447551e-02]\n",
      " [3.40938990e-03 4.45714322e-04 5.61297582e-02 ... 2.74777830e-02\n",
      "  3.35066901e-01 5.62809830e-02]\n",
      " ...\n",
      " [3.75994921e-01 2.70060745e-01 2.15474219e-02 ... 1.76868929e-02\n",
      "  1.28551489e-03 5.08536818e-02]\n",
      " [3.75893439e-02 1.26005162e-03 2.28053517e-01 ... 1.39382412e-01\n",
      "  7.78507432e-02 6.05164456e-02]\n",
      " [2.43460943e-01 1.98547794e-01 3.27555119e-02 ... 2.09642060e-02\n",
      "  1.35197829e-02 1.98392231e-01]] & cached\n",
      "Re-used Cached Value, runNum =  186\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  186\n",
      "Provided input from cache for runNum = 186\n",
      "Provided input from cache for runNum = 187\n",
      "activation = [[5.02879654 5.41185211 5.93602505 ... 9.03745541 4.09883517 3.12244122]\n",
      " [0.         3.26418218 0.         ... 0.         0.         0.        ]\n",
      " [0.18066239 3.89684782 1.36618392 ... 1.83366647 4.13685579 1.67464947]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.19224654 0.61656516 1.43812474 ... 0.24149789 5.37272156 0.        ]\n",
      " [3.14341604 6.21666641 0.         ... 2.73212786 1.4862492  1.54565057]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.8478027  0.         0.         ... 0.         0.57250802 0.        ]\n",
      " [1.49970203 5.65809916 2.03388648 ... 3.66207457 1.2572215  3.65976398]\n",
      " ...\n",
      " [4.27707837 2.6098857  2.58918262 ... 0.88904879 4.71009121 2.15375618]\n",
      " [3.65744235 0.         5.30737509 ... 4.60881246 2.79252801 2.4762818 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.08456486e-04 4.17341528e-04 4.95176046e-02 ... 3.27840443e-01\n",
      "  3.60038515e-03 8.15927595e-02]\n",
      " [2.52151973e-01 9.37673342e-05 2.80664267e-01 ... 2.54680009e-02\n",
      "  3.22671204e-02 3.85305696e-02]\n",
      " [3.33178642e-03 4.33029113e-04 5.57897798e-02 ... 2.71321156e-02\n",
      "  3.35290328e-01 5.56540286e-02]\n",
      " ...\n",
      " [3.78123994e-01 2.70005910e-01 2.14318895e-02 ... 1.72147523e-02\n",
      "  1.24710608e-03 5.07562867e-02]\n",
      " [3.72046708e-02 1.24337502e-03 2.29714074e-01 ... 1.39036566e-01\n",
      "  7.71246219e-02 6.02850938e-02]\n",
      " [2.44250242e-01 1.98529191e-01 3.26659223e-02 ... 2.07082257e-02\n",
      "  1.33046222e-02 1.99762160e-01]] & cached\n",
      "Re-used Cached Value, runNum =  187\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  187\n",
      "Provided input from cache for runNum = 187\n",
      "Provided input from cache for runNum = 188\n",
      "activation = [[5.03472216 5.41532935 5.94251318 ... 9.05841161 4.11001243 3.12598783]\n",
      " [0.         3.26538987 0.         ... 0.         0.         0.        ]\n",
      " [0.18181199 3.89991225 1.36383706 ... 1.83898942 4.14707769 1.67777196]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.19893036 0.63283293 1.4501709  ... 0.24635111 5.38597842 0.        ]\n",
      " [3.15870732 6.23830367 0.         ... 2.73875862 1.49646772 1.55653411]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.85277763 0.         0.         ... 0.         0.57260826 0.        ]\n",
      " [1.504171   5.6713559  2.03897371 ... 3.701421   1.2777991  3.68302084]\n",
      " ...\n",
      " [4.30506566 2.64063891 2.61333586 ... 0.9102515  4.74033894 2.1688271 ]\n",
      " [3.66324675 0.         5.33154019 ... 4.64535642 2.80866198 2.48262218]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.95597462e-04 3.95213365e-04 4.83202322e-02 ... 3.27954952e-01\n",
      "  3.51328607e-03 8.07918521e-02]\n",
      " [2.50973184e-01 9.00929404e-05 2.82206560e-01 ... 2.48190847e-02\n",
      "  3.16281886e-02 3.78282987e-02]\n",
      " [3.25661892e-03 4.20796553e-04 5.54486701e-02 ... 2.67817768e-02\n",
      "  3.35501638e-01 5.50293579e-02]\n",
      " ...\n",
      " [3.80253066e-01 2.69993320e-01 2.13199530e-02 ... 1.67565416e-02\n",
      "  1.21022359e-03 5.06572808e-02]\n",
      " [3.68276975e-02 1.22730214e-03 2.31357651e-01 ... 1.38671377e-01\n",
      "  7.64217465e-02 6.00513886e-02]\n",
      " [2.45000163e-01 1.98515209e-01 3.25776544e-02 ... 2.04555578e-02\n",
      "  1.30954336e-02 2.01135708e-01]] & cached\n",
      "Re-used Cached Value, runNum =  188\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  188\n",
      "Provided input from cache for runNum = 188\n",
      "Provided input from cache for runNum = 189\n",
      "activation = [[5.04057888 5.41872792 5.94887254 ... 9.07921887 4.1210573  3.12949747]\n",
      " [0.         3.26659565 0.         ... 0.         0.         0.        ]\n",
      " [0.18294761 3.90290697 1.36149752 ... 1.8442817  4.1572859  1.68086634]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.20563872 0.64913821 1.46220314 ... 0.25124104 5.39924979 0.        ]\n",
      " [3.17388315 6.25979237 0.         ... 2.74536113 1.50663543 1.56735924]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.857922   0.         0.         ... 0.         0.57284936 0.        ]\n",
      " [1.50859189 5.68439639 2.04395205 ... 3.74071371 1.29824804 3.70622933]\n",
      " ...\n",
      " [4.33311981 2.6715162  2.63759527 ... 0.9317682  4.77065514 2.18402807]\n",
      " [3.66904465 0.         5.35549046 ... 4.68182885 2.82468058 2.48893176]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.83489452e-04 3.74172487e-04 4.71371900e-02 ... 3.28005910e-01\n",
      "  3.42723799e-03 7.99836461e-02]\n",
      " [2.49790253e-01 8.65664285e-05 2.83735929e-01 ... 2.41854633e-02\n",
      "  3.09998391e-02 3.71362056e-02]\n",
      " [3.18453801e-03 4.09043555e-04 5.51215292e-02 ... 2.64408614e-02\n",
      "  3.35690573e-01 5.44138426e-02]\n",
      " ...\n",
      " [3.82365400e-01 2.69962113e-01 2.12115864e-02 ... 1.63092969e-02\n",
      "  1.17423205e-03 5.05575542e-02]\n",
      " [3.64567180e-02 1.21166759e-03 2.33005000e-01 ... 1.38317033e-01\n",
      "  7.57207878e-02 5.98182980e-02]\n",
      " [2.45728354e-01 1.98471771e-01 3.24924126e-02 ... 2.02080096e-02\n",
      "  1.28887679e-02 2.02521044e-01]] & cached\n",
      "Re-used Cached Value, runNum =  189\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  189\n",
      "Provided input from cache for runNum = 189\n",
      "Provided input from cache for runNum = 190\n",
      "activation = [[5.0464053  5.42210372 5.95513501 ... 9.09995056 4.13202789 3.13299983]\n",
      " [0.         3.26784067 0.         ... 0.         0.         0.        ]\n",
      " [0.18406024 3.90584671 1.3591567  ... 1.84956066 4.16749784 1.68393545]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.21239374 0.66552724 1.47426334 ... 0.25625824 5.41260647 0.        ]\n",
      " [3.18894969 6.28109249 0.         ... 2.75188676 1.51670642 1.57809993]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.63297397e-01 0.00000000e+00 2.10715548e-03 ... 0.00000000e+00\n",
      "  5.73295928e-01 0.00000000e+00]\n",
      " [1.51298628e+00 5.69721413e+00 2.04882087e+00 ... 3.77989712e+00\n",
      "  1.31853324e+00 3.72938685e+00]\n",
      " ...\n",
      " [4.36117388e+00 2.70239566e+00 2.66190544e+00 ... 9.53490189e-01\n",
      "  4.80097674e+00 2.19929018e+00]\n",
      " [3.67490083e+00 0.00000000e+00 5.37930088e+00 ... 4.71836588e+00\n",
      "  2.84071533e+00 2.49529132e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[1.72106212e-04 3.54280207e-04 4.59202282e-02 ... 3.28003034e-01\n",
      "  3.34246393e-03 7.91754244e-02]\n",
      " [2.48644488e-01 8.32031520e-05 2.85442937e-01 ... 2.35703248e-02\n",
      "  3.03848363e-02 3.64576859e-02]\n",
      " [3.11525708e-03 3.97803947e-04 5.48160783e-02 ... 2.61073095e-02\n",
      "  3.35870705e-01 5.38041135e-02]\n",
      " ...\n",
      " [3.84428394e-01 2.69917661e-01 2.10923711e-02 ... 1.58725462e-02\n",
      "  1.13907771e-03 5.04535220e-02]\n",
      " [3.60887902e-02 1.19653055e-03 2.34481837e-01 ... 1.37962297e-01\n",
      "  7.50243656e-02 5.95842444e-02]\n",
      " [2.46436625e-01 1.98424242e-01 3.24092550e-02 ... 1.99644796e-02\n",
      "  1.26837569e-02 2.03908484e-01]] & cached\n",
      "Re-used Cached Value, runNum =  190\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  190\n",
      "Provided input from cache for runNum = 190\n",
      "Provided input from cache for runNum = 191\n",
      "activation = [[5.05227247 5.42555172 5.96133359 ... 9.12070099 4.14304226 3.13652126]\n",
      " [0.         3.2690453  0.         ... 0.         0.         0.        ]\n",
      " [0.18514433 3.90867011 1.3568194  ... 1.85475541 4.17765163 1.6869536 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.21918366 0.68195954 1.48631403 ... 0.26134813 5.42602595 0.        ]\n",
      " [3.20386316 6.30219916 0.         ... 2.75834289 1.52664664 1.58876247]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.86877246 0.         0.01285062 ... 0.         0.57385697 0.        ]\n",
      " [1.51736619 5.71287775 2.05361297 ... 3.819011   1.33870853 3.7524961 ]\n",
      " ...\n",
      " [4.3892555  2.73683655 2.68632182 ... 0.97552981 4.83136845 2.21465857]\n",
      " [3.68092475 0.         5.40300619 ... 4.75505837 2.8569403  2.50174104]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.61448473e-04 3.34630277e-04 4.45239070e-02 ... 3.27939836e-01\n",
      "  3.25916250e-03 7.83697050e-02]\n",
      " [2.47531558e-01 7.99539437e-05 2.87807319e-01 ... 2.29699655e-02\n",
      "  2.97820069e-02 3.57912591e-02]\n",
      " [3.04907138e-03 3.86845247e-04 5.45787083e-02 ... 2.57811020e-02\n",
      "  3.36052183e-01 5.32046160e-02]\n",
      " ...\n",
      " [3.86447878e-01 2.69555466e-01 2.09256504e-02 ... 1.54450788e-02\n",
      "  1.10466976e-03 5.03420450e-02]\n",
      " [3.57343077e-02 1.17905753e-03 2.35345814e-01 ... 1.37630123e-01\n",
      "  7.43369472e-02 5.93533514e-02]\n",
      " [2.47111090e-01 1.98697982e-01 3.23295913e-02 ... 1.97223129e-02\n",
      "  1.24792717e-02 2.05286640e-01]] & cached\n",
      "Re-used Cached Value, runNum =  191\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  191\n",
      "Provided input from cache for runNum = 191\n",
      "Provided input from cache for runNum = 192\n",
      "activation = [[5.0581514  5.42898765 5.96745747 ... 9.14140257 4.15398986 3.14004939]\n",
      " [0.         3.2702444  0.         ... 0.         0.         0.        ]\n",
      " [0.18621358 3.91146597 1.35448118 ... 1.85994906 4.18780436 1.68995356]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.22602722 0.6984874  1.49838174 ... 0.26651169 5.43949932 0.        ]\n",
      " [3.2186599  6.32313078 0.         ... 2.76473237 1.53651167 1.59934895]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.87411403 0.         0.02351435 ... 0.         0.57440569 0.        ]\n",
      " [1.52161493 5.73146755 2.05823234 ... 3.85790301 1.35862439 3.77550029]\n",
      " ...\n",
      " [4.41717847 2.77481617 2.71064514 ... 0.99751676 4.86157445 2.22998526]\n",
      " [3.68693673 0.         5.42649478 ... 4.79166408 2.8730312  2.50815505]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.51491178e-04 3.15306540e-04 4.31667858e-02 ... 3.27867047e-01\n",
      "  3.17810811e-03 7.75713607e-02]\n",
      " [2.46406241e-01 7.68023540e-05 2.90150958e-01 ... 2.23811813e-02\n",
      "  2.91938287e-02 3.51348492e-02]\n",
      " [2.98548411e-03 3.76178719e-04 5.43478243e-02 ... 2.54589798e-02\n",
      "  3.36219719e-01 5.26123185e-02]\n",
      " ...\n",
      " [3.88477136e-01 2.68841873e-01 2.07611035e-02 ... 1.50284038e-02\n",
      "  1.07141763e-03 5.02286523e-02]\n",
      " [3.53935839e-02 1.15921750e-03 2.36214082e-01 ... 1.37311800e-01\n",
      "  7.36661368e-02 5.91247832e-02]\n",
      " [2.47742792e-01 1.99277458e-01 3.22472968e-02 ... 1.94807252e-02\n",
      "  1.22786338e-02 2.06661046e-01]] & cached\n",
      "Re-used Cached Value, runNum =  192\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  192\n",
      "Provided input from cache for runNum = 192\n",
      "Provided input from cache for runNum = 193\n",
      "activation = [[5.06394814 5.43227754 5.97345884 ... 9.16192016 4.16477905 3.14351259]\n",
      " [0.         3.27153004 0.         ... 0.         0.         0.        ]\n",
      " [0.18730954 3.91427135 1.35216974 ... 1.86518309 4.19798691 1.69296014]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.23294704 0.71517301 1.51048824 ... 0.27187324 5.45304639 0.        ]\n",
      " [3.23337426 6.34399235 0.         ... 2.77114593 1.54635749 1.60990852]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.87962507 0.         0.03432972 ... 0.         0.57513102 0.        ]\n",
      " [1.52587616 5.7499261  2.06278403 ... 3.89669009 1.37846252 3.79848655]\n",
      " ...\n",
      " [4.44506708 2.81283995 2.73498535 ... 1.01966231 4.89173167 2.24533897]\n",
      " [3.69279586 0.         5.44968556 ... 4.82793227 2.88884141 2.51440469]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.42091878e-04 2.96951662e-04 4.18348990e-02 ... 3.27748158e-01\n",
      "  3.09847572e-03 7.67660308e-02]\n",
      " [2.45222879e-01 7.37513787e-05 2.92496872e-01 ... 2.18048121e-02\n",
      "  2.86163411e-02 3.44848489e-02]\n",
      " [2.92359693e-03 3.65822680e-04 5.41247710e-02 ... 2.51441477e-02\n",
      "  3.36344706e-01 5.20217599e-02]\n",
      " ...\n",
      " [3.90547119e-01 2.68102478e-01 2.06047247e-02 ... 1.46238878e-02\n",
      "  1.03921588e-03 5.01231397e-02]\n",
      " [3.50485153e-02 1.13954239e-03 2.37049169e-01 ... 1.36994254e-01\n",
      "  7.29963186e-02 5.88919769e-02]\n",
      " [2.48366361e-01 1.99799216e-01 3.21725726e-02 ... 1.92460762e-02\n",
      "  1.20830228e-02 2.08066872e-01]] & cached\n",
      "Re-used Cached Value, runNum =  193\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  193\n",
      "Provided input from cache for runNum = 193\n",
      "Provided input from cache for runNum = 194\n",
      "activation = [[5.06965887 5.43543208 5.97934603 ... 9.18222222 4.17540305 3.14690575]\n",
      " [0.         3.27272897 0.         ... 0.         0.         0.        ]\n",
      " [0.18836979 3.91699294 1.34983549 ... 1.87039335 4.20813592 1.69592746]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.23977872 0.73172337 1.52252429 ... 0.27710456 5.46647068 0.        ]\n",
      " [3.24809555 6.36490208 0.         ... 2.77772626 1.55629022 1.6205189 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.88531342 0.         0.04531923 ... 0.         0.57599404 0.        ]\n",
      " [1.53005989 5.76819301 2.06718427 ... 3.935373   1.39816866 3.82138685]\n",
      " ...\n",
      " [4.47280352 2.85075342 2.75929317 ... 1.04177963 4.92172003 2.26072721]\n",
      " [3.69840281 0.         5.47259229 ... 4.8637031  2.90427196 2.52044215]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.33199225e-04 2.79530549e-04 4.05241437e-02 ... 3.27564285e-01\n",
      "  3.02063478e-03 7.59489834e-02]\n",
      " [2.43993080e-01 7.08212259e-05 2.94889618e-01 ... 2.12438870e-02\n",
      "  2.80511537e-02 3.38446766e-02]\n",
      " [2.86224127e-03 3.55649466e-04 5.38975661e-02 ... 2.48257210e-02\n",
      "  3.36426532e-01 5.14308726e-02]\n",
      " ...\n",
      " [3.92672364e-01 2.67409720e-01 2.04562784e-02 ... 1.42354377e-02\n",
      "  1.00817398e-03 5.00265910e-02]\n",
      " [3.46939806e-02 1.12002766e-03 2.37835580e-01 ... 1.36659606e-01\n",
      "  7.23323233e-02 5.86536201e-02]\n",
      " [2.48979795e-01 2.00302550e-01 3.21049163e-02 ... 1.90224251e-02\n",
      "  1.18934075e-02 2.09514396e-01]] & cached\n",
      "Re-used Cached Value, runNum =  194\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  194\n",
      "Provided input from cache for runNum = 194\n",
      "Provided input from cache for runNum = 195\n",
      "activation = [[5.0753882  5.43864924 5.98516356 ... 9.20249283 4.18600017 3.15031776]\n",
      " [0.         3.27388681 0.         ... 0.         0.         0.        ]\n",
      " [0.18938002 3.91958323 1.34748707 ... 1.87552612 4.21822798 1.69883323]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.24662623 0.74829962 1.53453799 ... 0.28240774 5.47988787 0.        ]\n",
      " [3.262589   6.38551526 0.         ... 2.78411923 1.56602276 1.63098308]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.8909735  0.         0.05624848 ... 0.         0.57683136 0.        ]\n",
      " [1.53419934 5.78627676 2.0714885  ... 3.97390296 1.41769232 3.84419752]\n",
      " ...\n",
      " [4.50041908 2.88858821 2.78352557 ... 1.06394533 4.95154352 2.27610936]\n",
      " [3.70420272 0.         5.49541294 ... 4.89970851 2.91987263 2.52662184]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.24901735e-04 2.63227224e-04 3.92503757e-02 ... 3.27356561e-01\n",
      "  2.94519306e-03 7.51420918e-02]\n",
      " [2.42805653e-01 6.80401594e-05 2.97279265e-01 ... 2.06978805e-02\n",
      "  2.75006806e-02 3.32186740e-02]\n",
      " [2.80341787e-03 3.45916427e-04 5.36747365e-02 ... 2.45120213e-02\n",
      "  3.36523360e-01 5.08489465e-02]\n",
      " ...\n",
      " [3.94753872e-01 2.66741735e-01 2.03074366e-02 ... 1.38546479e-02\n",
      "  9.78014080e-04 4.99211681e-02]\n",
      " [3.43530009e-02 1.10139721e-03 2.38606115e-01 ... 1.36333080e-01\n",
      "  7.16879782e-02 5.84180743e-02]\n",
      " [2.49556363e-01 2.00834289e-01 3.20333143e-02 ... 1.87975848e-02\n",
      "  1.17053772e-02 2.10943744e-01]] & cached\n",
      "Re-used Cached Value, runNum =  195\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  195\n",
      "Provided input from cache for runNum = 195\n",
      "Provided input from cache for runNum = 196\n",
      "activation = [[5.08108876 5.44184805 5.99088359 ... 9.22264706 4.19649546 3.15372166]\n",
      " [0.         3.27501527 0.         ... 0.         0.         0.        ]\n",
      " [0.19028892 3.92199129 1.34508246 ... 1.88053894 4.22821131 1.70165387]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.25353275 0.76494212 1.54658108 ... 0.28779571 5.49336962 0.        ]\n",
      " [3.27697771 6.40600911 0.         ... 2.79053167 1.57571962 1.64139994]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.89687479 0.         0.06739501 ... 0.         0.57791038 0.        ]\n",
      " [1.53815019 5.80401767 2.07557337 ... 4.01220081 1.43692967 3.8669057 ]\n",
      " ...\n",
      " [4.52800296 2.92644984 2.80779369 ... 1.08624074 4.98130155 2.2915311 ]\n",
      " [3.70995228 0.         5.51799044 ... 4.93545593 2.93525903 2.53271929]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.17063596e-04 2.47784182e-04 3.79952958e-02 ... 3.27065477e-01\n",
      "  2.87073700e-03 7.43308883e-02]\n",
      " [2.41626592e-01 6.53880553e-05 2.99718661e-01 ... 2.01684954e-02\n",
      "  2.69627186e-02 3.26041377e-02]\n",
      " [2.74560554e-03 3.36403166e-04 5.34481699e-02 ... 2.41983246e-02\n",
      "  3.36595259e-01 5.02713411e-02]\n",
      " ...\n",
      " [3.96850934e-01 2.66150922e-01 2.01639207e-02 ... 1.34874713e-02\n",
      "  9.48663444e-04 4.98172268e-02]\n",
      " [3.40137617e-02 1.08337016e-03 2.39348229e-01 ... 1.36024185e-01\n",
      "  7.10449116e-02 5.81806388e-02]\n",
      " [2.50101910e-01 2.01370663e-01 3.19652927e-02 ... 1.85800008e-02\n",
      "  1.15201080e-02 2.12392229e-01]] & cached\n",
      "Re-used Cached Value, runNum =  196\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  196\n",
      "Provided input from cache for runNum = 196\n",
      "Provided input from cache for runNum = 197\n",
      "activation = [[5.08682699 5.44512552 5.99654806 ... 9.24279224 4.20697155 3.15715123]\n",
      " [0.         3.27607271 0.         ... 0.         0.         0.        ]\n",
      " [0.19115316 3.9243178  1.34266378 ... 1.88551052 4.23816131 1.70443221]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.26050721 0.78162104 1.55865624 ... 0.29320684 5.50690756 0.        ]\n",
      " [3.29117748 6.42623089 0.         ... 2.79681486 1.58526135 1.65171446]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.90279565 0.         0.07852864 ... 0.         0.57897693 0.        ]\n",
      " [1.54200924 5.82155067 2.07952128 ... 4.05035818 1.45597889 3.88953497]\n",
      " ...\n",
      " [4.55554714 2.96429647 2.83207683 ... 1.10862076 5.01098914 2.30697815]\n",
      " [3.71591249 0.         5.54049145 ... 4.97134805 2.95079806 2.53892513]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.09759520e-04 2.33341168e-04 3.67754707e-02 ... 3.26755805e-01\n",
      "  2.79821537e-03 7.35301567e-02]\n",
      " [2.40506354e-01 6.28691769e-05 3.02154416e-01 ... 1.96517714e-02\n",
      "  2.64367965e-02 3.20016068e-02]\n",
      " [2.69093523e-03 3.27367407e-04 5.32342453e-02 ... 2.38919120e-02\n",
      "  3.36677323e-01 4.97041417e-02]\n",
      " ...\n",
      " [3.98866860e-01 2.65537328e-01 2.00167786e-02 ... 1.31273428e-02\n",
      "  9.20060317e-04 4.97023934e-02]\n",
      " [3.36898822e-02 1.06614831e-03 2.40070330e-01 ... 1.35720383e-01\n",
      "  7.04152709e-02 5.79453493e-02]\n",
      " [2.50620471e-01 2.01928027e-01 3.18919474e-02 ... 1.83621571e-02\n",
      "  1.13359285e-02 2.13822293e-01]] & cached\n",
      "Re-used Cached Value, runNum =  197\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  197\n",
      "Provided input from cache for runNum = 197\n",
      "Provided input from cache for runNum = 198\n",
      "activation = [[5.09253114 5.44836125 6.00211802 ... 9.26280164 4.21734003 3.16055679]\n",
      " [0.         3.27708134 0.         ... 0.         0.         0.        ]\n",
      " [0.19200782 3.92658172 1.3402448  ... 1.89050605 4.24811287 1.70720482]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.26743883 0.79821998 1.57069353 ... 0.29856035 5.5203678  0.        ]\n",
      " [3.30531486 6.44637309 0.         ... 2.8031681  1.5947981  1.66200435]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.9085615  0.         0.08957525 ... 0.         0.5798777  0.        ]\n",
      " [1.54591813 5.8390251  2.08340396 ... 4.08850582 1.47496378 3.91213241]\n",
      " ...\n",
      " [4.58297223 3.00205456 2.85630642 ... 1.13098767 5.04051171 2.32244657]\n",
      " [3.72172765 0.         5.56271116 ... 5.006882   2.96605907 2.54500028]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.02917011e-04 2.19727778e-04 3.55904800e-02 ... 3.26413897e-01\n",
      "  2.72804120e-03 7.27287009e-02]\n",
      " [2.39325510e-01 6.04516333e-05 3.04568917e-01 ... 1.91449229e-02\n",
      "  2.59197255e-02 3.14068903e-02]\n",
      " [2.63767575e-03 3.18588777e-04 5.30249748e-02 ... 2.35863097e-02\n",
      "  3.36723324e-01 4.91394174e-02]\n",
      " ...\n",
      " [4.00916859e-01 2.64937068e-01 1.98744440e-02 ... 1.27787913e-02\n",
      "  8.92558904e-04 4.95883108e-02]\n",
      " [3.33683772e-02 1.04925162e-03 2.40772031e-01 ... 1.35398664e-01\n",
      "  6.97996869e-02 5.77056697e-02]\n",
      " [2.51135226e-01 2.02483224e-01 3.18224191e-02 ... 1.81512773e-02\n",
      "  1.11573194e-02 2.15272697e-01]] & cached\n",
      "Re-used Cached Value, runNum =  198\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  198\n",
      "Provided input from cache for runNum = 198\n",
      "Provided input from cache for runNum = 199\n",
      "activation = [[5.09820463 5.45154632 6.0075948  ... 9.28270501 4.22760373 3.16394407]\n",
      " [0.         3.27807741 0.         ... 0.         0.         0.        ]\n",
      " [0.19284228 3.92874791 1.33781752 ... 1.89544398 4.25801955 1.70993814]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.2744886  0.81504292 1.58281929 ... 0.30419886 5.53396474 0.        ]\n",
      " [3.31937045 6.46641828 0.         ... 2.80956216 1.60431963 1.67226161]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.91452722 0.         0.1008095  ... 0.         0.58096057 0.        ]\n",
      " [1.54971299 5.85617907 2.08709699 ... 4.12637036 1.49367908 3.93463726]\n",
      " ...\n",
      " [4.61038832 3.03994482 2.88060408 ... 1.15362388 5.07004326 2.33795348]\n",
      " [3.7274491  0.         5.58468536 ... 5.04217384 2.98109833 2.5509715 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.64739338e-05 2.06855868e-04 3.44273167e-02 ... 3.25993240e-01\n",
      "  2.65867529e-03 7.19245679e-02]\n",
      " [2.38141460e-01 5.81400252e-05 3.07012315e-01 ... 1.86534687e-02\n",
      "  2.54130826e-02 3.08219540e-02]\n",
      " [2.58606911e-03 3.10128668e-04 5.28214642e-02 ... 2.32886999e-02\n",
      "  3.36741906e-01 4.85801691e-02]\n",
      " ...\n",
      " [4.02964270e-01 2.64339779e-01 1.97354150e-02 ... 1.24408925e-02\n",
      "  8.65763064e-04 4.94759193e-02]\n",
      " [3.30495678e-02 1.03289679e-03 2.41446620e-01 ... 1.35104911e-01\n",
      "  6.91834373e-02 5.74644610e-02]\n",
      " [2.51632377e-01 2.03022418e-01 3.17551147e-02 ... 1.79461521e-02\n",
      "  1.09810609e-02 2.16740172e-01]] & cached\n",
      "Re-used Cached Value, runNum =  199\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  199\n",
      "Provided input from cache for runNum = 199\n",
      "Provided input from cache for runNum = 200\n",
      "activation = [[5.10383623 5.45467359 6.01297032 ... 9.30248005 4.23774411 3.16730702]\n",
      " [0.         3.27903226 0.         ... 0.         0.         0.        ]\n",
      " [0.19363879 3.93083202 1.33537198 ... 1.90033907 4.26789323 1.71262501]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.28159351 0.83194068 1.59494545 ... 0.30993061 5.54757123 0.        ]\n",
      " [3.33328251 6.48626095 0.         ... 2.81589045 1.61372849 1.68243337]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.92062903 0.         0.11215822 ... 0.         0.58218509 0.        ]\n",
      " [1.55347053 5.87312994 2.09069268 ... 4.16409481 1.51224709 3.95707817]\n",
      " ...\n",
      " [4.6377156  3.07780457 2.90486252 ... 1.17633521 5.09944899 2.35347597]\n",
      " [3.73311606 0.         5.60640034 ... 5.07726951 2.99593535 2.55687875]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.04330105e-05 1.94754468e-04 3.32923353e-02 ... 3.25518551e-01\n",
      "  2.58872731e-03 7.11216917e-02]\n",
      " [2.36982029e-01 5.59413635e-05 3.09485641e-01 ... 1.81767535e-02\n",
      "  2.49072706e-02 3.02483976e-02]\n",
      " [2.53611204e-03 3.01980762e-04 5.26185905e-02 ... 2.29935642e-02\n",
      "  3.36809418e-01 4.80231237e-02]\n",
      " ...\n",
      " [4.04972197e-01 2.63749603e-01 1.95984743e-02 ... 1.21135814e-02\n",
      "  8.41965894e-04 4.93614024e-02]\n",
      " [3.27320759e-02 1.01700544e-03 2.42064996e-01 ... 1.34801523e-01\n",
      "  6.85960084e-02 5.72185382e-02]\n",
      " [2.52123256e-01 2.03580475e-01 3.16907003e-02 ... 1.77473263e-02\n",
      "  1.08200442e-02 2.18220888e-01]] & cached\n",
      "Re-used Cached Value, runNum =  200\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  200\n",
      "Provided input from cache for runNum = 200\n",
      "Provided input from cache for runNum = 201\n",
      "activation = [[5.10942118 5.4577277  6.01823904 ... 9.32212785 4.24777225 3.17062683]\n",
      " [0.         3.2800135  0.         ... 0.         0.         0.        ]\n",
      " [0.19440272 3.93283693 1.33289891 ... 1.90519365 4.27773194 1.71527724]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.28879774 0.8489861  1.60711619 ... 0.31581364 5.56126169 0.        ]\n",
      " [3.34703363 6.50593578 0.         ... 2.82216924 1.62303228 1.69253372]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.92695022 0.         0.12368442 ... 0.         0.58358364 0.        ]\n",
      " [1.55698658 5.8896582  2.09403278 ... 4.20139761 1.53040775 3.97932898]\n",
      " ...\n",
      " [4.66489974 3.11555896 2.92905146 ... 1.19899755 5.128659   2.36892718]\n",
      " [3.73863143 0.         5.62779426 ... 5.11201034 3.01047947 2.56260482]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.47785936e-05 1.83422976e-04 3.21862404e-02 ... 3.25019166e-01\n",
      "  2.51460064e-03 7.03269851e-02]\n",
      " [2.35838558e-01 5.38454106e-05 3.11955084e-01 ... 1.77106660e-02\n",
      "  2.43888380e-02 2.96837122e-02]\n",
      " [2.48834605e-03 2.94227967e-04 5.24239021e-02 ... 2.27039872e-02\n",
      "  3.37063878e-01 4.74749000e-02]\n",
      " ...\n",
      " [4.06960671e-01 2.63170182e-01 1.94627537e-02 ... 1.17963769e-02\n",
      "  8.24535735e-04 4.92454960e-02]\n",
      " [3.24269618e-02 1.00202441e-03 2.42683489e-01 ... 1.34533344e-01\n",
      "  6.80794672e-02 5.69756309e-02]\n",
      " [2.52583047e-01 2.04115402e-01 3.16241010e-02 ... 1.75504234e-02\n",
      "  1.06923123e-02 2.19700693e-01]] & cached\n",
      "Re-used Cached Value, runNum =  201\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  201\n",
      "Provided input from cache for runNum = 201\n",
      "Provided input from cache for runNum = 202\n",
      "activation = [[5.11502735 5.46081428 6.02346144 ... 9.34173128 4.25774362 3.17396611]\n",
      " [0.         3.28095384 0.         ... 0.         0.         0.        ]\n",
      " [0.19519579 3.93484705 1.33047302 ... 1.91010691 4.28761228 1.71793607]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.29602922 0.8660546  1.61927637 ... 0.32175007 5.57496089 0.        ]\n",
      " [3.36067496 6.52539534 0.         ... 2.82835602 1.63221032 1.70253633]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.9331524  0.         0.13510536 ... 0.         0.58491955 0.        ]\n",
      " [1.56045475 5.90599869 2.09724133 ... 4.23853143 1.54836186 4.00148509]\n",
      " ...\n",
      " [4.69187144 3.15308901 2.95306504 ... 1.22154252 5.15761033 2.3842996 ]\n",
      " [3.74422297 0.         5.64903936 ... 5.14675225 3.02502414 2.56836085]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.95204036e-05 1.72848490e-04 3.11188771e-02 ... 3.24524218e-01\n",
      "  2.44336154e-03 6.95463189e-02]\n",
      " [2.34726613e-01 5.18533868e-05 3.14416846e-01 ... 1.72556194e-02\n",
      "  2.38851805e-02 2.91304218e-02]\n",
      " [2.44274711e-03 2.86833098e-04 5.22359658e-02 ... 2.24188976e-02\n",
      "  3.37326751e-01 4.69330725e-02]\n",
      " ...\n",
      " [4.08905648e-01 2.62590330e-01 1.93250846e-02 ... 1.14865027e-02\n",
      "  8.07443344e-04 4.91218792e-02]\n",
      " [3.21323364e-02 9.87663170e-04 2.43270238e-01 ... 1.34254538e-01\n",
      "  6.75772409e-02 5.67320936e-02]\n",
      " [2.53016286e-01 2.04665433e-01 3.15523575e-02 ... 1.73537276e-02\n",
      "  1.05655689e-02 2.21163373e-01]] & cached\n",
      "Re-used Cached Value, runNum =  202\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  202\n",
      "Provided input from cache for runNum = 202\n",
      "Provided input from cache for runNum = 203\n",
      "activation = [[5.12061142 5.4638715  6.02859873 ... 9.36123763 4.26762663 3.17729288]\n",
      " [0.         3.28190571 0.         ... 0.         0.         0.        ]\n",
      " [0.19595412 3.93675455 1.32803475 ... 1.91496184 4.29743288 1.72054094]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.30325934 0.8831294  1.63145295 ... 0.32774273 5.58869146 0.        ]\n",
      " [3.37419155 6.54468479 0.         ... 2.83448513 1.6413084  1.71246086]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.93946455 0.         0.14665518 ... 0.         0.58647773 0.        ]\n",
      " [1.5638058  5.92206339 2.10026697 ... 4.27539506 1.56605504 4.02350135]\n",
      " ...\n",
      " [4.71874334 3.19059747 2.97709473 ... 1.24418483 5.18650342 2.39969029]\n",
      " [3.74986937 0.         5.67011812 ... 5.18137295 3.039513   2.57410686]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.45923303e-05 1.62876686e-04 3.00777077e-02 ... 3.23992784e-01\n",
      "  2.37350526e-03 6.87703690e-02]\n",
      " [2.33619545e-01 4.99402853e-05 3.16880274e-01 ... 1.68120458e-02\n",
      "  2.33937864e-02 2.85870366e-02]\n",
      " [2.39892206e-03 2.79711184e-04 5.20589867e-02 ... 2.21411982e-02\n",
      "  3.37588026e-01 4.64018209e-02]\n",
      " ...\n",
      " [4.10850396e-01 2.62012907e-01 1.91885459e-02 ... 1.11848481e-02\n",
      "  7.90661687e-04 4.89958831e-02]\n",
      " [3.18460243e-02 9.73791758e-04 2.43843910e-01 ... 1.33998915e-01\n",
      "  6.70763512e-02 5.64917987e-02]\n",
      " [2.53412882e-01 2.05183045e-01 3.14784463e-02 ... 1.71587404e-02\n",
      "  1.04396295e-02 2.22618370e-01]] & cached\n",
      "Re-used Cached Value, runNum =  203\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  203\n",
      "Provided input from cache for runNum = 203\n",
      "iterations = 200\n",
      "Accuracy = 0.5541219512195122\n",
      "Provided input from cache for runNum = 204\n",
      "activation = [[5.12614037 5.46687305 6.03363508 ... 9.3806082  4.27738812 3.18059275]\n",
      " [0.         3.28283418 0.         ... 0.         0.         0.        ]\n",
      " [0.19671182 3.93861754 1.3256068  ... 1.91979029 4.30725098 1.72312819]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.31046238 0.90012838 1.64359539 ... 0.33373069 5.60238776 0.        ]\n",
      " [3.38764645 6.56393442 0.         ... 2.84067605 1.6503846  1.72238552]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.94566201 0.         0.15811648 ... 0.         0.58791081 0.        ]\n",
      " [1.56706488 5.93789626 2.10315002 ... 4.31209443 1.58356781 4.04542564]\n",
      " ...\n",
      " [4.74550801 3.22803079 3.00112534 ... 1.26692466 5.21529228 2.41511284]\n",
      " [3.75533554 0.         5.69090827 ... 5.21562633 3.05373056 2.57969229]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.99604611e-05 1.53444858e-04 2.90641060e-02 ... 3.23402324e-01\n",
      "  2.30566603e-03 6.79916186e-02]\n",
      " [2.32465765e-01 4.81026528e-05 3.19347771e-01 ... 1.63807367e-02\n",
      "  2.29121685e-02 2.80521242e-02]\n",
      " [2.35598295e-03 2.72768137e-04 5.18855244e-02 ... 2.18671432e-02\n",
      "  3.37826943e-01 4.58740633e-02]\n",
      " ...\n",
      " [4.12834521e-01 2.61457818e-01 1.90572791e-02 ... 1.08934577e-02\n",
      "  7.74447995e-04 4.88722409e-02]\n",
      " [3.15588495e-02 9.60143440e-04 2.44387400e-01 ... 1.33739243e-01\n",
      "  6.65847665e-02 5.62473303e-02]\n",
      " [2.53797482e-01 2.05683484e-01 3.14089313e-02 ... 1.69710615e-02\n",
      "  1.03172373e-02 2.24096205e-01]] & cached\n",
      "Re-used Cached Value, runNum =  204\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  204\n",
      "Provided input from cache for runNum = 204\n",
      "Provided input from cache for runNum = 205\n",
      "activation = [[5.13164936 5.46985672 6.03858947 ... 9.39986423 4.28704412 3.18386939]\n",
      " [0.         3.28368739 0.         ... 0.         0.         0.        ]\n",
      " [0.19740028 3.9403426  1.32314312 ... 1.92452215 4.31697809 1.72565181]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.31759194 0.91704825 1.65567005 ... 0.33966389 5.61599637 0.        ]\n",
      " [3.40100498 6.58304541 0.         ... 2.84683008 1.6593913  1.73224519]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.95187513 0.         0.16961027 ... 0.         0.58945168 0.        ]\n",
      " [1.57031947 5.9535463  2.10594631 ... 4.34862208 1.60093156 4.0672401 ]\n",
      " ...\n",
      " [4.77199093 3.26515979 3.02496638 ... 1.28944632 5.24377718 2.43045462]\n",
      " [3.76059592 0.         5.71137158 ... 5.24950757 3.06763914 2.58512421]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.56214350e-05 1.44620331e-04 2.80805962e-02 ... 3.22773016e-01\n",
      "  2.24017553e-03 6.72179381e-02]\n",
      " [2.31301344e-01 4.63590775e-05 3.21840878e-01 ... 1.59612723e-02\n",
      "  2.24431148e-02 2.75279543e-02]\n",
      " [2.31346419e-03 2.66024962e-04 5.17015697e-02 ... 2.15871594e-02\n",
      "  3.38039751e-01 4.53460492e-02]\n",
      " ...\n",
      " [4.14827392e-01 2.60976171e-01 1.89292240e-02 ... 1.06136963e-02\n",
      "  7.58703734e-04 4.87473129e-02]\n",
      " [3.12707080e-02 9.47042912e-04 2.44877618e-01 ... 1.33463875e-01\n",
      "  6.60993096e-02 5.59968441e-02]\n",
      " [2.54180175e-01 2.06225857e-01 3.13427263e-02 ... 1.67900213e-02\n",
      "  1.01983937e-02 2.25589250e-01]] & cached\n",
      "Re-used Cached Value, runNum =  205\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  205\n",
      "Provided input from cache for runNum = 205\n",
      "Provided input from cache for runNum = 206\n",
      "activation = [[5.13719272 5.47289698 6.04349779 ... 9.41910745 4.29668678 3.18717408]\n",
      " [0.         3.28446633 0.         ... 0.         0.         0.        ]\n",
      " [0.19801982 3.94191033 1.32064676 ... 1.92911767 4.32659789 1.72811248]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.32494107 0.93430781 1.66785864 ... 0.34595547 5.62983833 0.        ]\n",
      " [3.41423489 6.60197935 0.         ... 2.85295521 1.66831547 1.74202809]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.95836343 0.         0.18134507 ... 0.         0.59134723 0.        ]\n",
      " [1.57348473 5.96892803 2.10859492 ... 4.3849565  1.61813614 4.08863509]\n",
      " ...\n",
      " [4.7987075  3.30277131 3.04900768 ... 1.31254776 5.27256212 2.44605066]\n",
      " [3.76615206 0.         5.73185245 ... 5.2836256  3.08180789 2.59080127]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.15389237e-05 1.36228180e-04 2.71148702e-02 ... 3.22037998e-01\n",
      "  2.17509996e-03 6.64008163e-02]\n",
      " [2.30215140e-01 4.46945866e-05 3.24390897e-01 ... 1.55593170e-02\n",
      "  2.19823898e-02 2.70325096e-02]\n",
      " [2.27308151e-03 2.59535640e-04 5.15278283e-02 ... 2.13202537e-02\n",
      "  3.38269547e-01 4.48160885e-02]\n",
      " ...\n",
      " [4.16737405e-01 2.60459590e-01 1.87995515e-02 ... 1.03400310e-02\n",
      "  7.42863946e-04 4.86457986e-02]\n",
      " [3.09890390e-02 9.34222127e-04 2.45317236e-01 ... 1.33215453e-01\n",
      "  6.56042001e-02 5.57374837e-02]\n",
      " [2.54537587e-01 2.06769686e-01 3.12738568e-02 ... 1.66126257e-02\n",
      "  1.00767350e-02 2.27156289e-01]] & cached\n",
      "Re-used Cached Value, runNum =  206\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  206\n",
      "Provided input from cache for runNum = 206\n",
      "Provided input from cache for runNum = 207\n",
      "activation = [[5.14268111 5.47586274 6.0483039  ... 9.43820339 4.30618527 3.19044363]\n",
      " [0.         3.28516735 0.         ... 0.         0.         0.        ]\n",
      " [0.19862445 3.94342681 1.31813969 ... 1.93368848 4.33619675 1.73054383]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.33223362 0.95147049 1.67997309 ... 0.35212982 5.64358171 0.        ]\n",
      " [3.42740967 6.62086291 0.         ... 2.85915465 1.67726126 1.75179178]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.96479957 0.         0.19305252 ... 0.         0.59319558 0.        ]\n",
      " [1.57661829 5.984117   2.11113356 ... 4.42112306 1.63516049 4.10975784]\n",
      " ...\n",
      " [4.82516233 3.34003543 3.07287277 ... 1.33539642 5.30100469 2.46161235]\n",
      " [3.77150493 0.         5.75202005 ... 5.31728934 3.09558791 2.59635062]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.77195295e-05 1.28346183e-04 2.61815606e-02 ... 3.21306332e-01\n",
      "  2.11238779e-03 6.55740010e-02]\n",
      " [2.29052582e-01 4.30875095e-05 3.26898893e-01 ... 1.51634334e-02\n",
      "  2.15316309e-02 2.65492006e-02]\n",
      " [2.23366025e-03 2.53220724e-04 5.13574158e-02 ... 2.10515532e-02\n",
      "  3.38480740e-01 4.42870196e-02]\n",
      " ...\n",
      " [4.18713280e-01 2.59978920e-01 1.86762255e-02 ... 1.00759571e-02\n",
      "  7.27779104e-04 4.85589262e-02]\n",
      " [3.07121949e-02 9.21779136e-04 2.45757664e-01 ... 1.32962864e-01\n",
      "  6.51236971e-02 5.54744510e-02]\n",
      " [2.54877998e-01 2.07274299e-01 3.12079705e-02 ... 1.64389280e-02\n",
      "  9.96029061e-03 2.28765119e-01]] & cached\n",
      "Re-used Cached Value, runNum =  207\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  207\n",
      "Provided input from cache for runNum = 207\n",
      "Provided input from cache for runNum = 208\n",
      "activation = [[5.1481379  5.47878946 6.0530183  ... 9.45718963 4.31557116 3.19369549]\n",
      " [0.         3.28586886 0.         ... 0.         0.         0.        ]\n",
      " [0.1992327  3.94488442 1.31563124 ... 1.93825651 4.34579065 1.73295442]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.33958102 0.96872261 1.69211043 ... 0.35843185 5.65739568 0.        ]\n",
      " [3.44049331 6.63964851 0.         ... 2.86532829 1.68616812 1.7615134 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.97132514 0.         0.20487577 ... 0.         0.59515807 0.        ]\n",
      " [1.57968932 5.99908207 2.11353978 ... 4.45712542 1.65203736 4.13081303]\n",
      " ...\n",
      " [4.85164034 3.37743605 3.0968111  ... 1.35849928 5.32947998 2.47725614]\n",
      " [3.7769183  0.         5.77203869 ... 5.35087729 3.10926454 2.60187978]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.41335496e-05 1.20875580e-04 2.52712018e-02 ... 3.20525432e-01\n",
      "  2.05096229e-03 6.47468135e-02]\n",
      " [2.27905182e-01 4.15444175e-05 3.29431708e-01 ... 1.47800148e-02\n",
      "  2.10890462e-02 2.60746760e-02]\n",
      " [2.19580644e-03 2.47106339e-04 5.11977712e-02 ... 2.07931391e-02\n",
      "  3.38683765e-01 4.37638814e-02]\n",
      " ...\n",
      " [4.20656554e-01 2.59470359e-01 1.85539158e-02 ... 9.81805963e-03\n",
      "  7.12979262e-04 4.84715571e-02]\n",
      " [3.04375380e-02 9.09443566e-04 2.46150227e-01 ... 1.32709594e-01\n",
      "  6.46433158e-02 5.52088663e-02]\n",
      " [2.55208981e-01 2.07763831e-01 3.11431991e-02 ... 1.62692556e-02\n",
      "  9.84508812e-03 2.30388174e-01]] & cached\n",
      "Re-used Cached Value, runNum =  208\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  208\n",
      "Provided input from cache for runNum = 208\n",
      "Provided input from cache for runNum = 209\n",
      "activation = [[5.15360192 5.48176078 6.05767107 ... 9.47615883 4.32494396 3.1969668 ]\n",
      " [0.         3.28652196 0.         ... 0.         0.         0.        ]\n",
      " [0.19984101 3.94631469 1.31311324 ... 1.94282523 4.35535353 1.73534071]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.346729   0.9856829  1.70410471 ... 0.36445228 5.6709493  0.        ]\n",
      " [3.45346374 6.65826745 0.         ... 2.871458   1.69495574 1.77115125]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.97773852 0.         0.21665342 ... 0.         0.5970257  0.        ]\n",
      " [1.58294845 6.01417016 2.11600972 ... 4.49325543 1.66902313 4.15186248]\n",
      " ...\n",
      " [4.87774028 3.41441178 3.12048312 ... 1.3812323  5.35756394 2.49281551]\n",
      " [3.78243448 0.         5.79195891 ... 5.38447182 3.12304822 2.607479  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.07991117e-05 1.13896573e-04 2.43957076e-02 ... 3.19773542e-01\n",
      "  1.99266332e-03 6.39369705e-02]\n",
      " [2.26721318e-01 4.00597596e-05 3.31931479e-01 ... 1.44021383e-02\n",
      "  2.06574280e-02 2.56072179e-02]\n",
      " [2.15895225e-03 2.41147647e-04 5.10364460e-02 ... 2.05273683e-02\n",
      "  3.38897384e-01 4.32456348e-02]\n",
      " ...\n",
      " [4.22624219e-01 2.58995413e-01 1.84346985e-02 ... 9.56708884e-03\n",
      "  6.98717868e-04 4.83774481e-02]\n",
      " [3.01682672e-02 8.97355209e-04 2.46514052e-01 ... 1.32417098e-01\n",
      "  6.41851497e-02 5.49433060e-02]\n",
      " [2.55525050e-01 2.08267796e-01 3.10786208e-02 ... 1.60997936e-02\n",
      "  9.73311813e-03 2.31993685e-01]] & cached\n",
      "Re-used Cached Value, runNum =  209\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  209\n",
      "Provided input from cache for runNum = 209\n",
      "Provided input from cache for runNum = 210\n",
      "activation = [[5.15906934 5.48473986 6.06226231 ... 9.49503122 4.33424737 3.20022186]\n",
      " [0.         3.28717982 0.         ... 0.         0.         0.        ]\n",
      " [0.2003841  3.94761082 1.31058601 ... 1.94729418 4.36479972 1.73767294]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.35390005 1.00266811 1.71608811 ... 0.3705547  5.68453733 0.        ]\n",
      " [3.46632649 6.6767521  0.         ... 2.87760408 1.70369759 1.78074973]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.98421848 0.         0.22847911 ... 0.         0.59904935 0.        ]\n",
      " [1.58613584 6.02907061 2.11833483 ... 4.52917831 1.68581642 4.17282751]\n",
      " ...\n",
      " [4.90392427 3.45159795 3.14428623 ... 1.40435732 5.38578923 2.50851347]\n",
      " [3.78804169 0.         5.8117345  ... 5.41797159 3.13681004 2.61305494]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.76761456e-05 1.07316991e-04 2.35436204e-02 ... 3.18947371e-01\n",
      "  1.93520909e-03 6.31299251e-02]\n",
      " [2.25559502e-01 3.86350456e-05 3.34419718e-01 ... 1.40352671e-02\n",
      "  2.02335643e-02 2.51471360e-02]\n",
      " [2.12396484e-03 2.35448857e-04 5.08891544e-02 ... 2.02724135e-02\n",
      "  3.39101705e-01 4.27401450e-02]\n",
      " ...\n",
      " [4.24553604e-01 2.58503807e-01 1.83145652e-02 ... 9.32234354e-03\n",
      "  6.84542968e-04 4.82781494e-02]\n",
      " [2.99117480e-02 8.85797525e-04 2.46883534e-01 ... 1.32163101e-01\n",
      "  6.37239491e-02 5.46825203e-02]\n",
      " [2.55822506e-01 2.08762599e-01 3.10111498e-02 ... 1.59333924e-02\n",
      "  9.62058882e-03 2.33596259e-01]] & cached\n",
      "Re-used Cached Value, runNum =  210\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  210\n",
      "Provided input from cache for runNum = 210\n",
      "Provided input from cache for runNum = 211\n",
      "activation = [[5.1644905  5.48765761 6.06676975 ... 9.51374899 4.34340672 3.20344078]\n",
      " [0.         3.28782359 0.         ... 0.         0.         0.        ]\n",
      " [0.20089463 3.94882237 1.30804886 ... 1.9517299  4.37421966 1.73997042]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.36096039 1.01944472 1.72798289 ... 0.37648029 5.69797131 0.        ]\n",
      " [3.47915586 6.69525239 0.         ... 2.88392035 1.71251769 1.79036555]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.99058642 0.         0.24021835 ... 0.         0.60091051 0.        ]\n",
      " [1.58932984 6.04391088 2.12056212 ... 4.56503861 1.70251934 4.19370731]\n",
      " ...\n",
      " [4.92982445 3.4884589  3.16789434 ... 1.42725781 5.41368527 2.52414411]\n",
      " [3.79335836 0.         5.83116296 ... 5.45090882 3.15008523 2.61839929]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.47457963e-05 1.01121542e-04 2.27202190e-02 ... 3.18099142e-01\n",
      "  1.87998653e-03 6.23276325e-02]\n",
      " [2.24326073e-01 3.72610276e-05 3.36894667e-01 ... 1.36759883e-02\n",
      "  1.98182379e-02 2.46927632e-02]\n",
      " [2.08916629e-03 2.29840943e-04 5.07393352e-02 ... 2.00130246e-02\n",
      "  3.39270053e-01 4.22368070e-02]\n",
      " ...\n",
      " [4.26551356e-01 2.58070616e-01 1.82010330e-02 ... 9.08749911e-03\n",
      "  6.71099336e-04 4.81828283e-02]\n",
      " [2.96529114e-02 8.74404188e-04 2.47233096e-01 ... 1.31887242e-01\n",
      "  6.32761935e-02 5.44177958e-02]\n",
      " [2.56114800e-01 2.09248140e-01 3.09484261e-02 ... 1.57741244e-02\n",
      "  9.51397870e-03 2.35222816e-01]] & cached\n",
      "Re-used Cached Value, runNum =  211\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  211\n",
      "Provided input from cache for runNum = 211\n",
      "Provided input from cache for runNum = 212\n",
      "activation = [[5.169871   5.49050782 6.0711824  ... 9.53232984 4.35243955 3.20662738]\n",
      " [0.         3.28846634 0.         ... 0.         0.         0.        ]\n",
      " [0.20143721 3.95002985 1.3055322  ... 1.95621656 4.38366585 1.74227435]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.36787825 1.0359873  1.73977583 ... 0.38220121 5.71124032 0.        ]\n",
      " [3.49191784 6.71364112 0.         ... 2.89022278 1.72129888 1.799926  ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.9966635  0.         0.25179852 ... 0.         0.60256569 0.        ]\n",
      " [1.59253009 6.05864574 2.12268722 ... 4.60076375 1.719089   4.21447495]\n",
      " ...\n",
      " [4.95545759 3.52501985 3.19134572 ... 1.44992213 5.44128129 2.53972906]\n",
      " [3.79852453 0.         5.85032596 ... 5.4835145  3.16306749 2.62362773]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.20135321e-05 9.53143784e-05 2.19264846e-02 ... 3.17261315e-01\n",
      "  1.82699176e-03 6.15354664e-02]\n",
      " [2.23056453e-01 3.59419857e-05 3.39350227e-01 ... 1.33247239e-02\n",
      "  1.94124661e-02 2.42461086e-02]\n",
      " [2.05546050e-03 2.24427368e-04 5.05966943e-02 ... 1.97557049e-02\n",
      "  3.39425182e-01 4.17411362e-02]\n",
      " ...\n",
      " [4.28567114e-01 2.57637407e-01 1.80897588e-02 ... 8.85959062e-03\n",
      "  6.58176099e-04 4.80838257e-02]\n",
      " [2.93989532e-02 8.63272540e-04 2.47563721e-01 ... 1.31591962e-01\n",
      "  6.28431367e-02 5.41535245e-02]\n",
      " [2.56396973e-01 2.09724336e-01 3.08859286e-02 ... 1.56179022e-02\n",
      "  9.41099977e-03 2.36847751e-01]] & cached\n",
      "Re-used Cached Value, runNum =  212\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  212\n",
      "Provided input from cache for runNum = 212\n",
      "Provided input from cache for runNum = 213\n",
      "activation = [[5.17521515 5.49332056 6.07551931 ... 9.55082607 4.36136304 3.20979224]\n",
      " [0.         3.28910716 0.         ... 0.         0.         0.        ]\n",
      " [0.20193746 3.95116062 1.3029857  ... 1.96065369 4.39305631 1.74454233]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.37469253 1.052348   1.75147779 ... 0.38774123 5.72436764 0.        ]\n",
      " [3.50454166 6.73186833 0.         ... 2.89644868 1.7300038  1.80943013]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.00275156 0.         0.26338234 ... 0.         0.6042153  0.        ]\n",
      " [1.59564867 6.07321132 2.12465144 ... 4.63630557 1.735478   4.23512533]\n",
      " ...\n",
      " [4.98100223 3.56150572 3.21479049 ... 1.47260303 5.46877349 2.55537217]\n",
      " [3.80370749 0.         5.86933494 ... 5.51600835 3.17594835 2.62881756]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.94511929e-05 8.98465116e-05 2.11542711e-02 ... 3.16385286e-01\n",
      "  1.77546225e-03 6.07472593e-02]\n",
      " [2.21822756e-01 3.46820333e-05 3.41840784e-01 ... 1.29849147e-02\n",
      "  1.90160337e-02 2.38094307e-02]\n",
      " [2.02293789e-03 2.19200835e-04 5.04589239e-02 ... 1.95026902e-02\n",
      "  3.39578598e-01 4.12539130e-02]\n",
      " ...\n",
      " [4.30546234e-01 2.57205567e-01 1.79789750e-02 ... 8.63808278e-03\n",
      "  6.45530378e-04 4.79804187e-02]\n",
      " [2.91483113e-02 8.52386044e-04 2.47856209e-01 ... 1.31295263e-01\n",
      "  6.24163536e-02 5.38900732e-02]\n",
      " [2.56661667e-01 2.10207794e-01 3.08220332e-02 ... 1.54647563e-02\n",
      "  9.30913819e-03 2.38472734e-01]] & cached\n",
      "Re-used Cached Value, runNum =  213\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  213\n",
      "Provided input from cache for runNum = 213\n",
      "Provided input from cache for runNum = 214\n",
      "activation = [[5.18055626 5.49614653 6.07979139 ... 9.56922204 4.37020237 3.21295406]\n",
      " [0.         3.28969281 0.         ... 0.         0.         0.        ]\n",
      " [0.20245201 3.95228543 1.30045042 ... 1.96513543 4.40245756 1.74680188]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.38143553 1.06861027 1.76310434 ... 0.39320499 5.73739302 0.        ]\n",
      " [3.51703748 6.74995475 0.         ... 2.90267587 1.73864606 1.81888012]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.00855251 0.         0.27476778 ... 0.         0.60562773 0.        ]\n",
      " [1.59883748 6.08769705 2.12659988 ... 4.67172641 1.75175771 4.25570898]\n",
      " ...\n",
      " [5.00636418 3.5977984  3.23816078 ... 1.49521112 5.4960443  2.57099272]\n",
      " [3.80897386 0.         5.88821174 ... 5.54836123 3.18875061 2.63398645]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.70709640e-05 8.47351379e-05 2.04114741e-02 ... 3.15512358e-01\n",
      "  1.72600949e-03 5.99728264e-02]\n",
      " [2.20582978e-01 3.34733179e-05 3.44286835e-01 ... 1.26524934e-02\n",
      "  1.86286209e-02 2.33795715e-02]\n",
      " [1.99206848e-03 2.14217863e-04 5.03330103e-02 ... 1.92553249e-02\n",
      "  3.39731826e-01 4.07764501e-02]\n",
      " ...\n",
      " [4.32512836e-01 2.56753307e-01 1.78681236e-02 ... 8.42166609e-03\n",
      "  6.33260471e-04 4.78703317e-02]\n",
      " [2.89096837e-02 8.41904881e-04 2.48145348e-01 ... 1.30993972e-01\n",
      "  6.20058679e-02 5.36302235e-02]\n",
      " [2.56905164e-01 2.10682328e-01 3.07551430e-02 ... 1.53122269e-02\n",
      "  9.20916496e-03 2.40078110e-01]] & cached\n",
      "Re-used Cached Value, runNum =  214\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  214\n",
      "Provided input from cache for runNum = 214\n",
      "Provided input from cache for runNum = 215\n",
      "activation = [[5.18582474 5.49886164 6.08395451 ... 9.58741112 4.37888158 3.21606143]\n",
      " [0.         3.29032341 0.         ... 0.         0.         0.        ]\n",
      " [0.20293698 3.95334338 1.2978978  ... 1.96959437 4.41180853 1.74902844]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.38815187 1.08482058 1.77467812 ... 0.3986303  5.75034258 0.        ]\n",
      " [3.52941142 6.76789017 0.         ... 2.90886774 1.74719962 1.82825628]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.01451733 0.         0.28630257 ... 0.         0.607206   0.        ]\n",
      " [1.60203327 6.10204346 2.12847648 ... 4.70695347 1.76787623 4.27620326]\n",
      " ...\n",
      " [5.03152286 3.63382844 3.26135906 ... 1.51759539 5.52304347 2.58653825]\n",
      " [3.81411531 0.         5.90680846 ... 5.58034209 3.20127561 2.63902168]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.48361106e-05 7.99409009e-05 1.96905459e-02 ... 3.14373874e-01\n",
      "  1.67806510e-03 5.92060490e-02]\n",
      " [2.19364619e-01 3.23205355e-05 3.46772089e-01 ... 1.23336667e-02\n",
      "  1.82515209e-02 2.29585979e-02]\n",
      " [1.96189206e-03 2.09393911e-04 5.02046884e-02 ... 1.90434436e-02\n",
      "  3.39860966e-01 4.03009218e-02]\n",
      " ...\n",
      " [4.34457159e-01 2.56335785e-01 1.77590348e-02 ... 8.27694955e-03\n",
      "  6.21302089e-04 4.77589985e-02]\n",
      " [2.86697926e-02 8.31716217e-04 2.48371038e-01 ... 1.30956444e-01\n",
      "  6.15982796e-02 5.33673282e-02]\n",
      " [2.57143966e-01 2.11173864e-01 3.06901089e-02 ... 1.52249209e-02\n",
      "  9.11149362e-03 2.41691836e-01]] & cached\n",
      "Re-used Cached Value, runNum =  215\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  215\n",
      "Provided input from cache for runNum = 215\n",
      "Provided input from cache for runNum = 216\n",
      "activation = [[5.19105058 5.50152961 6.08803989 ... 9.60545482 4.38743539 3.21913879]\n",
      " [0.         3.29096637 0.         ... 0.         0.         0.        ]\n",
      " [0.2033939  3.95432982 1.29532721 ... 1.97401495 4.42110719 1.75122824]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.39490779 1.10105894 1.78625821 ... 0.40411118 5.76332232 0.        ]\n",
      " [3.54170564 6.78572094 0.         ... 2.9150839  1.75571744 1.83758755]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.02045378 0.         0.2978225  ... 0.         0.60878538 0.        ]\n",
      " [1.60503213 6.11604897 2.13010524 ... 4.74184431 1.78366639 4.2965326 ]\n",
      " ...\n",
      " [5.0566402  3.66989157 3.28455721 ... 1.54010684 5.55001534 2.60211032]\n",
      " [3.81918885 0.         5.92519368 ... 5.61205953 3.21359719 2.64395462]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.27393747e-05 7.54197356e-05 1.89903957e-02 ... 3.13153923e-01\n",
      "  1.63108141e-03 5.84448792e-02]\n",
      " [2.18161664e-01 3.12174279e-05 3.49258087e-01 ... 1.20252523e-02\n",
      "  1.78828112e-02 2.25459308e-02]\n",
      " [1.93307467e-03 2.04780233e-04 5.00869635e-02 ... 1.88459727e-02\n",
      "  3.39981589e-01 3.98359438e-02]\n",
      " ...\n",
      " [4.36391751e-01 2.55911152e-01 1.76499878e-02 ... 8.14605092e-03\n",
      "  6.09534855e-04 4.76454108e-02]\n",
      " [2.84397159e-02 8.21951531e-04 2.48604299e-01 ... 1.30996891e-01\n",
      "  6.11943226e-02 5.31095496e-02]\n",
      " [2.57349585e-01 2.11640135e-01 3.06207062e-02 ... 1.51493580e-02\n",
      "  9.01408983e-03 2.43300068e-01]] & cached\n",
      "Re-used Cached Value, runNum =  216\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  216\n",
      "Provided input from cache for runNum = 216\n",
      "Provided input from cache for runNum = 217\n",
      "activation = [[5.19627585 5.50422628 6.09208189 ... 9.62345977 4.39594964 3.22222585]\n",
      " [0.         3.29158181 0.         ... 0.         0.         0.        ]\n",
      " [0.20388133 3.95531976 1.29278539 ... 1.97846798 4.43042376 1.75342181]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.40174231 1.11739437 1.79785    ... 0.40973636 5.77639195 0.        ]\n",
      " [3.55386422 6.80331875 0.         ... 2.92116493 1.76409479 1.84681269]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.02623114 0.         0.30917302 ... 0.         0.61023266 0.        ]\n",
      " [1.60796747 6.12985074 2.13160998 ... 4.77647259 1.79923629 4.31676757]\n",
      " ...\n",
      " [5.08166125 3.70591011 3.30769412 ... 1.56270715 5.57691426 2.6176545 ]\n",
      " [3.82440095 0.         5.94348328 ... 5.64382692 3.22597029 2.64891563]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.07909126e-05 7.12016005e-05 1.83167436e-02 ... 3.11934597e-01\n",
      "  1.58565810e-03 5.77005376e-02]\n",
      " [2.17008966e-01 3.01688006e-05 3.51719284e-01 ... 1.17254422e-02\n",
      "  1.75236250e-02 2.21415502e-02]\n",
      " [1.90617417e-03 2.00444103e-04 4.99814382e-02 ... 1.86563476e-02\n",
      "  3.40106453e-01 3.93811001e-02]\n",
      " ...\n",
      " [4.38262487e-01 2.55457882e-01 1.75372684e-02 ... 8.01392241e-03\n",
      "  5.97827712e-04 4.75207270e-02]\n",
      " [2.82244681e-02 8.12765723e-04 2.48833940e-01 ... 1.31044109e-01\n",
      "  6.07991343e-02 5.28560987e-02]\n",
      " [2.57530098e-01 2.12120880e-01 3.05436612e-02 ... 1.50696581e-02\n",
      "  8.91578779e-03 2.44875107e-01]] & cached\n",
      "Re-used Cached Value, runNum =  217\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  217\n",
      "Provided input from cache for runNum = 217\n",
      "Provided input from cache for runNum = 218\n",
      "activation = [[5.20149306 5.50692739 6.09605531 ... 9.64138111 4.40438308 3.22530547]\n",
      " [0.         3.29214299 0.         ... 0.         0.         0.        ]\n",
      " [0.20427041 3.9561194  1.29017921 ... 1.98276141 4.43959963 1.7555287 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.40850901 1.13363907 1.80938374 ... 0.41532199 5.78937383 0.        ]\n",
      " [3.56587023 6.82074285 0.         ... 2.92719398 1.77236378 1.85596805]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.03204078 0.         0.32057366 ... 0.         0.61179572 0.        ]\n",
      " [1.61083099 6.14344737 2.13298566 ... 4.81084979 1.81458901 4.33688553]\n",
      " ...\n",
      " [5.10647404 3.7417655  3.33073307 ... 1.58527241 5.60361056 2.633177  ]\n",
      " [3.82953469 0.         5.96150616 ... 5.67531589 3.23814442 2.65377161]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.89649831e-05 6.72463455e-05 1.76644111e-02 ... 3.10679037e-01\n",
      "  1.54156113e-03 5.69659352e-02]\n",
      " [2.15861132e-01 2.91686489e-05 3.54179505e-01 ... 1.14336326e-02\n",
      "  1.71742346e-02 2.17444783e-02]\n",
      " [1.87983601e-03 1.96252453e-04 4.98745560e-02 ... 1.84663366e-02\n",
      "  3.40213327e-01 3.89331234e-02]\n",
      " ...\n",
      " [4.40147652e-01 2.55073532e-01 1.74263456e-02 ... 7.88568161e-03\n",
      "  5.86373095e-04 4.73924144e-02]\n",
      " [2.80153793e-02 8.04118233e-04 2.49053744e-01 ... 1.31109696e-01\n",
      "  6.04072737e-02 5.26040569e-02]\n",
      " [2.57682169e-01 2.12620123e-01 3.04654573e-02 ... 1.49914046e-02\n",
      "  8.81900124e-03 2.46441734e-01]] & cached\n",
      "Re-used Cached Value, runNum =  218\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  218\n",
      "Provided input from cache for runNum = 218\n",
      "Provided input from cache for runNum = 219\n",
      "activation = [[5.20671631 5.50962911 6.09998051 ... 9.65922947 4.41274477 3.22838244]\n",
      " [0.         3.29267255 0.         ... 0.         0.         0.        ]\n",
      " [0.20459082 3.95679879 1.28754014 ... 1.98697667 4.44869521 1.75758574]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.41519747 1.14977492 1.82084293 ... 0.42084451 5.80224681 0.        ]\n",
      " [3.57783608 6.83813877 0.         ... 2.93327612 1.78064883 1.86512148]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.03777337 0.         0.33193069 ... 0.         0.61333999 0.        ]\n",
      " [1.61379981 6.15703958 2.13437228 ... 4.8452047  1.82997377 4.3569719 ]\n",
      " ...\n",
      " [5.13108583 3.77744938 3.35363969 ... 1.60776071 5.63009848 2.64867676]\n",
      " [3.83456993 0.         5.97930928 ... 5.70651426 3.25011345 2.65851292]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.72545949e-05 6.35282897e-05 1.70355844e-02 ... 3.09409044e-01\n",
      "  1.49906789e-03 5.62401053e-02]\n",
      " [2.14686310e-01 2.82067588e-05 3.56626968e-01 ... 1.11479602e-02\n",
      "  1.68321295e-02 2.13533171e-02]\n",
      " [1.85394786e-03 1.92171598e-04 4.97677657e-02 ... 1.82754277e-02\n",
      "  3.40296661e-01 3.84895967e-02]\n",
      " ...\n",
      " [4.42055757e-01 2.54720975e-01 1.73177262e-02 ... 7.76117368e-03\n",
      "  5.75283485e-04 4.72617560e-02]\n",
      " [2.78073262e-02 7.95716347e-04 2.49243283e-01 ... 1.31165129e-01\n",
      "  6.00206434e-02 5.23491429e-02]\n",
      " [2.57826577e-01 2.13125437e-01 3.03883558e-02 ... 1.49157631e-02\n",
      "  8.72528356e-03 2.48009540e-01]] & cached\n",
      "Re-used Cached Value, runNum =  219\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  219\n",
      "Provided input from cache for runNum = 219\n",
      "Provided input from cache for runNum = 220\n",
      "activation = [[5.21196213 5.51236443 6.10386682 ... 9.67703631 4.42106843 3.23147174]\n",
      " [0.         3.29313134 0.         ... 0.         0.         0.        ]\n",
      " [0.20487746 3.95739635 1.28488276 ... 1.99114454 4.45773355 1.75962033]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.42189959 1.16594198 1.83228059 ... 0.42644622 5.81513946 0.        ]\n",
      " [3.58964977 6.8553446  0.         ... 2.93926491 1.78882062 1.87418539]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.04332196 0.         0.34311559 ... 0.         0.61476953 0.        ]\n",
      " [1.61666026 6.17033959 2.13561494 ... 4.87924038 1.84509683 4.37692672]\n",
      " ...\n",
      " [5.15552446 3.81300132 3.37643728 ... 1.63020104 5.65640826 2.66411476]\n",
      " [3.8396644  0.         5.99696723 ... 5.73763107 3.26203192 2.66323776]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.56555101e-05 6.00346415e-05 1.64287944e-02 ... 3.08128924e-01\n",
      "  1.45801355e-03 5.55267795e-02]\n",
      " [2.13542364e-01 2.72914382e-05 3.59086611e-01 ... 1.08720308e-02\n",
      "  1.64995988e-02 2.09713207e-02]\n",
      " [1.82901291e-03 1.88253329e-04 4.96633540e-02 ... 1.80886955e-02\n",
      "  3.40387355e-01 3.80536213e-02]\n",
      " ...\n",
      " [4.43946870e-01 2.54383478e-01 1.72094053e-02 ... 7.63852872e-03\n",
      "  5.64390814e-04 4.71257662e-02]\n",
      " [2.76059686e-02 7.87678921e-04 2.49409775e-01 ... 1.31226498e-01\n",
      "  5.96427617e-02 5.20952170e-02]\n",
      " [2.57937172e-01 2.13636761e-01 3.03079266e-02 ... 1.48396752e-02\n",
      "  8.63194136e-03 2.49560390e-01]] & cached\n",
      "Re-used Cached Value, runNum =  220\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  220\n",
      "Provided input from cache for runNum = 220\n",
      "Provided input from cache for runNum = 221\n",
      "activation = [[5.21719995 5.51509819 6.10769986 ... 9.69478351 4.42930283 3.2345557 ]\n",
      " [0.         3.293596   0.         ... 0.         0.         0.        ]\n",
      " [0.20507144 3.95782336 1.28216309 ... 1.99514562 4.4666607  1.76157757]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.42858359 1.18203607 1.84369222 ... 0.4320475  5.82798692 0.        ]\n",
      " [3.60131736 6.87231869 0.         ... 2.94512585 1.79683509 1.88315496]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.04909499 0.         0.35451117 ... 0.         0.61644032 0.        ]\n",
      " [1.61946222 6.18345511 2.13673919 ... 4.91307639 1.86005685 4.39678036]\n",
      " ...\n",
      " [5.17983089 3.84840889 3.39916759 ... 1.65263918 5.68256751 2.67954187]\n",
      " [3.84488489 0.         6.01453653 ... 5.76882027 3.27398982 2.6680171 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.41498498e-05 5.67343049e-05 1.58379249e-02 ... 3.06810601e-01\n",
      "  1.41806093e-03 5.48205416e-02]\n",
      " [2.12449983e-01 2.64172866e-05 3.61610639e-01 ... 1.06063444e-02\n",
      "  1.61762524e-02 2.05990374e-02]\n",
      " [1.80464318e-03 1.84410459e-04 4.95559659e-02 ... 1.79024194e-02\n",
      "  3.40482414e-01 3.76226995e-02]\n",
      " ...\n",
      " [4.45805611e-01 2.54092348e-01 1.71015618e-02 ... 7.51889694e-03\n",
      "  5.53602657e-04 4.69869383e-02]\n",
      " [2.74039797e-02 7.79788816e-04 2.49511021e-01 ... 1.31289004e-01\n",
      "  5.92639435e-02 5.18396615e-02]\n",
      " [2.58025306e-01 2.14165721e-01 3.02264993e-02 ... 1.47647578e-02\n",
      "  8.53853729e-03 2.51105024e-01]] & cached\n",
      "Re-used Cached Value, runNum =  221\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  221\n",
      "Provided input from cache for runNum = 221\n",
      "Provided input from cache for runNum = 222\n",
      "activation = [[5.22235544 5.51768494 6.11143657 ... 9.71230086 4.43735175 3.23757602]\n",
      " [0.         3.29407231 0.         ... 0.         0.         0.        ]\n",
      " [0.20527409 3.95824887 1.27945294 ... 1.99917751 4.4755809  1.76353635]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.43526674 1.1980795  1.85507889 ... 0.43765658 5.84083022 0.        ]\n",
      " [3.61297383 6.88932034 0.         ... 2.95112178 1.80488426 1.89214062]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.05480287 0.         0.36588282 ... 0.         0.61816225 0.        ]\n",
      " [1.62220445 6.19640842 2.13772259 ... 4.94672005 1.87484136 4.41654382]\n",
      " ...\n",
      " [5.20406383 3.8837674  3.42185312 ... 1.6751233  5.70868583 2.69499693]\n",
      " [3.84992466 0.         6.0318492  ... 5.799531   3.28561771 2.67261358]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.27267050e-05 5.35826315e-05 1.52645338e-02 ... 3.05438598e-01\n",
      "  1.37880448e-03 5.41110342e-02]\n",
      " [2.11288642e-01 2.55630202e-05 3.64131410e-01 ... 1.03485801e-02\n",
      "  1.58574680e-02 2.02317458e-02]\n",
      " [1.78064095e-03 1.80607742e-04 4.94563670e-02 ... 1.77236439e-02\n",
      "  3.40558004e-01 3.71952292e-02]\n",
      " ...\n",
      " [4.47726660e-01 2.53783768e-01 1.70003777e-02 ... 7.40777680e-03\n",
      "  5.43229634e-04 4.68566435e-02]\n",
      " [2.71977037e-02 7.71682280e-04 2.49587706e-01 ... 1.31368311e-01\n",
      "  5.88850301e-02 5.15805466e-02]\n",
      " [2.58104040e-01 2.14625062e-01 3.01506939e-02 ... 1.47002833e-02\n",
      "  8.44809235e-03 2.52683427e-01]] & cached\n",
      "Re-used Cached Value, runNum =  222\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  222\n",
      "Provided input from cache for runNum = 222\n",
      "Provided input from cache for runNum = 223\n",
      "activation = [[5.22749626 5.52026449 6.11511495 ... 9.72969829 4.44532257 3.24058712]\n",
      " [0.         3.29449917 0.         ... 0.         0.         0.        ]\n",
      " [0.20538808 3.95852681 1.27668488 ... 2.00308157 4.48436376 1.76542388]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.44203961 1.21428045 1.86652784 ... 0.44346018 5.85373894 0.        ]\n",
      " [3.62449772 6.90611922 0.         ... 2.95704476 1.81281756 1.90103892]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.06087737 0.         0.37757043 ... 0.         0.62027984 0.        ]\n",
      " [1.62487279 6.20910995 2.13857334 ... 4.98003479 1.88940609 4.43621473]\n",
      " ...\n",
      " [5.22824026 3.91914227 3.44454148 ... 1.69773024 5.7347432  2.7104355 ]\n",
      " [3.85512156 0.         6.04910145 ... 5.83025762 3.29733499 2.67727906]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.13927866e-05 5.06213736e-05 1.47072890e-02 ... 3.04037180e-01\n",
      "  1.34030516e-03 5.34138626e-02]\n",
      " [2.10199079e-01 2.47479437e-05 3.66681416e-01 ... 1.00993738e-02\n",
      "  1.55470477e-02 1.98719625e-02]\n",
      " [1.75809574e-03 1.76989588e-04 4.93660934e-02 ... 1.75516200e-02\n",
      "  3.40643796e-01 3.67757663e-02]\n",
      " ...\n",
      " [4.49558002e-01 2.53449055e-01 1.68956094e-02 ... 7.29795199e-03\n",
      "  5.32854611e-04 4.67194538e-02]\n",
      " [2.69985941e-02 7.63934499e-04 2.49620915e-01 ... 1.31468199e-01\n",
      "  5.85009585e-02 5.13241284e-02]\n",
      " [2.58179526e-01 2.15095679e-01 3.00704384e-02 ... 1.46353133e-02\n",
      "  8.35655400e-03 2.54240513e-01]] & cached\n",
      "Re-used Cached Value, runNum =  223\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  223\n",
      "Provided input from cache for runNum = 223\n",
      "Provided input from cache for runNum = 224\n",
      "activation = [[5.23260203 5.5227619  6.11872505 ... 9.74693697 4.4531734  3.24356143]\n",
      " [0.         3.29490114 0.         ... 0.         0.         0.        ]\n",
      " [0.20550065 3.95876631 1.27391594 ... 2.00697352 4.49311432 1.76729862]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.44881874 1.23047762 1.87800023 ... 0.44930783 5.86663035 0.        ]\n",
      " [3.63602202 6.92292902 0.         ... 2.96309861 1.82076641 1.90996057]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.06687576 0.         0.38925164 ... 0.         0.6223874  0.        ]\n",
      " [1.62759932 6.22175355 2.13936252 ... 5.01326246 1.90391149 4.45585715]\n",
      " ...\n",
      " [5.25235343 3.95454763 3.46725249 ... 1.72048975 5.76077024 2.72592065]\n",
      " [3.86015193 0.         6.06611408 ... 5.86057201 3.30876342 2.68177328]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.01378554e-05 4.78110900e-05 1.41684595e-02 ... 3.02583090e-01\n",
      "  1.30275676e-03 5.27179845e-02]\n",
      " [2.09043527e-01 2.39559766e-05 3.69196858e-01 ... 9.85573091e-03\n",
      "  1.52407814e-02 1.95156627e-02]\n",
      " [1.73597977e-03 1.73441046e-04 4.92844484e-02 ... 1.73846371e-02\n",
      "  3.40698548e-01 3.63596461e-02]\n",
      " ...\n",
      " [4.51433467e-01 2.53115393e-01 1.67943579e-02 ... 7.19388536e-03\n",
      "  5.22808606e-04 4.65858960e-02]\n",
      " [2.67999424e-02 7.56202297e-04 2.49648588e-01 ... 1.31583907e-01\n",
      "  5.81186403e-02 5.10649435e-02]\n",
      " [2.58260563e-01 2.15542749e-01 2.99934396e-02 ... 1.45774959e-02\n",
      "  8.26775092e-03 2.55820535e-01]] & cached\n",
      "Re-used Cached Value, runNum =  224\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  224\n",
      "Provided input from cache for runNum = 224\n",
      "Provided input from cache for runNum = 225\n",
      "activation = [[5.23770909 5.52526702 6.12230103 ... 9.76411167 4.46095726 3.24654046]\n",
      " [0.         3.29528445 0.         ... 0.         0.         0.        ]\n",
      " [0.20557675 3.95892037 1.27114281 ... 2.0107911  4.50179337 1.76912582]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.45546923 1.246462   1.88935008 ... 0.45493594 5.87934534 0.        ]\n",
      " [3.64753141 6.93971468 0.         ... 2.96923005 1.82873099 1.91887669]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.0725893  0.         0.40070014 ... 0.         0.62429617 0.        ]\n",
      " [1.63039537 6.23438961 2.14012758 ... 5.04646936 1.91836892 4.47544533]\n",
      " ...\n",
      " [5.2761406  3.9895569  3.48969536 ... 1.74289705 5.78642151 2.74132202]\n",
      " [3.86512968 0.         6.08296444 ... 5.89062887 3.31999705 2.68619739]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.89593859e-05 4.51623972e-05 1.36504447e-02 ... 3.01117520e-01\n",
      "  1.26694910e-03 5.20296464e-02]\n",
      " [2.07824567e-01 2.31912389e-05 3.71705424e-01 ... 9.61800998e-03\n",
      "  1.49416732e-02 1.91656807e-02]\n",
      " [1.71348304e-03 1.69906982e-04 4.91902343e-02 ... 1.72103587e-02\n",
      "  3.40750626e-01 3.59440857e-02]\n",
      " ...\n",
      " [4.53393837e-01 2.52860795e-01 1.67003195e-02 ... 7.09604784e-03\n",
      "  5.13250380e-04 4.64546941e-02]\n",
      " [2.65986752e-02 7.48522185e-04 2.49655130e-01 ... 1.31679233e-01\n",
      "  5.77537904e-02 5.08015091e-02]\n",
      " [2.58319698e-01 2.15999945e-01 2.99203789e-02 ... 1.45248629e-02\n",
      "  8.18278312e-03 2.57411418e-01]] & cached\n",
      "Re-used Cached Value, runNum =  225\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  225\n",
      "Provided input from cache for runNum = 225\n",
      "Provided input from cache for runNum = 226\n",
      "activation = [[5.24279574 5.52774377 6.12581646 ... 9.78115561 4.46864117 3.24949239]\n",
      " [0.         3.29562525 0.         ... 0.         0.         0.        ]\n",
      " [0.20564184 3.95904663 1.26837063 ... 2.01461391 4.51043541 1.77093915]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.4621807  1.26250081 1.90072473 ... 0.4607071  5.89207709 0.        ]\n",
      " [3.65891479 6.9563437  0.         ... 2.97532088 1.83660926 1.92771911]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.07830565 0.         0.41213461 ... 0.         0.6262192  0.        ]\n",
      " [1.6331605  6.24687757 2.14079436 ... 5.07940874 1.932671   4.49495483]\n",
      " ...\n",
      " [5.29992035 4.02462452 3.51217309 ... 1.76548405 5.81206238 2.75675901]\n",
      " [3.87017799 0.         6.09969338 ... 5.92054022 3.33114504 2.69058237]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.78586294e-05 4.26758735e-05 1.31507461e-02 ... 2.99638179e-01\n",
      "  1.23202483e-03 5.13521994e-02]\n",
      " [2.06641462e-01 2.24573697e-05 3.74189256e-01 ... 9.38672246e-03\n",
      "  1.46482359e-02 1.88214372e-02]\n",
      " [1.69253215e-03 1.66562272e-04 4.91126805e-02 ... 1.70463013e-02\n",
      "  3.40796778e-01 3.55387814e-02]\n",
      " ...\n",
      " [4.55283694e-01 2.52552876e-01 1.66034992e-02 ... 6.99904468e-03\n",
      "  5.03772703e-04 4.63159148e-02]\n",
      " [2.64081188e-02 7.41200779e-04 2.49655790e-01 ... 1.31796164e-01\n",
      "  5.73886965e-02 5.05415000e-02]\n",
      " [2.58380631e-01 2.16454372e-01 2.98433207e-02 ... 1.44723222e-02\n",
      "  8.09780413e-03 2.58985436e-01]] & cached\n",
      "Re-used Cached Value, runNum =  226\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  226\n",
      "Provided input from cache for runNum = 226\n",
      "Provided input from cache for runNum = 227\n",
      "activation = [[5.24784626 5.5301654  6.12926678 ... 9.79807052 4.47621681 3.25241814]\n",
      " [0.         3.29595893 0.         ... 0.         0.         0.        ]\n",
      " [0.20570587 3.95918053 1.26558897 ... 2.01845802 4.51905611 1.77275199]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.46881996 1.27844485 1.91201728 ... 0.46637257 5.9047521  0.        ]\n",
      " [3.67014851 6.97279571 0.         ... 2.98133437 1.84436909 1.93647717]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.08403626 0.         0.4235809  ... 0.         0.6281954  0.        ]\n",
      " [1.63591656 6.25925221 2.14139459 ... 5.11217005 1.94681618 4.51437495]\n",
      " ...\n",
      " [5.32349502 4.05949893 3.53452092 ... 1.78792995 5.83752119 2.77216442]\n",
      " [3.87524108 0.         6.11624611 ... 5.9502507  3.34217032 2.69492253]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.68302780e-05 4.03484914e-05 1.26696232e-02 ... 2.98175159e-01\n",
      "  1.19822846e-03 5.06872646e-02]\n",
      " [2.05485666e-01 2.17540061e-05 3.76668534e-01 ... 9.16108311e-03\n",
      "  1.43621198e-02 1.84837587e-02]\n",
      " [1.67257275e-03 1.63371990e-04 4.90408733e-02 ... 1.68845372e-02\n",
      "  3.40834554e-01 3.51401248e-02]\n",
      " ...\n",
      " [4.57131529e-01 2.52219313e-01 1.65060770e-02 ... 6.90317351e-03\n",
      "  4.94459254e-04 4.61709540e-02]\n",
      " [2.62225516e-02 7.34131945e-04 2.49624290e-01 ... 1.31898309e-01\n",
      "  5.70270161e-02 5.02826247e-02]\n",
      " [2.58436656e-01 2.16911910e-01 2.97645145e-02 ... 1.44195414e-02\n",
      "  8.01374300e-03 2.60543011e-01]] & cached\n",
      "Re-used Cached Value, runNum =  227\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  227\n",
      "Provided input from cache for runNum = 227\n",
      "Provided input from cache for runNum = 228\n",
      "activation = [[5.25284123 5.53251    6.13263302 ... 9.81482205 4.4836606  3.25529724]\n",
      " [0.         3.2963028  0.         ... 0.         0.         0.        ]\n",
      " [0.20575543 3.95925453 1.26279445 ... 2.0222726  4.52763711 1.77454271]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.47548498 1.29441435 1.9233106  ... 0.47208479 5.91743394 0.        ]\n",
      " [3.68124175 6.98906356 0.         ... 2.98727734 1.85203022 1.94514718]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.08999127 0.         0.43522404 ... 0.         0.6303741  0.        ]\n",
      " [1.63858005 6.27139764 2.14186594 ... 5.14467401 1.96075937 4.53367964]\n",
      " ...\n",
      " [5.34690813 4.09423631 3.55677533 ... 1.81033331 5.86282252 2.78751956]\n",
      " [3.88027172 0.         6.13259361 ... 5.97971734 3.35303182 2.69918219]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.58642504e-05 3.81580734e-05 1.22039251e-02 ... 2.96707836e-01\n",
      "  1.16529073e-03 5.00329362e-02]\n",
      " [2.04358381e-01 2.10789075e-05 3.79160493e-01 ... 8.94146844e-03\n",
      "  1.40832422e-02 1.81524928e-02]\n",
      " [1.65349458e-03 1.60305569e-04 4.89765882e-02 ... 1.67274428e-02\n",
      "  3.40861899e-01 3.47475565e-02]\n",
      " ...\n",
      " [4.58951575e-01 2.51886853e-01 1.64086837e-02 ... 6.80951429e-03\n",
      "  4.85307849e-04 4.60237221e-02]\n",
      " [2.60401574e-02 7.27299002e-04 2.49558072e-01 ... 1.32007935e-01\n",
      "  5.66649278e-02 5.00253980e-02]\n",
      " [2.58478225e-01 2.17356471e-01 2.96841965e-02 ... 1.43677101e-02\n",
      "  7.93053812e-03 2.62090340e-01]] & cached\n",
      "Re-used Cached Value, runNum =  228\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  228\n",
      "Provided input from cache for runNum = 228\n",
      "Provided input from cache for runNum = 229\n",
      "activation = [[5.25783728 5.53485568 6.13595579 ... 9.83150734 4.4910309  3.25817463]\n",
      " [0.         3.29663402 0.         ... 0.         0.         0.        ]\n",
      " [0.20575383 3.9592198  1.25997704 ... 2.02598602 4.53612594 1.77628911]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.48208008 1.31025522 1.9345402  ... 0.47773428 5.93003015 0.        ]\n",
      " [3.69230742 7.00533168 0.         ... 2.99332467 1.85972354 1.95382911]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.09577539 0.         0.44674835 ... 0.         0.63247487 0.        ]\n",
      " [1.64131627 6.28353822 2.14230801 ... 5.17712647 1.97466843 4.55294684]\n",
      " ...\n",
      " [5.37020595 4.12888974 3.5789888  ... 1.83281796 5.8880227  2.80291526]\n",
      " [3.8852948  0.         6.14880101 ... 6.00897006 3.36376193 2.70338323]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.49536009e-05 3.60763335e-05 1.17539249e-02 ... 2.95181806e-01\n",
      "  1.13335308e-03 4.93799622e-02]\n",
      " [2.03176154e-01 2.04232610e-05 3.81632474e-01 ... 8.72788450e-03\n",
      "  1.38091098e-02 1.78266407e-02]\n",
      " [1.63434781e-03 1.57250045e-04 4.89103530e-02 ... 1.65716091e-02\n",
      "  3.40882122e-01 3.43579685e-02]\n",
      " ...\n",
      " [4.60842213e-01 2.51605865e-01 1.63168802e-02 ... 6.72149694e-03\n",
      "  4.76501897e-04 4.58805837e-02]\n",
      " [2.58574175e-02 7.20442655e-04 2.49491426e-01 ... 1.32133051e-01\n",
      "  5.63111642e-02 4.97660953e-02]\n",
      " [2.58498332e-01 2.17791664e-01 2.96066802e-02 ... 1.43220067e-02\n",
      "  7.84980734e-03 2.63652721e-01]] & cached\n",
      "Re-used Cached Value, runNum =  229\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  229\n",
      "Provided input from cache for runNum = 229\n",
      "Provided input from cache for runNum = 230\n",
      "activation = [[5.26281619 5.53715071 6.13921962 ... 9.84806724 4.49829617 3.26102407]\n",
      " [0.         3.29700949 0.         ... 0.         0.         0.        ]\n",
      " [0.20572237 3.95913661 1.25714569 ... 2.02967745 4.54455899 1.77801685]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.48871516 1.32617751 1.94577437 ... 0.48353133 5.94268816 0.        ]\n",
      " [3.70329581 7.02146502 0.         ... 2.99934669 1.86733854 1.96245368]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.10171369 0.         0.45843703 ... 0.         0.63484693 0.        ]\n",
      " [1.64406646 6.29552768 2.14269422 ... 5.20936323 1.98840651 4.57215163]\n",
      " ...\n",
      " [5.39339564 4.16347777 3.60114428 ... 1.85538942 5.91313719 2.81828626]\n",
      " [3.89030746 0.         6.16484845 ... 6.03803566 3.37435837 2.70751245]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.41007283e-05 3.41216516e-05 1.13198085e-02 ... 2.93654324e-01\n",
      "  1.10212045e-03 4.87396801e-02]\n",
      " [2.02020297e-01 1.97927342e-05 3.84097847e-01 ... 8.51949420e-03\n",
      "  1.35415149e-02 1.75059185e-02]\n",
      " [1.61623431e-03 1.54339397e-04 4.88547242e-02 ... 1.64223453e-02\n",
      "  3.40890218e-01 3.39755784e-02]\n",
      " ...\n",
      " [4.62682592e-01 2.51298855e-01 1.62230669e-02 ... 6.63404877e-03\n",
      "  4.67768204e-04 4.57316007e-02]\n",
      " [2.56801797e-02 7.13880642e-04 2.49396006e-01 ... 1.32266985e-01\n",
      "  5.59521179e-02 4.95084822e-02]\n",
      " [2.58523160e-01 2.18218926e-01 2.95266145e-02 ... 1.42758304e-02\n",
      "  7.76933945e-03 2.65199155e-01]] & cached\n",
      "Re-used Cached Value, runNum =  230\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  230\n",
      "Provided input from cache for runNum = 230\n",
      "Provided input from cache for runNum = 231\n",
      "activation = [[5.26779195 5.53945727 6.14244723 ... 9.86455491 4.50549544 3.26387217]\n",
      " [0.         3.29736245 0.         ... 0.         0.         0.        ]\n",
      " [0.20567784 3.95902346 1.25429589 ... 2.03336268 4.5529712  1.7797377 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.49529696 1.34204739 1.95697855 ... 0.48934144 5.95528986 0.        ]\n",
      " [3.71417454 7.03742782 0.         ... 3.00532053 1.87486943 1.97100316]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.10756085 0.         0.47006238 ... 0.         0.63714651 0.        ]\n",
      " [1.64685652 6.30742498 2.14305288 ... 5.2414378  2.00203534 4.59128967]\n",
      " ...\n",
      " [5.4163163  4.19779952 3.62313939 ... 1.87780616 5.93796544 2.83356585]\n",
      " [3.89537208 0.         6.18080214 ... 6.06703214 3.38489302 2.71164695]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.33054069e-05 3.22953899e-05 1.09033152e-02 ... 2.92151999e-01\n",
      "  1.07221621e-03 4.81157597e-02]\n",
      " [2.00882961e-01 1.91893201e-05 3.86550332e-01 ... 8.31608657e-03\n",
      "  1.32813936e-02 1.71913922e-02]\n",
      " [1.59886646e-03 1.51555948e-04 4.88025590e-02 ... 1.62743654e-02\n",
      "  3.40897151e-01 3.35990050e-02]\n",
      " ...\n",
      " [4.64499016e-01 2.50997994e-01 1.61277552e-02 ... 6.54678328e-03\n",
      "  4.59231196e-04 4.55758519e-02]\n",
      " [2.55093406e-02 7.07632961e-04 2.49277588e-01 ... 1.32387115e-01\n",
      "  5.56035523e-02 4.92528074e-02]\n",
      " [2.58536748e-01 2.18663354e-01 2.94427149e-02 ... 1.42273672e-02\n",
      "  7.69013852e-03 2.66718003e-01]] & cached\n",
      "Re-used Cached Value, runNum =  231\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  231\n",
      "Provided input from cache for runNum = 231\n",
      "Provided input from cache for runNum = 232\n",
      "activation = [[5.27275361 5.54176842 6.14563615 ... 9.88096566 4.51263113 3.26671849]\n",
      " [0.         3.29773174 0.         ... 0.         0.         0.        ]\n",
      " [0.20565878 3.95892326 1.25145681 ... 2.03708086 4.56137484 1.78147051]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.50177428 1.35774115 1.96810497 ... 0.49501088 5.967769   0.        ]\n",
      " [3.72495746 7.05325685 0.         ... 3.01126765 1.88232243 1.97950358]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.11331447 0.         0.48165651 ... 0.         0.63941755 0.        ]\n",
      " [1.64967194 6.31928921 2.14334595 ... 5.27340887 2.0155796  4.61036201]\n",
      " ...\n",
      " [5.43907592 4.23195568 3.64507808 ... 1.90021723 5.96266388 2.84886569]\n",
      " [3.90053421 0.         6.19666749 ... 6.09595732 3.39543545 2.71580274]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.25602694e-05 3.05764682e-05 1.05015590e-02 ... 2.90642341e-01\n",
      "  1.04331714e-03 4.75008611e-02]\n",
      " [1.99760485e-01 1.86077444e-05 3.89006940e-01 ... 8.11828216e-03\n",
      "  1.30274423e-02 1.68834131e-02]\n",
      " [1.58216740e-03 1.48863758e-04 4.87551738e-02 ... 1.61294681e-02\n",
      "  3.40903883e-01 3.32288153e-02]\n",
      " ...\n",
      " [4.66299436e-01 2.50686917e-01 1.60322328e-02 ... 6.46119591e-03\n",
      "  4.50858232e-04 4.54158034e-02]\n",
      " [2.53422299e-02 7.01486963e-04 2.49133936e-01 ... 1.32501677e-01\n",
      "  5.52619490e-02 4.89990704e-02]\n",
      " [2.58535246e-01 2.19105694e-01 2.93565283e-02 ... 1.41797631e-02\n",
      "  7.61169872e-03 2.68221769e-01]] & cached\n",
      "Re-used Cached Value, runNum =  232\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  232\n",
      "Provided input from cache for runNum = 232\n",
      "Provided input from cache for runNum = 233\n",
      "activation = [[5.27766883 5.54401716 6.14874818 ... 9.89724277 4.51966749 3.26953509]\n",
      " [0.         3.29809649 0.         ... 0.         0.         0.        ]\n",
      " [0.20564974 3.95879552 1.24862099 ... 2.04079345 4.56974614 1.78319906]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.50819497 1.37335111 1.97920202 ... 0.5006074  5.98017162 0.        ]\n",
      " [3.73571361 7.06906336 0.         ... 3.01730555 1.88980019 1.98800773]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.11909198 0.         0.49330552 ... 0.         0.64168386 0.        ]\n",
      " [1.65242227 6.33098412 2.14349157 ... 5.30520999 2.02899151 4.62933596]\n",
      " ...\n",
      " [5.46162959 4.26591854 3.66689904 ... 1.92250712 5.98716531 2.86413492]\n",
      " [3.90560446 0.         6.21234084 ... 6.12455084 3.40575633 2.71984977]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.18560154e-05 2.89448994e-05 1.01125186e-02 ... 2.89113132e-01\n",
      "  1.01529454e-03 4.68896412e-02]\n",
      " [1.98606417e-01 1.80439041e-05 3.91477716e-01 ... 7.92576867e-03\n",
      "  1.27790541e-02 1.65809504e-02]\n",
      " [1.56551294e-03 1.46204457e-04 4.87083577e-02 ... 1.59864052e-02\n",
      "  3.40897864e-01 3.28617612e-02]\n",
      " ...\n",
      " [4.68153314e-01 2.50408829e-01 1.59411109e-02 ... 6.38077120e-03\n",
      "  4.42799634e-04 4.52609940e-02]\n",
      " [2.51726989e-02 6.95337796e-04 2.48970156e-01 ... 1.32626835e-01\n",
      "  5.49271009e-02 4.87446628e-02]\n",
      " [2.58508689e-01 2.19518605e-01 2.92719446e-02 ... 1.41374013e-02\n",
      "  7.53570423e-03 2.69739440e-01]] & cached\n",
      "Re-used Cached Value, runNum =  233\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  233\n",
      "Provided input from cache for runNum = 233\n",
      "Provided input from cache for runNum = 234\n",
      "activation = [[5.28257041 5.54624037 6.15182005 ... 9.91345662 4.52663382 3.27234445]\n",
      " [0.         3.29847341 0.         ... 0.         0.         0.        ]\n",
      " [0.20556771 3.95857141 1.24574406 ... 2.04439156 4.57799562 1.78486897]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.51461786 1.38895212 1.99026254 ... 0.50626175 5.99255701 0.        ]\n",
      " [3.74638842 7.08475991 0.         ... 3.02331878 1.89721661 1.99644673]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.12504403 0.         0.50509077 ... 0.         0.64421199 0.        ]\n",
      " [1.65519419 6.3425871  2.14359699 ... 5.33682839 2.04230579 4.64824016]\n",
      " ...\n",
      " [5.48407969 4.29979515 3.68864622 ... 1.94484951 6.01159387 2.87939508]\n",
      " [3.91064897 0.         6.2278596  ... 6.15298485 3.41599325 2.72386873]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.11911594e-05 2.74057090e-05 9.73538470e-03 ... 2.87527380e-01\n",
      "  9.87893113e-04 4.62848688e-02]\n",
      " [1.97476909e-01 1.75045335e-05 3.93993930e-01 ... 7.74042840e-03\n",
      "  1.25361536e-02 1.62853578e-02]\n",
      " [1.54904313e-03 1.43613366e-04 4.86553221e-02 ... 1.58431718e-02\n",
      "  3.40879983e-01 3.24973841e-02]\n",
      " ...\n",
      " [4.69975821e-01 2.50155248e-01 1.58508878e-02 ... 6.30309682e-03\n",
      "  4.34833562e-04 4.51030222e-02]\n",
      " [2.50013571e-02 6.89346416e-04 2.48749789e-01 ... 1.32750667e-01\n",
      "  5.45885793e-02 4.84882889e-02]\n",
      " [2.58490917e-01 2.19968037e-01 2.91890252e-02 ... 1.40988009e-02\n",
      "  7.46027859e-03 2.71258703e-01]] & cached\n",
      "Re-used Cached Value, runNum =  234\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  234\n",
      "Provided input from cache for runNum = 234\n",
      "Provided input from cache for runNum = 235\n",
      "activation = [[5.28744484 5.54845769 6.15483333 ... 9.92958175 4.53351875 3.27514024]\n",
      " [0.         3.29883271 0.         ... 0.         0.         0.        ]\n",
      " [0.20548916 3.95832978 1.24287179 ... 2.04799536 4.58621616 1.78653365]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.52113583 1.40467801 2.00135009 ... 0.5120939  6.00503664 0.        ]\n",
      " [3.75696771 7.10030807 0.         ... 3.02928949 1.90453745 2.00482243]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.13103702 0.         0.51689378 ... 0.         0.64682353 0.        ]\n",
      " [1.65781562 6.35389376 2.14354042 ... 5.36803993 2.05531126 4.66700446]\n",
      " ...\n",
      " [5.50626823 4.33340121 3.71018157 ... 1.9669725  6.03578048 2.89449293]\n",
      " [3.91568194 0.         6.24323676 ... 6.18126993 3.4261725  2.72784449]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.05699417e-05 2.59640210e-05 9.37361235e-03 ... 2.85987979e-01\n",
      "  9.61338332e-04 4.56973822e-02]\n",
      " [1.96359839e-01 1.69851207e-05 3.96471018e-01 ... 7.55914037e-03\n",
      "  1.23001680e-02 1.59945982e-02]\n",
      " [1.53368313e-03 1.41159112e-04 4.86143191e-02 ... 1.57060091e-02\n",
      "  3.40878206e-01 3.21410723e-02]\n",
      " ...\n",
      " [4.71784366e-01 2.49883804e-01 1.57604127e-02 ... 6.22653574e-03\n",
      "  4.27014370e-04 4.49428919e-02]\n",
      " [2.48402293e-02 6.83714502e-04 2.48542916e-01 ... 1.32899920e-01\n",
      "  5.42564262e-02 4.82378853e-02]\n",
      " [2.58448100e-01 2.20382022e-01 2.91012598e-02 ... 1.40573802e-02\n",
      "  7.38506189e-03 2.72748420e-01]] & cached\n",
      "Re-used Cached Value, runNum =  235\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  235\n",
      "Provided input from cache for runNum = 235\n",
      "Provided input from cache for runNum = 236\n",
      "activation = [[5.29232636 5.55067207 6.1577985  ... 9.94563813 4.54034762 3.2779329 ]\n",
      " [0.         3.29920108 0.         ... 0.         0.         0.        ]\n",
      " [0.20534448 3.95798722 1.23996032 ... 2.05152316 4.59433444 1.7881513 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.52754511 1.42024025 2.01236401 ... 0.51780129 6.01736187 0.        ]\n",
      " [3.76739578 7.11564174 0.         ... 3.03515922 1.91171486 2.01309792]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.13701511 0.         0.52868316 ... 0.         0.649468   0.        ]\n",
      " [1.66032034 6.36499257 2.14332345 ... 5.39901357 2.06809968 4.68563213]\n",
      " ...\n",
      " [5.52823492 4.36678372 3.73159416 ... 1.98896389 6.05973523 2.90956681]\n",
      " [3.920897   0.         6.25855261 ... 6.20959658 3.43643059 2.73189844]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.98717028e-06 2.46060620e-05 9.02492114e-03 ... 2.84460229e-01\n",
      "  9.35704437e-04 4.51213307e-02]\n",
      " [1.95268706e-01 1.64856678e-05 3.98941120e-01 ... 7.38231185e-03\n",
      "  1.20710715e-02 1.57103475e-02]\n",
      " [1.51888706e-03 1.38779201e-04 4.85775686e-02 ... 1.55710799e-02\n",
      "  3.40893224e-01 3.17931266e-02]\n",
      " ...\n",
      " [4.73585752e-01 2.49640541e-01 1.56702391e-02 ... 6.15109197e-03\n",
      "  4.19337738e-04 4.47772832e-02]\n",
      " [2.46856949e-02 6.78318627e-04 2.48332821e-01 ... 1.33060647e-01\n",
      "  5.39319962e-02 4.79919096e-02]\n",
      " [2.58374133e-01 2.20799685e-01 2.90097599e-02 ... 1.40138746e-02\n",
      "  7.31013536e-03 2.74209247e-01]] & cached\n",
      "Re-used Cached Value, runNum =  236\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  236\n",
      "Provided input from cache for runNum = 236\n",
      "Provided input from cache for runNum = 237\n",
      "activation = [[5.29720441 5.55287716 6.1607359  ... 9.96161619 4.54710554 3.28071695]\n",
      " [0.         3.2995598  0.         ... 0.         0.         0.        ]\n",
      " [0.2051981  3.95759359 1.23704984 ... 2.05503055 4.60240084 1.78975793]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.53409539 1.4359513  2.02340712 ... 0.52365748 6.02984468 0.        ]\n",
      " [3.77777571 7.1309201  0.         ... 3.04106688 1.91887099 2.0213594 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.14289776 0.         0.540384   ... 0.         0.65213673 0.        ]\n",
      " [1.66284707 6.37598606 2.14308095 ... 5.42985853 2.08078839 4.70424622]\n",
      " ...\n",
      " [5.55022247 4.40025417 3.75300693 ... 2.0111068  6.08375775 2.92467469]\n",
      " [3.92603035 0.         6.27367927 ... 6.23764429 3.44650875 2.73584922]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.43631312e-06 2.33146661e-05 8.68780980e-03 ... 2.82866550e-01\n",
      "  9.10547565e-04 4.45457733e-02]\n",
      " [1.94158034e-01 1.60039599e-05 4.01411297e-01 ... 7.21085127e-03\n",
      "  1.18452570e-02 1.54305657e-02]\n",
      " [1.50419216e-03 1.36439279e-04 4.85368316e-02 ... 1.54378530e-02\n",
      "  3.40876595e-01 3.14456336e-02]\n",
      " ...\n",
      " [4.75387766e-01 2.49414854e-01 1.55836458e-02 ... 6.07908061e-03\n",
      "  4.11784619e-04 4.46128766e-02]\n",
      " [2.45303685e-02 6.72987445e-04 2.48099132e-01 ... 1.33228639e-01\n",
      "  5.36030103e-02 4.77397132e-02]\n",
      " [2.58314952e-01 2.21237218e-01 2.89229512e-02 ... 1.39758806e-02\n",
      "  7.23625722e-03 2.75694596e-01]] & cached\n",
      "Re-used Cached Value, runNum =  237\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  237\n",
      "Provided input from cache for runNum = 237\n",
      "Provided input from cache for runNum = 238\n",
      "activation = [[5.30205896 5.55503726 6.16360573 ... 9.97746613 4.55376661 3.2834792 ]\n",
      " [0.         3.29997354 0.         ... 0.         0.         0.        ]\n",
      " [0.20502797 3.95716346 1.23412167 ... 2.05851747 4.61041771 1.79134659]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.54062831 1.45161177 2.03442437 ... 0.52948903 6.04229781 0.        ]\n",
      " [3.78805173 7.14606539 0.         ... 3.04692314 1.92593426 2.02954911]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.14908745 0.         0.55235515 ... 0.         0.65514473 0.        ]\n",
      " [1.66533222 6.38686606 2.14274347 ... 5.46048329 2.09330208 4.72276398]\n",
      " ...\n",
      " [5.57197642 4.43346553 3.77425093 ... 2.03303766 6.1075428  2.93969213]\n",
      " [3.93113671 0.         6.288626   ... 6.26545193 3.45643424 2.73973836]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.91850786e-06 2.20998992e-05 8.36270155e-03 ... 2.81298714e-01\n",
      "  8.86025587e-04 4.39828426e-02]\n",
      " [1.93051340e-01 1.55353540e-05 4.03868465e-01 ... 7.04243090e-03\n",
      "  1.16253568e-02 1.51547060e-02]\n",
      " [1.49022003e-03 1.34180812e-04 4.85046959e-02 ... 1.53068252e-02\n",
      "  3.40856537e-01 3.11050087e-02]\n",
      " ...\n",
      " [4.77177428e-01 2.49185526e-01 1.54982135e-02 ... 6.00954669e-03\n",
      "  4.04399031e-04 4.44465783e-02]\n",
      " [2.43783525e-02 6.67850693e-04 2.47844289e-01 ... 1.33407946e-01\n",
      "  5.32703831e-02 4.74895607e-02]\n",
      " [2.58250504e-01 2.21642683e-01 2.88358317e-02 ... 1.39382094e-02\n",
      "  7.16367970e-03 2.77162631e-01]] & cached\n",
      "Re-used Cached Value, runNum =  238\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  238\n",
      "Provided input from cache for runNum = 238\n",
      "Provided input from cache for runNum = 239\n",
      "activation = [[5.30691843 5.55722373 6.16644098 ... 9.993261   4.56037078 3.28624978]\n",
      " [0.         3.30040858 0.         ... 0.         0.         0.        ]\n",
      " [0.20482985 3.95665546 1.23118473 ... 2.06197058 4.61836275 1.79290463]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [2.54713432 1.46723401 2.0453974  ... 0.53531282 6.05473747 0.        ]\n",
      " [3.79825245 7.16113788 0.         ... 3.05282781 1.93298585 2.03771678]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.15517394 0.         0.56424271 ... 0.         0.65812856 0.        ]\n",
      " [1.66781887 6.39763759 2.14235439 ... 5.49093369 2.1056869  4.7412051 ]\n",
      " ...\n",
      " [5.59358645 4.46656661 3.79543412 ... 2.0549664  6.13123964 2.95469845]\n",
      " [3.93617597 0.         6.30340085 ... 6.29299271 3.46616699 2.74354105]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.43138073e-06 2.09521179e-05 8.04974858e-03 ... 2.79709861e-01\n",
      "  8.62175379e-04 4.34270680e-02]\n",
      " [1.91921373e-01 1.50823133e-05 4.06298531e-01 ... 6.87798845e-03\n",
      "  1.14097391e-02 1.48832027e-02]\n",
      " [1.47653141e-03 1.31979763e-04 4.84761445e-02 ... 1.51783897e-02\n",
      "  3.40818624e-01 3.07696212e-02]\n",
      " ...\n",
      " [4.78980267e-01 2.48979350e-01 1.54149805e-02 ... 5.94270494e-03\n",
      "  3.97183365e-04 4.42778311e-02]\n",
      " [2.42289801e-02 6.62897901e-04 2.47585644e-01 ... 1.33599597e-01\n",
      "  5.29382256e-02 4.72386215e-02]\n",
      " [2.58186610e-01 2.22054360e-01 2.87501935e-02 ... 1.39032829e-02\n",
      "  7.09257003e-03 2.78628475e-01]] & cached\n",
      "Re-used Cached Value, runNum =  239\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  239\n",
      "Provided input from cache for runNum = 239\n",
      "Provided input from cache for runNum = 240\n",
      "activation = [[ 5.31178859  5.55942121  6.16924364 ... 10.0089925   4.56692544\n",
      "   3.28903021]\n",
      " [ 0.          3.30087968  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20463403  3.95613894  1.22825463 ...  2.06542507  4.62627975\n",
      "   1.79446198]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.55364388  1.48280516  2.0563407  ...  0.54114174  6.0671484\n",
      "   0.        ]\n",
      " [ 3.80838049  7.17612201  0.         ...  3.05873247  1.93998538\n",
      "   2.04584685]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.16112994 0.         0.57604428 ... 0.         0.66103958 0.        ]\n",
      " [1.67035489 6.40842249 2.14193783 ... 5.52130929 2.11804057 4.75962214]\n",
      " ...\n",
      " [5.6151422  4.49964294 3.81659802 ... 2.07699602 6.15491285 2.96975089]\n",
      " [3.94124031 0.         6.31805301 ... 6.32038496 3.47582928 2.74732226]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.97257712e-06 1.98657459e-05 7.74761502e-03 ... 2.78081374e-01\n",
      "  8.39038379e-04 4.28667414e-02]\n",
      " [1.90806332e-01 1.46462001e-05 4.08758927e-01 ... 6.71917921e-03\n",
      "  1.11980388e-02 1.46191109e-02]\n",
      " [1.46314147e-03 1.29831554e-04 4.84465857e-02 ... 1.50519162e-02\n",
      "  3.40767829e-01 3.04125543e-02]\n",
      " ...\n",
      " [4.80748900e-01 2.48752544e-01 1.53327451e-02 ... 5.87771697e-03\n",
      "  3.90086126e-04 4.41385259e-02]\n",
      " [2.40776726e-02 6.57896870e-04 2.47270508e-01 ... 1.33769760e-01\n",
      "  5.26067413e-02 4.70124562e-02]\n",
      " [2.58135159e-01 2.22491233e-01 2.86666337e-02 ... 1.38722834e-02\n",
      "  7.02238810e-03 2.80166534e-01]] & cached\n",
      "Re-used Cached Value, runNum =  240\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  240\n",
      "Provided input from cache for runNum = 240\n",
      "Provided input from cache for runNum = 241\n",
      "activation = [[ 5.31662207  5.56154818  6.1719935  ... 10.02458453  4.57336169\n",
      "   3.29177872]\n",
      " [ 0.          3.30137483  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20448907  3.95567648  1.22537184 ...  2.06898279  4.63423585\n",
      "   1.79604878]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.56019481  1.49850263  2.06727098 ...  0.54711674  6.07963464\n",
      "   0.        ]\n",
      " [ 3.81840995  7.19097495  0.         ...  3.0645998   1.94690194\n",
      "   2.05392298]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.16717511 0.         0.58788274 ... 0.         0.6641274  0.        ]\n",
      " [1.67286471 6.41902682 2.14146462 ... 5.55147895 2.13023572 4.7779676 ]\n",
      " ...\n",
      " [5.63652181 4.53260252 3.83760398 ... 2.09896773 6.17843714 2.98473614]\n",
      " [3.9462282  0.         6.33251157 ... 6.34751332 3.48530176 2.75100588]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.54126821e-06 1.88417654e-05 7.45745569e-03 ... 2.76476378e-01\n",
      "  8.16508332e-04 4.23022180e-02]\n",
      " [1.89691288e-01 1.42247156e-05 4.11193401e-01 ... 6.56354343e-03\n",
      "  1.09912652e-02 1.43615885e-02]\n",
      " [1.45033474e-03 1.27774089e-04 4.84244902e-02 ... 1.49306694e-02\n",
      "  3.40704831e-01 3.00188876e-02]\n",
      " ...\n",
      " [4.82510614e-01 2.48508241e-01 1.52517304e-02 ... 5.81386197e-03\n",
      "  3.83115493e-04 4.40580810e-02]\n",
      " [2.39294837e-02 6.53082772e-04 2.46944501e-01 ... 1.33946863e-01\n",
      "  5.22738369e-02 4.68423228e-02]\n",
      " [2.58077051e-01 2.22901981e-01 2.85827012e-02 ... 1.38412543e-02\n",
      "  6.95312745e-03 2.81814391e-01]] & cached\n",
      "Re-used Cached Value, runNum =  241\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  241\n",
      "Provided input from cache for runNum = 241\n",
      "Provided input from cache for runNum = 242\n",
      "activation = [[ 5.32148605  5.56371157  6.17472719 ... 10.04014948  4.57977296\n",
      "   3.29454071]\n",
      " [ 0.          3.30188312  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20429366  3.95513026  1.22246955 ...  2.07246361  4.64209985\n",
      "   1.79759858]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.56663053  1.51404879  2.078125   ...  0.55295348  6.09196718\n",
      "   0.        ]\n",
      " [ 3.82832691  7.20567339  0.         ...  3.07040752  1.95372031\n",
      "   2.0619291 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.17300633 0.         0.59955802 ... 0.         0.66705401 0.        ]\n",
      " [1.6754955  6.42970717 2.14101893 ... 5.58168621 2.14246693 4.79629959]\n",
      " ...\n",
      " [5.65771762 4.5654141  3.85850268 ... 2.12090534 6.20180516 2.99973389]\n",
      " [3.95132799 0.         6.34688704 ... 6.37465038 3.49478149 2.75473668]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.13607097e-06 1.78761167e-05 7.17849253e-03 ... 2.74844753e-01\n",
      "  7.94921810e-04 4.17445614e-02]\n",
      " [1.88582307e-01 1.38210239e-05 4.13632055e-01 ... 6.41253206e-03\n",
      "  1.07888474e-02 1.41100799e-02]\n",
      " [1.43748190e-03 1.25747143e-04 4.83950401e-02 ... 1.48061725e-02\n",
      "  3.40636751e-01 2.96283437e-02]\n",
      " ...\n",
      " [4.84272163e-01 2.48302036e-01 1.51720157e-02 ... 5.75124004e-03\n",
      "  3.76290928e-04 4.39731852e-02]\n",
      " [2.37823092e-02 6.48343004e-04 2.46592161e-01 ... 1.34098205e-01\n",
      "  5.19504082e-02 4.66697836e-02]\n",
      " [2.58011834e-01 2.23368110e-01 2.84991408e-02 ... 1.38115549e-02\n",
      "  6.88499605e-03 2.83458421e-01]] & cached\n",
      "Re-used Cached Value, runNum =  242\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  242\n",
      "Provided input from cache for runNum = 242\n",
      "Provided input from cache for runNum = 243\n",
      "activation = [[ 5.32635089  5.56589867  6.17742299 ... 10.05566201  4.58614404\n",
      "   3.29730433]\n",
      " [ 0.          3.30240632  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20414111  3.9546214   1.21959432 ...  2.07601526  4.6499876\n",
      "   1.79917389]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.57307837  1.5295857   2.08896206 ...  0.55885625  6.10430493\n",
      "   0.        ]\n",
      " [ 3.83811015  7.22017712  0.         ...  3.07608488  1.96042211\n",
      "   2.0698445 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.17877411 0.         0.61119163 ... 0.         0.66995691 0.        ]\n",
      " [1.67813034 6.44028232 2.14053167 ... 5.61165273 2.15456726 4.81453865]\n",
      " ...\n",
      " [5.67867922 4.59797129 3.87924321 ... 2.14268458 6.224953   3.01462417]\n",
      " [3.95649801 0.         6.36115619 ... 6.40175954 3.50426296 2.75848287]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.75843464e-06 1.69728208e-05 6.91169344e-03 ... 2.73275502e-01\n",
      "  7.74154976e-04 4.12044212e-02]\n",
      " [1.87498491e-01 1.34320877e-05 4.16030905e-01 ... 6.26460702e-03\n",
      "  1.05917038e-02 1.38632252e-02]\n",
      " [1.42570289e-03 1.23833263e-04 4.83789243e-02 ... 1.46871392e-02\n",
      "  3.40576137e-01 2.92490619e-02]\n",
      " ...\n",
      " [4.85993211e-01 2.48053157e-01 1.50902039e-02 ... 5.68723464e-03\n",
      "  3.69563678e-04 4.38791784e-02]\n",
      " [2.36445668e-02 6.43840626e-04 2.46238825e-01 ... 1.34246460e-01\n",
      "  5.16338678e-02 4.65003250e-02]\n",
      " [2.57934566e-01 2.23814052e-01 2.84110159e-02 ... 1.37768996e-02\n",
      "  6.81706883e-03 2.85059195e-01]] & cached\n",
      "Re-used Cached Value, runNum =  243\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  243\n",
      "Provided input from cache for runNum = 243\n",
      "Provided input from cache for runNum = 244\n",
      "activation = [[ 5.331192    5.56807154  6.18005926 ... 10.07106172  4.59243166\n",
      "   3.30005003]\n",
      " [ 0.          3.30286044  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.203927    3.95400584  1.2166804  ...  2.07949754  4.65778443\n",
      "   1.80070546]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.57946518  1.54502512  2.0997599  ...  0.56467079  6.11654701\n",
      "   0.        ]\n",
      " [ 3.84777433  7.23452399  0.         ...  3.08171935  1.96704143\n",
      "   2.07769035]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.18480701 0.         0.62303017 ... 0.         0.67309024 0.        ]\n",
      " [1.68079727 6.4507812  2.14000793 ... 5.64151595 2.16662512 4.8327122 ]\n",
      " ...\n",
      " [5.69951574 4.6304473  3.89994887 ... 2.16446188 6.24798035 3.02953704]\n",
      " [3.96174117 0.         6.375322   ... 6.42875286 3.51371828 2.76224485]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.40067439e-06 1.61144771e-05 6.65296312e-03 ... 2.71678236e-01\n",
      "  7.53917771e-04 4.06684241e-02]\n",
      " [1.86435608e-01 1.30573525e-05 4.18462577e-01 ... 6.12122483e-03\n",
      "  1.03990074e-02 1.36219294e-02]\n",
      " [1.41405258e-03 1.21936374e-04 4.83626333e-02 ... 1.45692773e-02\n",
      "  3.40509806e-01 2.88745148e-02]\n",
      " ...\n",
      " [4.87707337e-01 2.47846553e-01 1.50094794e-02 ... 5.62575624e-03\n",
      "  3.62968148e-04 4.37856331e-02]\n",
      " [2.35052461e-02 6.39372442e-04 2.45843580e-01 ... 1.34395477e-01\n",
      "  5.13170570e-02 4.63309835e-02]\n",
      " [2.57844623e-01 2.24272803e-01 2.83233467e-02 ... 1.37451048e-02\n",
      "  6.75001692e-03 2.86661439e-01]] & cached\n",
      "Re-used Cached Value, runNum =  244\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  244\n",
      "Provided input from cache for runNum = 244\n",
      "Provided input from cache for runNum = 245\n",
      "activation = [[ 5.33603623  5.57025955  6.18267906 ... 10.08640156  4.59867834\n",
      "   3.30280047]\n",
      " [ 0.          3.30329867  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20373114  3.95338152  1.21377813 ...  2.08297779  4.66554933\n",
      "   1.8022288 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.58574691  1.56033478  2.11046018 ...  0.57039907  6.12868504\n",
      "   0.        ]\n",
      " [ 3.85745582  7.24888554  0.         ...  3.08745322  1.97370754\n",
      "   2.08553988]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.19039644 0.         0.63451991 ... 0.         0.67582847 0.        ]\n",
      " [1.68348171 6.4611903  2.13943001 ... 5.67121883 2.17859892 4.85079213]\n",
      " ...\n",
      " [5.72012847 4.66269624 3.92050671 ... 2.1860984  6.27079126 3.04440979]\n",
      " [3.96689266 0.         6.38934723 ... 6.4554728  3.52299087 2.76593137]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.06384760e-06 1.53009352e-05 6.40533044e-03 ... 2.70085610e-01\n",
      "  7.34536002e-04 4.01393400e-02]\n",
      " [1.85289522e-01 1.26910737e-05 4.20819330e-01 ... 5.98039714e-03\n",
      "  1.02097349e-02 1.33841467e-02]\n",
      " [1.40233735e-03 1.20069470e-04 4.83476028e-02 ... 1.44533866e-02\n",
      "  3.40435730e-01 2.85058136e-02]\n",
      " ...\n",
      " [4.89521952e-01 2.47659624e-01 1.49344559e-02 ... 5.56771641e-03\n",
      "  3.56664568e-04 4.36962806e-02]\n",
      " [2.33692965e-02 6.34978034e-04 2.45501672e-01 ... 1.34571737e-01\n",
      "  5.10159297e-02 4.61666631e-02]\n",
      " [2.57723679e-01 2.24691144e-01 2.82371509e-02 ... 1.37156588e-02\n",
      "  6.68538525e-03 2.88265121e-01]] & cached\n",
      "Re-used Cached Value, runNum =  245\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  245\n",
      "Provided input from cache for runNum = 245\n",
      "Provided input from cache for runNum = 246\n",
      "activation = [[ 5.34084946  5.57241531  6.18524003 ... 10.10164034  4.60484091\n",
      "   3.30552467]\n",
      " [ 0.          3.30367507  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20346316  3.95260832  1.2108434  ...  2.08632601  4.67319627\n",
      "   1.80368713]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.59204393  1.57563327  2.12115165 ...  0.5761988   6.1408158\n",
      "   0.        ]\n",
      " [ 3.86711408  7.26321254  0.         ...  3.09325752  1.98036745\n",
      "   2.09338485]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.19615878 0.         0.6461425  ... 0.         0.67874822 0.        ]\n",
      " [1.68610452 6.47142116 2.13874493 ... 5.70070068 2.1904359  4.86877767]\n",
      " ...\n",
      " [5.74070001 4.69500231 3.94107411 ... 2.20790921 6.29361123 3.05933424]\n",
      " [3.97193378 0.         6.40318487 ... 6.48190576 3.53206371 2.76951123]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.74369313e-06 1.45254471e-05 6.16510950e-03 ... 2.68436852e-01\n",
      "  7.15451847e-04 3.96104810e-02]\n",
      " [1.84147825e-01 1.23384334e-05 4.23206182e-01 ... 5.84412970e-03\n",
      "  1.00239960e-02 1.31511712e-02]\n",
      " [1.39070364e-03 1.18227416e-04 4.83350512e-02 ... 1.43417170e-02\n",
      "  3.40339659e-01 2.81405438e-02]\n",
      " ...\n",
      " [4.91330905e-01 2.47509845e-01 1.48608662e-02 ... 5.51288745e-03\n",
      "  3.50445506e-04 4.36106225e-02]\n",
      " [2.32305947e-02 6.30672056e-04 2.45123499e-01 ... 1.34773046e-01\n",
      "  5.07064773e-02 4.60017395e-02]\n",
      " [2.57605994e-01 2.25127120e-01 2.81527539e-02 ... 1.36911883e-02\n",
      "  6.62148227e-03 2.89893998e-01]] & cached\n",
      "Re-used Cached Value, runNum =  246\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  246\n",
      "Provided input from cache for runNum = 246\n",
      "Provided input from cache for runNum = 247\n",
      "activation = [[ 5.34566361  5.57459539  6.18776861 ... 10.11681386  4.61095696\n",
      "   3.30824383]\n",
      " [ 0.          3.30405023  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20314618  3.95176148  1.20789096 ...  2.08961995  4.68074399\n",
      "   1.8051144 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.59822455  1.59075898  2.13175106 ...  0.58187368  6.15280993\n",
      "   0.        ]\n",
      " [ 3.87668697  7.27740551  0.         ...  3.09901107  1.98695635\n",
      "   2.10116719]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.20207452 0.         0.65789236 ... 0.         0.68185388 0.        ]\n",
      " [1.68876421 6.48158239 2.13803003 ... 5.73005677 2.202224   4.88667665]\n",
      " ...\n",
      " [5.7610516  4.7270542  3.96147243 ... 2.22950892 6.31621263 3.07421193]\n",
      " [3.97713246 0.         6.41699557 ... 6.50834828 3.54118786 2.77315531]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.44116958e-06 1.37897311e-05 5.93349979e-03 ... 2.66802883e-01\n",
      "  6.96979253e-04 3.90895339e-02]\n",
      " [1.83013879e-01 1.19963988e-05 4.25605579e-01 ... 5.71124698e-03\n",
      "  9.84287898e-03 1.29232102e-02]\n",
      " [1.37915334e-03 1.16390544e-04 4.83219490e-02 ... 1.42300292e-02\n",
      "  3.40262025e-01 2.77804297e-02]\n",
      " ...\n",
      " [4.93147643e-01 2.47402744e-01 1.47895185e-02 ... 5.46043368e-03\n",
      "  3.44382756e-04 4.35255457e-02]\n",
      " [2.30905926e-02 6.26360314e-04 2.44711274e-01 ... 1.34971884e-01\n",
      "  5.03986401e-02 4.58377614e-02]\n",
      " [2.57471794e-01 2.25563808e-01 2.80685312e-02 ... 1.36673780e-02\n",
      "  6.55863181e-03 2.91515016e-01]] & cached\n",
      "Re-used Cached Value, runNum =  247\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  247\n",
      "Provided input from cache for runNum = 247\n",
      "Provided input from cache for runNum = 248\n",
      "activation = [[ 5.35045604  5.57674573  6.19024787 ... 10.13190015  4.61698895\n",
      "   3.31095339]\n",
      " [ 0.          3.30440472  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20282247  3.95087464  1.20493018 ...  2.09290303  4.68823491\n",
      "   1.80652843]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.60435542  1.60576529  2.14229642 ...  0.58745648  6.16473759\n",
      "   0.        ]\n",
      " [ 3.8862232   7.29154008  0.         ...  3.10480401  1.99353575\n",
      "   2.10892478]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.20787252 0.         0.66956785 ... 0.         0.68492161 0.        ]\n",
      " [1.6914397  6.49169419 2.13726452 ... 5.75927832 2.21392141 4.90450795]\n",
      " ...\n",
      " [5.78124765 4.75893305 3.9817776  ... 2.25100288 6.33869411 3.08906081]\n",
      " [3.98220989 0.         6.43061401 ... 6.53447704 3.55008951 2.77669172]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.15690893e-06 1.30971091e-05 5.71115638e-03 ... 2.65178138e-01\n",
      "  6.79043130e-04 3.85773810e-02]\n",
      " [1.81866031e-01 1.16655570e-05 4.27977514e-01 ... 5.58107582e-03\n",
      "  9.66491960e-03 1.26987711e-02]\n",
      " [1.36807349e-03 1.14619383e-04 4.83159758e-02 ... 1.41204206e-02\n",
      "  3.40160436e-01 2.74266529e-02]\n",
      " ...\n",
      " [4.94944488e-01 2.47277181e-01 1.47184624e-02 ... 5.40931605e-03\n",
      "  3.38442349e-04 4.34345799e-02]\n",
      " [2.29539980e-02 6.22210101e-04 2.44289327e-01 ... 1.35162987e-01\n",
      "  5.00900962e-02 4.56727082e-02]\n",
      " [2.57356082e-01 2.26008067e-01 2.79853387e-02 ... 1.36454032e-02\n",
      "  6.49705858e-03 2.93131307e-01]] & cached\n",
      "Re-used Cached Value, runNum =  248\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  248\n",
      "Provided input from cache for runNum = 248\n",
      "Provided input from cache for runNum = 249\n",
      "activation = [[ 5.35524321  5.57889016  6.19270457 ... 10.1469033   4.62296458\n",
      "   3.31365656]\n",
      " [ 0.          3.30476674  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20248379  3.9499197   1.20196756 ...  2.0961437   4.69565824\n",
      "   1.80792201]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.61048347  1.62077407  2.15281356 ...  0.59305693  6.17666212\n",
      "   0.        ]\n",
      " [ 3.89562536  7.30550762  0.         ...  3.11051722  1.99999553\n",
      "   2.11659748]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.21379841 0.         0.68133266 ... 0.         0.68815184 0.        ]\n",
      " [1.6941558  6.50175553 2.1364815  ... 5.78841768 2.22557111 4.92229511]\n",
      " ...\n",
      " [5.8014091  4.79086872 4.00205841 ... 2.27262149 6.36119619 3.10395629]\n",
      " [3.98741119 0.         6.44418201 ... 6.56060819 3.55905782 2.78026921]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.88886878e-06 1.24412769e-05 5.49637863e-03 ... 2.63529512e-01\n",
      "  6.61512604e-04 3.80713741e-02]\n",
      " [1.80769783e-01 1.13480399e-05 4.30380138e-01 ... 5.45498085e-03\n",
      "  9.49046629e-03 1.24795269e-02]\n",
      " [1.35750347e-03 1.12904724e-04 4.83107777e-02 ... 1.40131324e-02\n",
      "  3.40055378e-01 2.70785570e-02]\n",
      " ...\n",
      " [4.96670545e-01 2.47134953e-01 1.46459929e-02 ... 5.35835363e-03\n",
      "  3.32494815e-04 4.33376346e-02]\n",
      " [2.28193356e-02 6.18154493e-04 2.43820563e-01 ... 1.35343390e-01\n",
      "  4.97766282e-02 4.55061291e-02]\n",
      " [2.57251873e-01 2.26481352e-01 2.79008928e-02 ... 1.36238718e-02\n",
      "  6.43491186e-03 2.94738245e-01]] & cached\n",
      "Re-used Cached Value, runNum =  249\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  249\n",
      "Provided input from cache for runNum = 249\n",
      "Provided input from cache for runNum = 250\n",
      "activation = [[ 5.36006089  5.58107902  6.19514811 ... 10.1618771   4.62892471\n",
      "   3.31637432]\n",
      " [ 0.          3.30516399  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20212951  3.94891372  1.19901701 ...  2.09937232  4.70302825\n",
      "   1.80929817]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.616717    1.63591065  2.16334171 ...  0.59885361  6.18870803\n",
      "   0.        ]\n",
      " [ 3.90497121  7.3193914   0.         ...  3.11623402  2.0063971\n",
      "   2.12424223]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.21957743 0.         0.69291666 ... 0.         0.69134796 0.        ]\n",
      " [1.69673756 6.51154842 2.13558213 ... 5.81721516 2.2369668  4.9399581 ]\n",
      " ...\n",
      " [5.82144908 4.82270656 4.02222132 ... 2.29421408 6.3835831  3.11878053]\n",
      " [3.99265562 0.         6.45766079 ... 6.58662343 3.56802261 2.7838245 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.63629875e-06 1.18200661e-05 5.29036044e-03 ... 2.61888971e-01\n",
      "  6.44487062e-04 3.75737297e-02]\n",
      " [1.79674232e-01 1.10417639e-05 4.32750004e-01 ... 5.33237899e-03\n",
      "  9.32027031e-03 1.22646142e-02]\n",
      " [1.34732370e-03 1.11242218e-04 4.83106234e-02 ... 1.39110590e-02\n",
      "  3.39961685e-01 2.67358711e-02]\n",
      " ...\n",
      " [4.98413726e-01 2.47016472e-01 1.45750686e-02 ... 5.30861769e-03\n",
      "  3.26625994e-04 4.32431930e-02]\n",
      " [2.26909027e-02 6.14308949e-04 2.43369185e-01 ... 1.35550245e-01\n",
      "  4.94671675e-02 4.53434899e-02]\n",
      " [2.57116695e-01 2.26938903e-01 2.78142827e-02 ... 1.36017813e-02\n",
      "  6.37253006e-03 2.96337735e-01]] & cached\n",
      "Re-used Cached Value, runNum =  250\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  250\n",
      "Provided input from cache for runNum = 250\n",
      "Provided input from cache for runNum = 251\n",
      "activation = [[ 5.36486797  5.58324498  6.19755292 ... 10.17674354  4.63480802\n",
      "   3.31907634]\n",
      " [ 0.          3.30561273  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20175234  3.9478596   1.19604955 ...  2.10255867  4.71033869\n",
      "   1.81065421]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.62288847  1.65091456  2.17380906 ...  0.60458398  6.20069529\n",
      "   0.        ]\n",
      " [ 3.91427659  7.33325655  0.         ...  3.12200451  2.01276822\n",
      "   2.13187248]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.22527388 0.         0.70446229 ... 0.         0.69454429 0.        ]\n",
      " [1.69932681 6.52131566 2.13461695 ... 5.84588204 2.24823257 4.95755507]\n",
      " ...\n",
      " [5.8414226  4.85450654 4.04239413 ... 2.31595345 6.40595757 3.13366669]\n",
      " [3.99785658 0.         6.47101003 ... 6.61243457 3.57688706 2.78733942]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.39780613e-06 1.12299782e-05 5.09155909e-03 ... 2.60210234e-01\n",
      "  6.27864259e-04 3.70797282e-02]\n",
      " [1.78576360e-01 1.07439993e-05 4.35127532e-01 ... 5.21357812e-03\n",
      "  9.15309815e-03 1.20536288e-02]\n",
      " [1.33742809e-03 1.09611877e-04 4.83141752e-02 ... 1.38124761e-02\n",
      "  3.39853476e-01 2.63988563e-02]\n",
      " ...\n",
      " [5.00139023e-01 2.46877837e-01 1.45047378e-02 ... 5.26092713e-03\n",
      "  3.20841856e-04 4.31460907e-02]\n",
      " [2.25624947e-02 6.10425479e-04 2.42886978e-01 ... 1.35754537e-01\n",
      "  4.91561170e-02 4.51785300e-02]\n",
      " [2.56992119e-01 2.27390157e-01 2.77289396e-02 ... 1.35838870e-02\n",
      "  6.31062527e-03 2.97940093e-01]] & cached\n",
      "Re-used Cached Value, runNum =  251\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  251\n",
      "Provided input from cache for runNum = 251\n",
      "Provided input from cache for runNum = 252\n",
      "activation = [[ 5.3697052   5.58545563  6.19994781 ... 10.19160494  4.64066756\n",
      "   3.32180247]\n",
      " [ 0.          3.30603347  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20135783  3.94674577  1.1930707  ...  2.10570784  4.71760153\n",
      "   1.811981  ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.629054    1.66584761  2.18423754 ...  0.61027508  6.21265773\n",
      "   0.        ]\n",
      " [ 3.92353085  7.34702537  0.         ...  3.1277367   2.0191031\n",
      "   2.13944832]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.23103146 0.         0.71606399 ... 0.         0.69783187 0.        ]\n",
      " [1.70198116 6.53105691 2.13367228 ... 5.87449828 2.25946314 4.975111  ]\n",
      " ...\n",
      " [5.86136227 4.88630381 4.06260974 ... 2.33785717 6.42833334 3.14860915]\n",
      " [4.00311587 0.         6.48427426 ... 6.63818112 3.58570393 2.79088084]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.17310404e-06 1.06724072e-05 4.90005468e-03 ... 2.58511959e-01\n",
      "  6.11666572e-04 3.65935985e-02]\n",
      " [1.77492369e-01 1.04570406e-05 4.37485009e-01 ... 5.09733981e-03\n",
      "  8.98885548e-03 1.18467435e-02]\n",
      " [1.32793261e-03 1.08027791e-04 4.83203758e-02 ... 1.37149351e-02\n",
      "  3.39726725e-01 2.60668403e-02]\n",
      " ...\n",
      " [5.01835783e-01 2.46755889e-01 1.44335787e-02 ... 5.21339453e-03\n",
      "  3.15099523e-04 4.30449920e-02]\n",
      " [2.24388995e-02 6.06728635e-04 2.42401495e-01 ... 1.35966176e-01\n",
      "  4.88426796e-02 4.50171387e-02]\n",
      " [2.56873135e-01 2.27869439e-01 2.76427224e-02 ... 1.35655703e-02\n",
      "  6.24890468e-03 2.99529519e-01]] & cached\n",
      "Re-used Cached Value, runNum =  252\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  252\n",
      "Provided input from cache for runNum = 252\n",
      "Provided input from cache for runNum = 253\n",
      "activation = [[ 5.37447633  5.58754757  6.20227783 ... 10.20628285  4.64638173\n",
      "   3.32447279]\n",
      " [ 0.          3.30649305  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20103194  3.94573078  1.19013846 ...  2.10898578  4.72493395\n",
      "   1.81336261]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.63535331  1.68098744  2.19476439 ...  0.61624875  6.22482517\n",
      "   0.        ]\n",
      " [ 3.93274306  7.36074301  0.         ...  3.13351267  2.02540493\n",
      "   2.14700472]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.23689813 0.         0.72777563 ... 0.         0.70129957 0.        ]\n",
      " [1.70463213 6.54069219 2.13266476 ... 5.90284759 2.27052926 4.99262558]\n",
      " ...\n",
      " [5.88115477 4.917967   4.08273009 ... 2.35974256 6.45061026 3.16343282]\n",
      " [4.00823244 0.         6.49734878 ... 6.66356171 3.59428021 2.79425575]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.96257031e-06 1.01492049e-05 4.71669821e-03 ... 2.56870637e-01\n",
      "  5.95851476e-04 3.61218605e-02]\n",
      " [1.76422357e-01 1.01774742e-05 4.39821870e-01 ... 4.98347461e-03\n",
      "  8.82827117e-03 1.16417760e-02]\n",
      " [1.31948524e-03 1.06555894e-04 4.83467682e-02 ... 1.36263777e-02\n",
      "  3.39576059e-01 2.57434977e-02]\n",
      " ...\n",
      " [5.03475739e-01 2.46517838e-01 1.43599272e-02 ... 5.16560689e-03\n",
      "  3.09433782e-04 4.29365382e-02]\n",
      " [2.23181809e-02 6.03083018e-04 2.41880208e-01 ... 1.36163285e-01\n",
      "  4.85240370e-02 4.48502071e-02]\n",
      " [2.56772217e-01 2.28285795e-01 2.75544993e-02 ... 1.35469433e-02\n",
      "  6.18785330e-03 3.01095483e-01]] & cached\n",
      "Re-used Cached Value, runNum =  253\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  253\n",
      "Provided input from cache for runNum = 253\n",
      "iterations = 250\n",
      "Accuracy = 0.5997317073170731\n",
      "Provided input from cache for runNum = 254\n",
      "activation = [[ 5.37926253  5.58967578  6.20458629 ... 10.22092025  4.65205347\n",
      "   3.32714834]\n",
      " [ 0.          3.30693518  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20070384  3.94467351  1.18721585 ...  2.11223318  4.73221444\n",
      "   1.81472772]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.64145898  1.69581175  2.20514953 ...  0.62196133  6.23676321\n",
      "   0.        ]\n",
      " [ 3.94196056  7.37446116  0.         ...  3.13937072  2.03174583\n",
      "   2.15457336]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.24248238 0.         0.73931065 ... 0.         0.70459543 0.        ]\n",
      " [1.70740043 6.55040701 2.13167395 ... 5.93121728 2.28164348 5.01008754]\n",
      " ...\n",
      " [5.90058541 4.94919271 4.10260075 ... 2.38130767 6.47251902 3.17817799]\n",
      " [4.01326254 0.         6.51027431 ... 6.68867234 3.60267309 2.79755697]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.76349682e-06 9.65396812e-06 4.54105829e-03 ... 2.55240647e-01\n",
      "  5.80823875e-04 3.56560695e-02]\n",
      " [1.75284050e-01 9.90540139e-06 4.42130900e-01 ... 4.87155078e-03\n",
      "  8.67158510e-03 1.14404109e-02]\n",
      " [1.31032501e-03 1.05057390e-04 4.83620070e-02 ... 1.35324054e-02\n",
      "  3.39413789e-01 2.54206124e-02]\n",
      " ...\n",
      " [5.05224708e-01 2.46387607e-01 1.42916943e-02 ... 5.12087503e-03\n",
      "  3.04064877e-04 4.28337852e-02]\n",
      " [2.21945166e-02 5.99444051e-04 2.41365262e-01 ... 1.36344776e-01\n",
      "  4.82212803e-02 4.46836996e-02]\n",
      " [2.56639295e-01 2.28720159e-01 2.74690363e-02 ... 1.35309954e-02\n",
      "  6.13003872e-03 3.02671532e-01]] & cached\n",
      "Re-used Cached Value, runNum =  254\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  254\n",
      "Provided input from cache for runNum = 254\n",
      "Provided input from cache for runNum = 255\n",
      "activation = [[ 5.38399271  5.59174211  6.20684016 ... 10.23542839  4.65761498\n",
      "   3.32978536]\n",
      " [ 0.          3.30736879  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.20034298  3.94355417  1.18426998 ...  2.11544215  4.73942263\n",
      "   1.8160809 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.64755624  1.71057887  2.21548351 ...  0.62766194  6.24867236\n",
      "   0.        ]\n",
      " [ 3.95113228  7.38813921  0.         ...  3.14530065  2.03808517\n",
      "   2.16213106]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.24814774 0.         0.75090957 ... 0.         0.70803367 0.        ]\n",
      " [1.71011561 6.55999871 2.13060517 ... 5.95944058 2.29263926 5.02747489]\n",
      " ...\n",
      " [5.91992759 4.98034452 4.12241634 ... 2.40292573 6.49436967 3.19293129]\n",
      " [4.01821523 0.         6.52305113 ... 6.71349466 3.61087683 2.80075561]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.57420218e-06 9.18137538e-06 4.37121680e-03 ... 2.53584260e-01\n",
      "  5.66085767e-04 3.51933883e-02]\n",
      " [1.74131420e-01 9.63982778e-06 4.44438897e-01 ... 4.76254775e-03\n",
      "  8.51794105e-03 1.12422813e-02]\n",
      " [1.30125153e-03 1.03574802e-04 4.83793770e-02 ... 1.34421640e-02\n",
      "  3.39244073e-01 2.51023460e-02]\n",
      " ...\n",
      " [5.06999178e-01 2.46275143e-01 1.42268304e-02 ... 5.07975470e-03\n",
      "  2.98844468e-04 4.27352550e-02]\n",
      " [2.20703694e-02 5.95812638e-04 2.40844766e-01 ... 1.36553748e-01\n",
      "  4.79175166e-02 4.45182834e-02]\n",
      " [2.56491194e-01 2.29124040e-01 2.73853326e-02 ... 1.35201162e-02\n",
      "  6.07352044e-03 3.04260847e-01]] & cached\n",
      "Re-used Cached Value, runNum =  255\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  255\n",
      "Provided input from cache for runNum = 255\n",
      "Provided input from cache for runNum = 256\n",
      "activation = [[ 5.38872241  5.59379584  6.20907019 ... 10.24986402  4.66310488\n",
      "   3.3324083 ]\n",
      " [ 0.          3.30783412  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19997117  3.94240364  1.18132079 ...  2.11863158  4.74658831\n",
      "   1.81742498]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.6535555   1.72517364  2.2257243  ...  0.6332337   6.26051204\n",
      "   0.        ]\n",
      " [ 3.96023792  7.40171     0.         ...  3.1512052   2.04435218\n",
      "   2.1696405 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.25371008 0.         0.7624071  ... 0.         0.71143651 0.        ]\n",
      " [1.71285899 6.56958113 2.12951456 ... 5.98755527 2.30356076 5.04478892]\n",
      " ...\n",
      " [5.93898576 5.01115533 4.1420152  ... 2.42427786 6.51597964 3.20759264]\n",
      " [4.02317787 0.         6.53573105 ... 6.73817343 3.61902042 2.80392581]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.39609906e-06 8.73611197e-06 4.20852981e-03 ... 2.51959146e-01\n",
      "  5.51922710e-04 3.47401862e-02]\n",
      " [1.72983349e-01 9.38313435e-06 4.46733615e-01 ... 4.65589020e-03\n",
      "  8.36827348e-03 1.10480048e-02]\n",
      " [1.29231778e-03 1.02122251e-04 4.83953557e-02 ... 1.33511281e-02\n",
      "  3.39077239e-01 2.47885238e-02]\n",
      " ...\n",
      " [5.08766434e-01 2.46177745e-01 1.41631032e-02 ... 5.03964856e-03\n",
      "  2.93753025e-04 4.26339927e-02]\n",
      " [2.19477104e-02 5.92238390e-04 2.40312312e-01 ... 1.36745080e-01\n",
      "  4.76200862e-02 4.43524154e-02]\n",
      " [2.56340534e-01 2.29546213e-01 2.73012671e-02 ... 1.35091921e-02\n",
      "  6.01817423e-03 3.05839780e-01]] & cached\n",
      "Re-used Cached Value, runNum =  256\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  256\n",
      "Provided input from cache for runNum = 256\n",
      "Provided input from cache for runNum = 257\n",
      "activation = [[ 5.39346974  5.59587096  6.21129968 ... 10.26425522  4.66855194\n",
      "   3.33504026]\n",
      " [ 0.          3.30830618  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19955709  3.94119981  1.17834927 ...  2.12177241  4.7536976\n",
      "   1.81874016]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.65952426  1.73970115  2.23591875 ...  0.63873192  6.27230637\n",
      "   0.        ]\n",
      " [ 3.96931812  7.41528258  0.         ...  3.15719522  2.05065341\n",
      "   2.17714567]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.25910594 0.         0.77374019 ... 0.         0.71466415 0.        ]\n",
      " [1.71556379 6.57905347 2.12834058 ... 6.01549084 2.31435252 5.06201655]\n",
      " ...\n",
      " [5.9578674  5.04174936 4.16148007 ... 2.4454605  6.53740077 3.22219378]\n",
      " [4.02812339 0.         6.54831903 ... 6.76261557 3.62700474 2.80703654]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.22791523e-06 8.31479723e-06 4.05221256e-03 ... 2.50334159e-01\n",
      "  5.38257963e-04 3.42938945e-02]\n",
      " [1.71833035e-01 9.13493136e-06 4.49013173e-01 ... 4.55184456e-03\n",
      "  8.22188518e-03 1.08574308e-02]\n",
      " [1.28348917e-03 1.00701507e-04 4.84095605e-02 ... 1.32600524e-02\n",
      "  3.38905777e-01 2.44787351e-02]\n",
      " ...\n",
      " [5.10542788e-01 2.46107167e-01 1.41008491e-02 ... 5.00163429e-03\n",
      "  2.88819373e-04 4.25332947e-02]\n",
      " [2.18282859e-02 5.88800925e-04 2.39789648e-01 ... 1.36945604e-01\n",
      "  4.73297127e-02 4.41888422e-02]\n",
      " [2.56177020e-01 2.29974668e-01 2.72159114e-02 ... 1.35000190e-02\n",
      "  5.96420155e-03 3.07417016e-01]] & cached\n",
      "Re-used Cached Value, runNum =  257\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  257\n",
      "Provided input from cache for runNum = 257\n",
      "Provided input from cache for runNum = 258\n",
      "activation = [[ 5.39822651  5.59794464  6.21351692 ... 10.27860182  4.67393912\n",
      "   3.33767155]\n",
      " [ 0.          3.30877256  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19916521  3.94001828  1.17540286 ...  2.12494143  4.76079147\n",
      "   1.8200716 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.66550531  1.75425513  2.24608283 ...  0.64428479  6.28413125\n",
      "   0.        ]\n",
      " [ 3.97834761  7.42880099  0.         ...  3.16318594  2.05693879\n",
      "   2.18462197]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.26417812 0.         0.78479359 ... 0.         0.71770778 0.        ]\n",
      " [1.71836197 6.58854232 2.12718428 ... 6.04336459 2.32512455 5.07923137]\n",
      " ...\n",
      " [5.97668554 5.07232738 4.1808745  ... 2.46670085 6.55880814 3.23679835]\n",
      " [4.03308192 0.         6.56082344 ... 6.78695724 3.6349258  2.81012775]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.06992038e-06 7.91706598e-06 3.90255800e-03 ... 2.48709537e-01\n",
      "  5.25027586e-04 3.38551380e-02]\n",
      " [1.70702239e-01 8.89642658e-06 4.51284296e-01 ... 4.45068936e-03\n",
      "  8.07795522e-03 1.06707135e-02]\n",
      " [1.27508299e-03 9.93422694e-05 4.84268472e-02 ... 1.31717229e-02\n",
      "  3.38715538e-01 2.41744065e-02]\n",
      " ...\n",
      " [5.12265803e-01 2.45987576e-01 1.40373206e-02 ... 4.96294061e-03\n",
      "  2.83924715e-04 4.24238162e-02]\n",
      " [2.17116028e-02 5.85425202e-04 2.39242343e-01 ... 1.37118585e-01\n",
      "  4.70402002e-02 4.40207392e-02]\n",
      " [2.56033686e-01 2.30431277e-01 2.71296252e-02 ... 1.34906986e-02\n",
      "  5.91045455e-03 3.08980058e-01]] & cached\n",
      "Re-used Cached Value, runNum =  258\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  258\n",
      "Provided input from cache for runNum = 258\n",
      "Provided input from cache for runNum = 259\n",
      "activation = [[ 5.40300705  5.60005585  6.21572259 ... 10.29292107  4.67929381\n",
      "   3.34031998]\n",
      " [ 0.          3.30923999  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19875947  3.93880948  1.17244981 ...  2.12809255  4.76783949\n",
      "   1.82139069]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.67135803  1.7686441   2.25613558 ...  0.64969626  6.29583099\n",
      "   0.        ]\n",
      " [ 3.98727109  7.44218238  0.         ...  3.16913129  2.0631314\n",
      "   2.19203589]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.26897578 0.         0.79562159 ... 0.         0.72056219 0.        ]\n",
      " [1.72113836 6.59792626 2.12597312 ... 6.07107305 2.33578459 5.09633969]\n",
      " ...\n",
      " [5.99519157 5.10255255 4.20002637 ... 2.48762577 6.57990931 3.25129542]\n",
      " [4.03818764 0.         6.57329738 ... 6.811304   3.64290841 2.81329369]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.92153221e-06 7.54168639e-06 3.75937011e-03 ... 2.47124492e-01\n",
      "  5.12415174e-04 3.34273754e-02]\n",
      " [1.69589442e-01 8.66692669e-06 4.53546230e-01 ... 4.35216778e-03\n",
      "  7.93817272e-03 1.04886812e-02]\n",
      " [1.26685636e-03 9.80108043e-05 4.84420654e-02 ... 1.30828020e-02\n",
      "  3.38544852e-01 2.38760203e-02]\n",
      " ...\n",
      " [5.13998113e-01 2.45896309e-01 1.39747941e-02 ... 4.92484946e-03\n",
      "  2.79156416e-04 4.23118166e-02]\n",
      " [2.15987441e-02 5.82118837e-04 2.38691488e-01 ... 1.37278550e-01\n",
      "  4.67629465e-02 4.38555558e-02]\n",
      " [2.55855118e-01 2.30892989e-01 2.70402839e-02 ... 1.34793397e-02\n",
      "  5.85731286e-03 3.10511802e-01]] & cached\n",
      "Re-used Cached Value, runNum =  259\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  259\n",
      "Provided input from cache for runNum = 259\n",
      "Provided input from cache for runNum = 260\n",
      "activation = [[ 5.40782312  5.60222552  6.21792344 ... 10.30724083  4.68463593\n",
      "   3.3429925 ]\n",
      " [ 0.          3.30969085  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19834551  3.93757152  1.16950016 ...  2.1312228   4.77483282\n",
      "   1.82269914]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.67728653  1.78314906  2.266205   ...  0.65528143  6.30763092\n",
      "   0.        ]\n",
      " [ 3.99610752  7.45544373  0.         ...  3.17504348  2.06925677\n",
      "   2.19939921]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27346245 0.         0.80618595 ... 0.         0.72320317 0.        ]\n",
      " [1.72380082 6.60708759 2.12462774 ... 6.09847028 2.34621377 5.11334392]\n",
      " ...\n",
      " [6.01365567 5.13280025 4.21913656 ... 2.50866922 6.60101233 3.26576782]\n",
      " [4.04342702 0.         6.58573148 ... 6.83567329 3.65095903 2.81649463]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.78183902e-06 7.18596046e-06 3.62176357e-03 ... 2.45525813e-01\n",
      "  5.00138638e-04 3.30071697e-02]\n",
      " [1.68511380e-01 8.44709990e-06 4.55793109e-01 ... 4.25704015e-03\n",
      "  7.80141325e-03 1.03109682e-02]\n",
      " [1.25917231e-03 9.67416105e-05 4.84614803e-02 ... 1.29991741e-02\n",
      "  3.38380107e-01 2.35837476e-02]\n",
      " ...\n",
      " [5.15697792e-01 2.45785532e-01 1.39113212e-02 ... 4.88653737e-03\n",
      "  2.74412189e-04 4.21965426e-02]\n",
      " [2.14939709e-02 5.78998239e-04 2.38156501e-01 ... 1.37459394e-01\n",
      "  4.64899784e-02 4.36937899e-02]\n",
      " [2.55658165e-01 2.31363049e-01 2.69467137e-02 ... 1.34664831e-02\n",
      "  5.80339597e-03 3.12023874e-01]] & cached\n",
      "Re-used Cached Value, runNum =  260\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  260\n",
      "Provided input from cache for runNum = 260\n",
      "Provided input from cache for runNum = 261\n",
      "activation = [[ 5.41260265  5.60431857  6.22008716 ... 10.32142897  4.68987881\n",
      "   3.34562017]\n",
      " [ 0.          3.31014421  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19790695  3.93630045  1.16652856 ...  2.13431026  4.78176335\n",
      "   1.82399795]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.68319262  1.79754856  2.27623672 ...  0.66082668  6.31938314\n",
      "   0.        ]\n",
      " [ 4.00492437  7.468695    0.         ...  3.18104693  2.07539388\n",
      "   2.20677284]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27782398 0.         0.81666055 ... 0.         0.72580948 0.        ]\n",
      " [1.72647471 6.61622502 2.12324995 ... 6.12577309 2.35659988 5.13029415]\n",
      " ...\n",
      " [6.03201733 5.16293365 4.23819176 ... 2.52972931 6.62205826 3.28024211]\n",
      " [4.0485567  0.         6.59801969 ... 6.85972304 3.65881834 2.81957804]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.64895693e-06 6.84619557e-06 3.48903386e-03 ... 2.43898495e-01\n",
      "  4.88147873e-04 3.25886646e-02]\n",
      " [1.67405506e-01 8.23195321e-06 4.58038915e-01 ... 4.16458455e-03\n",
      "  7.66660147e-03 1.01360236e-02]\n",
      " [1.25145112e-03 9.54798145e-05 4.84815113e-02 ... 1.29176077e-02\n",
      "  3.38192036e-01 2.32946396e-02]\n",
      " ...\n",
      " [5.17425008e-01 2.45663925e-01 1.38512670e-02 ... 4.85144260e-03\n",
      "  2.69817459e-04 4.20843936e-02]\n",
      " [2.13853550e-02 5.75726016e-04 2.37601442e-01 ... 1.37639172e-01\n",
      "  4.62164649e-02 4.35276592e-02]\n",
      " [2.55458194e-01 2.31805875e-01 2.68566408e-02 ... 1.34596362e-02\n",
      "  5.75099843e-03 3.13554585e-01]] & cached\n",
      "Re-used Cached Value, runNum =  261\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  261\n",
      "Provided input from cache for runNum = 261\n",
      "Provided input from cache for runNum = 262\n",
      "activation = [[ 5.41737286  5.60638208  6.22221441 ... 10.33551951  4.69505076\n",
      "   3.34822752]\n",
      " [ 0.          3.31063354  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19748752  3.93505042  1.16356722 ...  2.13745389  4.78869559\n",
      "   1.82532187]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.68912351  1.81197336  2.28627534 ...  0.66645496  6.33122976\n",
      "   0.        ]\n",
      " [ 4.01367823  7.48187725  0.         ...  3.18706242  2.08149629\n",
      "   2.21412846]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28219137 0.         0.82715483 ... 0.         0.72851323 0.        ]\n",
      " [1.7291383  6.62525848 2.12182058 ... 6.15286552 2.3668504  5.14718623]\n",
      " ...\n",
      " [6.05025494 5.19292897 4.25719233 ... 2.5507516  6.64305178 3.29466749]\n",
      " [4.05370885 0.         6.6102123  ... 6.88358523 3.66659711 2.82260646]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.52379738e-06 6.52577477e-06 3.36147424e-03 ... 2.42303064e-01\n",
      "  4.76427949e-04 3.21800449e-02]\n",
      " [1.66322628e-01 8.02347930e-06 4.60268310e-01 ... 4.07402688e-03\n",
      "  7.53468647e-03 9.96378342e-03]\n",
      " [1.24434382e-03 9.42821427e-05 4.85118686e-02 ... 1.28410092e-02\n",
      "  3.37990390e-01 2.30125254e-02]\n",
      " ...\n",
      " [5.19111834e-01 2.45505366e-01 1.37896571e-02 ... 4.81612312e-03\n",
      "  2.65254337e-04 4.19667853e-02]\n",
      " [2.12809169e-02 5.72577360e-04 2.37035835e-01 ... 1.37819155e-01\n",
      "  4.59399190e-02 4.33615034e-02]\n",
      " [2.55261983e-01 2.32234591e-01 2.67643379e-02 ... 1.34514953e-02\n",
      "  5.69861826e-03 3.15063570e-01]] & cached\n",
      "Re-used Cached Value, runNum =  262\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  262\n",
      "Provided input from cache for runNum = 262\n",
      "Provided input from cache for runNum = 263\n",
      "activation = [[ 5.42214859  5.60844511  6.22432047 ... 10.34953518  4.70015301\n",
      "   3.3508299 ]\n",
      " [ 0.          3.31110182  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1970897   3.93381289  1.16063378 ...  2.14061623  4.79561389\n",
      "   1.82665755]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.69495615  1.82624597  2.2962317  ...  0.67198881  6.34297931\n",
      "   0.        ]\n",
      " [ 4.02235529  7.49494668  0.         ...  3.19303814  2.08753569\n",
      "   2.22143368]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28610698 0.         0.83727586 ... 0.         0.7308913  0.        ]\n",
      " [1.73184729 6.63428244 2.12038182 ... 6.17985726 2.37705391 5.1640132 ]\n",
      " ...\n",
      " [6.06818169 5.22257501 4.27595973 ... 2.57150996 6.66377662 3.30898324]\n",
      " [4.05884441 0.         6.62227112 ... 6.90727108 3.67426497 2.82559592]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.40625652e-06 6.22412505e-06 3.23992840e-03 ... 2.40742416e-01\n",
      "  4.65274450e-04 3.17808995e-02]\n",
      " [1.65224080e-01 7.82249038e-06 4.62460153e-01 ... 3.98539207e-03\n",
      "  7.40593230e-03 9.79496020e-03]\n",
      " [1.23720919e-03 9.31138265e-05 4.85400921e-02 ... 1.27633599e-02\n",
      "  3.37775626e-01 2.27341466e-02]\n",
      " ...\n",
      " [5.20821177e-01 2.45366779e-01 1.37297659e-02 ... 4.78102493e-03\n",
      "  2.60843277e-04 4.18453557e-02]\n",
      " [2.11783975e-02 5.69495657e-04 2.36475038e-01 ... 1.37969636e-01\n",
      "  4.56755169e-02 4.31934264e-02]\n",
      " [2.55053434e-01 2.32687503e-01 2.66723471e-02 ... 1.34426450e-02\n",
      "  5.64783925e-03 3.16556655e-01]] & cached\n",
      "Re-used Cached Value, runNum =  263\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  263\n",
      "Provided input from cache for runNum = 263\n",
      "Provided input from cache for runNum = 264\n",
      "activation = [[ 5.42691124  5.61051266  6.22639872 ... 10.36348288  4.70519402\n",
      "   3.3534207 ]\n",
      " [ 0.          3.31152046  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19668917  3.93253823  1.15772067 ...  2.14376479  4.80248266\n",
      "   1.82798961]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.70071879  1.8403879   2.30612749 ...  0.6774668   6.35464575\n",
      "   0.        ]\n",
      " [ 4.03098958  7.50791778  0.         ...  3.19898935  2.09350851\n",
      "   2.2286972 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28982893 0.         0.84721869 ... 0.         0.73314155 0.        ]\n",
      " [1.73453098 6.64319372 2.11887178 ... 6.20665431 2.38713098 5.1807436 ]\n",
      " ...\n",
      " [6.08593135 5.25204826 4.29461072 ... 2.59219467 6.68433617 3.32326526]\n",
      " [4.06395055 0.         6.6342131  ... 6.93080997 3.68183933 2.8285463 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.29485166e-06 5.93768254e-06 3.12292521e-03 ... 2.39174519e-01\n",
      "  4.54506181e-04 3.13869113e-02]\n",
      " [1.64145178e-01 7.63045079e-06 4.64679398e-01 ... 3.89992203e-03\n",
      "  7.28036764e-03 9.63045218e-03]\n",
      " [1.23005587e-03 9.19637073e-05 4.85655706e-02 ... 1.26867108e-02\n",
      "  3.37550399e-01 2.24596241e-02]\n",
      " ...\n",
      " [5.22516493e-01 2.45247149e-01 1.36696339e-02 ... 4.74638292e-03\n",
      "  2.56516506e-04 4.17206886e-02]\n",
      " [2.10745815e-02 5.66413605e-04 2.35880851e-01 ... 1.38100865e-01\n",
      "  4.54166053e-02 4.30231429e-02]\n",
      " [2.54838775e-01 2.33174612e-01 2.65794588e-02 ... 1.34348750e-02\n",
      "  5.59769221e-03 3.18044247e-01]] & cached\n",
      "Re-used Cached Value, runNum =  264\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  264\n",
      "Provided input from cache for runNum = 264\n",
      "Provided input from cache for runNum = 265\n",
      "activation = [[ 5.43165174  5.6125429   6.22844581 ... 10.37733094  4.71015819\n",
      "   3.35599006]\n",
      " [ 0.          3.3119278   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19627502  3.93126102  1.15480414 ...  2.14693688  4.809322\n",
      "   1.82932356]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.70655527  1.85457325  2.31604534 ...  0.68306319  6.36633478\n",
      "   0.        ]\n",
      " [ 4.03956128  7.52082476  0.         ...  3.20494373  2.09941336\n",
      "   2.23592349]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29349645 0.         0.85708388 ... 0.         0.73537207 0.        ]\n",
      " [1.73710487 6.651926   2.11724283 ... 6.23313826 2.39701032 5.19736731]\n",
      " ...\n",
      " [6.10363047 5.28148336 4.31323782 ... 2.61294387 6.70482929 3.33751722]\n",
      " [4.0690652  0.         6.64606112 ... 6.95417002 3.68937184 2.83146292]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.18977863e-06 5.66676764e-06 3.01049268e-03 ... 2.37625764e-01\n",
      "  4.44002348e-04 3.10022871e-02]\n",
      " [1.63092381e-01 7.44422210e-06 4.66863669e-01 ... 3.81654980e-03\n",
      "  7.15772568e-03 9.46887076e-03]\n",
      " [1.22367262e-03 9.08848625e-05 4.86048478e-02 ... 1.26173315e-02\n",
      "  3.37323491e-01 2.21935914e-02]\n",
      " ...\n",
      " [5.24179470e-01 2.45071254e-01 1.36076332e-02 ... 4.71157226e-03\n",
      "  2.52236277e-04 4.15918556e-02]\n",
      " [2.09784499e-02 5.63484710e-04 2.35307265e-01 ... 1.38257390e-01\n",
      "  4.51607537e-02 4.28573990e-02]\n",
      " [2.54610102e-01 2.33620235e-01 2.64820230e-02 ... 1.34255393e-02\n",
      "  5.54738672e-03 3.19500156e-01]] & cached\n",
      "Re-used Cached Value, runNum =  265\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  265\n",
      "Provided input from cache for runNum = 265\n",
      "Provided input from cache for runNum = 266\n",
      "activation = [[ 5.43641427  5.61461099  6.23048075 ... 10.391127    4.71511095\n",
      "   3.35856441]\n",
      " [ 0.          3.31231217  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19582034  3.92990719  1.15189166 ...  2.15004946  4.81606367\n",
      "   1.83062695]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.7122257   1.86853606  2.32583143 ...  0.68846285  6.37782476\n",
      "   0.        ]\n",
      " [ 4.04810289  7.53370037  0.         ...  3.21096058  2.10530155\n",
      "   2.24314453]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29675815 0.         0.86661937 ... 0.         0.73734645 0.        ]\n",
      " [1.73982367 6.66075408 2.11570107 ... 6.25967756 2.40701135 5.21395766]\n",
      " ...\n",
      " [6.12103132 5.31059205 4.33165876 ... 2.63347217 6.72505869 3.35170948]\n",
      " [4.07422397 0.         6.65782623 ... 6.97734921 3.69688228 2.83436868]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.09008769e-06 5.40900403e-06 2.90284666e-03 ... 2.36069081e-01\n",
      "  4.34019328e-04 3.06217396e-02]\n",
      " [1.61986368e-01 7.26270234e-06 4.69015895e-01 ... 3.73491619e-03\n",
      "  7.03754316e-03 9.31034833e-03]\n",
      " [1.21664431e-03 8.97834015e-05 4.86321498e-02 ... 1.25436878e-02\n",
      "  3.37090883e-01 2.19281438e-02]\n",
      " ...\n",
      " [5.25935414e-01 2.44977384e-01 1.35511443e-02 ... 4.67951372e-03\n",
      "  2.48154518e-04 4.14679455e-02]\n",
      " [2.08794716e-02 5.60515341e-04 2.34740741e-01 ... 1.38397361e-01\n",
      "  4.49185662e-02 4.26914584e-02]\n",
      " [2.54349072e-01 2.34088055e-01 2.63888058e-02 ... 1.34197689e-02\n",
      "  5.49934794e-03 3.20964042e-01]] & cached\n",
      "Re-used Cached Value, runNum =  266\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  266\n",
      "Provided input from cache for runNum = 266\n",
      "Provided input from cache for runNum = 267\n",
      "activation = [[ 5.44114836  5.616625    6.2324788  ... 10.40480433  4.71996791\n",
      "   3.36110253]\n",
      " [ 0.          3.31274185  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19534373  3.92850798  1.14898029 ...  2.1531182   4.82274074\n",
      "   1.83191958]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.7178429   1.88238072  2.3355517  ...  0.69383558  6.38925106\n",
      "   0.        ]\n",
      " [ 4.05657951  7.54649104  0.         ...  3.21695096  2.11114872\n",
      "   2.25032445]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3000373  0.         0.87616331 ... 0.         0.7393999  0.        ]\n",
      " [1.74258944 6.66957596 2.11416087 ... 6.28609673 2.41697415 5.23049089]\n",
      " ...\n",
      " [6.1383061  5.33953563 4.34998814 ... 2.65398731 6.74519035 3.36588547]\n",
      " [4.07934541 0.         6.6694616  ... 7.00033048 3.7042584  2.83720809]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.99533386e-06 5.16406849e-06 2.79904740e-03 ... 2.34505836e-01\n",
      "  4.24279275e-04 3.02462235e-02]\n",
      " [1.60890248e-01 7.08669020e-06 4.71174248e-01 ... 3.65558979e-03\n",
      "  6.91973267e-03 9.15497022e-03]\n",
      " [1.20970235e-03 8.86997454e-05 4.86600471e-02 ... 1.24717888e-02\n",
      "  3.36841553e-01 2.16668589e-02]\n",
      " ...\n",
      " [5.27675401e-01 2.44887319e-01 1.34954310e-02 ... 4.64859587e-03\n",
      "  2.44143880e-04 4.13425945e-02]\n",
      " [2.07796007e-02 5.57549111e-04 2.34154543e-01 ... 1.38537074e-01\n",
      "  4.46748825e-02 4.25250212e-02]\n",
      " [2.54092551e-01 2.34562896e-01 2.62966674e-02 ... 1.34161210e-02\n",
      "  5.45210024e-03 3.22428361e-01]] & cached\n",
      "Re-used Cached Value, runNum =  267\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  267\n",
      "Provided input from cache for runNum = 267\n",
      "Provided input from cache for runNum = 268\n",
      "activation = [[ 5.44588123  5.61862653  6.23446645 ... 10.41840592  4.72476931\n",
      "   3.36363351]\n",
      " [ 0.          3.3131473   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19479801  3.92700242  1.14602763 ...  2.15607368  4.82928511\n",
      "   1.83316353]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.72349262  1.89628576  2.34527128 ...  0.69932485  6.40072747\n",
      "   0.        ]\n",
      " [ 4.06495088  7.55915461  0.         ...  3.22285676  2.11689473\n",
      "   2.25743663]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30329302 0.         0.88568309 ... 0.         0.74153177 0.        ]\n",
      " [1.74533371 6.6782553  2.11258371 ... 6.31226424 2.42679724 5.24694907]\n",
      " ...\n",
      " [6.15559439 5.36856998 4.36837051 ... 2.67474585 6.76541337 3.38010369]\n",
      " [4.08446702 0.         6.68098957 ... 7.02319117 3.7115625  2.84000927]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.90552497e-06 4.93196900e-06 2.69874675e-03 ... 2.32904052e-01\n",
      "  4.14679351e-04 2.98760992e-02]\n",
      " [1.59837004e-01 6.91879969e-06 4.73344031e-01 ... 3.57918375e-03\n",
      "  6.80420011e-03 9.00334281e-03]\n",
      " [1.20312541e-03 8.76699215e-05 4.86893130e-02 ... 1.24032158e-02\n",
      "  3.36571852e-01 2.14101321e-02]\n",
      " ...\n",
      " [5.29359511e-01 2.44792564e-01 1.34374298e-02 ... 4.61735875e-03\n",
      "  2.40094711e-04 4.12127823e-02]\n",
      " [2.06844395e-02 5.54822121e-04 2.33558153e-01 ... 1.38694391e-01\n",
      "  4.44254814e-02 4.23615284e-02]\n",
      " [2.53842677e-01 2.35072943e-01 2.62021480e-02 ... 1.34125331e-02\n",
      "  5.40404955e-03 3.23882343e-01]] & cached\n",
      "Re-used Cached Value, runNum =  268\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  268\n",
      "Provided input from cache for runNum = 268\n",
      "Provided input from cache for runNum = 269\n",
      "activation = [[ 5.45062643  5.62064722  6.23645894 ... 10.43196191  4.72954495\n",
      "   3.36616956]\n",
      " [ 0.          3.31357052  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19419187  3.92540431  1.14303412 ...  2.1589464   4.8357023\n",
      "   1.83436186]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.72907421  1.91012359  2.35492623 ...  0.70475753  6.41211562\n",
      "   0.        ]\n",
      " [ 4.07324879  7.57172665  0.         ...  3.22873847  2.12256647\n",
      "   2.26450316]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30649987 0.         0.89514959 ... 0.         0.74368299 0.        ]\n",
      " [1.74802359 6.68677956 2.11095402 ... 6.33824994 2.43652203 5.26329442]\n",
      " ...\n",
      " [6.17261618 5.39731218 4.38656596 ... 2.69526374 6.78537387 3.39422052]\n",
      " [4.08955655 0.         6.69241813 ... 7.04585515 3.71878681 2.84276143]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.81998146e-06 4.71153977e-06 2.60209510e-03 ... 2.31313822e-01\n",
      "  4.05444848e-04 2.95122691e-02]\n",
      " [1.58783985e-01 6.75730059e-06 4.75536957e-01 ... 3.50492810e-03\n",
      "  6.69181860e-03 8.85546793e-03]\n",
      " [1.19619018e-03 8.66424501e-05 4.87069814e-02 ... 1.23318473e-02\n",
      "  3.36307880e-01 2.11548266e-02]\n",
      " ...\n",
      " [5.31087420e-01 2.44773169e-01 1.33818787e-02 ... 4.58766222e-03\n",
      "  2.36145186e-04 4.10872978e-02]\n",
      " [2.05873546e-02 5.52194813e-04 2.32945828e-01 ... 1.38848094e-01\n",
      "  4.41804569e-02 4.22000254e-02]\n",
      " [2.53557321e-01 2.35595442e-01 2.61074721e-02 ... 1.34096631e-02\n",
      "  5.35683224e-03 3.25336021e-01]] & cached\n",
      "Re-used Cached Value, runNum =  269\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  269\n",
      "Provided input from cache for runNum = 269\n",
      "Provided input from cache for runNum = 270\n",
      "activation = [[ 5.45533725  5.62259909  6.23841933 ... 10.44539122  4.734232\n",
      "   3.36866477]\n",
      " [ 0.          3.31405908  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19360876  3.92384547  1.14007048 ...  2.16185752  4.84211028\n",
      "   1.83558532]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.73475537  1.9240784   2.3646215  ...  0.71037863  6.42362093\n",
      "   0.        ]\n",
      " [ 4.08149113  7.58423803  0.         ...  3.23463443  2.12820047\n",
      "   2.27155378]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3097304  0.         0.90462321 ... 0.         0.74590676 0.        ]\n",
      " [1.7506783  6.69524353 2.10926791 ... 6.36402919 2.44610475 5.27960015]\n",
      " ...\n",
      " [6.18972239 5.42624355 4.40484083 ... 2.7161086  6.8054972  3.40840356]\n",
      " [4.09464945 0.         6.70376878 ... 7.06837662 3.72594885 2.84545084]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.73899007e-06 4.50114679e-06 2.50905977e-03 ... 2.29723475e-01\n",
      "  3.96269675e-04 2.91541589e-02]\n",
      " [1.57740500e-01 6.59788532e-06 4.77685703e-01 ... 3.43249990e-03\n",
      "  6.58067525e-03 8.70922485e-03]\n",
      " [1.19014103e-03 8.56745600e-05 4.87456713e-02 ... 1.22706008e-02\n",
      "  3.36043895e-01 2.09078361e-02]\n",
      " ...\n",
      " [5.32777308e-01 2.44645526e-01 1.33257759e-02 ... 4.55861309e-03\n",
      "  2.32218349e-04 4.09593023e-02]\n",
      " [2.04954280e-02 5.49564274e-04 2.32344711e-01 ... 1.39032297e-01\n",
      "  4.39284105e-02 4.20393485e-02]\n",
      " [2.53276556e-01 2.36042027e-01 2.60116002e-02 ... 1.34082319e-02\n",
      "  5.30923721e-03 3.26775434e-01]] & cached\n",
      "Re-used Cached Value, runNum =  270\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  270\n",
      "Provided input from cache for runNum = 270\n",
      "Provided input from cache for runNum = 271\n",
      "activation = [[ 5.46009264  5.62464094  6.24038975 ... 10.45882739  4.73895835\n",
      "   3.37118354]\n",
      " [ 0.          3.31449382  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19302684  3.9222818   1.13711788 ...  2.16479252  4.84849638\n",
      "   1.83681653]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.74038685  1.93793625  2.37427343 ...  0.71596984  6.43506453\n",
      "   0.        ]\n",
      " [ 4.08961772  7.59656097  0.         ...  3.24043601  2.13368151\n",
      "   2.27852355]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31278169 0.         0.91393456 ... 0.         0.74803867 0.        ]\n",
      " [1.75337288 6.70367924 2.10760136 ... 6.38970312 2.45566041 5.29584584]\n",
      " ...\n",
      " [6.20654068 5.4548338  4.42290333 ... 2.73664919 6.825302   3.42244699]\n",
      " [4.09994339 0.         6.7151387  ... 7.09098424 3.7333051  2.84823064]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.66288868e-06 4.30296213e-06 2.42031800e-03 ... 2.28199990e-01\n",
      "  3.87578090e-04 2.88079355e-02]\n",
      " [1.56712821e-01 6.44423487e-06 4.79802334e-01 ... 3.36117050e-03\n",
      "  6.47309155e-03 8.56604995e-03]\n",
      " [1.18428471e-03 8.47290698e-05 4.87844469e-02 ... 1.22083164e-02\n",
      "  3.35800955e-01 2.06646479e-02]\n",
      " ...\n",
      " [5.34464222e-01 2.44548105e-01 1.32698913e-02 ... 4.52884144e-03\n",
      "  2.28367352e-04 4.08278007e-02]\n",
      " [2.04069184e-02 5.47021385e-04 2.31744018e-01 ... 1.39193415e-01\n",
      "  4.36861650e-02 4.18793874e-02]\n",
      " [2.52977679e-01 2.36519642e-01 2.59137255e-02 ... 1.34022683e-02\n",
      "  5.26213166e-03 3.28180682e-01]] & cached\n",
      "Re-used Cached Value, runNum =  271\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  271\n",
      "Provided input from cache for runNum = 271\n",
      "Provided input from cache for runNum = 272\n",
      "activation = [[ 5.46483146  5.62663991  6.24233527 ... 10.47216353  4.74360987\n",
      "   3.37367852]\n",
      " [ 0.          3.31493975  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19239235  3.92065195  1.13414124 ...  2.16766709  4.8547792\n",
      "   1.83802973]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.74593934  1.95165707  2.38384158 ...  0.72144377  6.44642416\n",
      "   0.        ]\n",
      " [ 4.09775356  7.60891722  0.         ...  3.2463498   2.13918896\n",
      "   2.28551339]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31569269 0.         0.92313335 ... 0.         0.75013479 0.        ]\n",
      " [1.75607734 6.71210078 2.10590191 ... 6.41534285 2.46521719 5.31203844]\n",
      " ...\n",
      " [6.22336669 5.4835052  4.4410051  ... 2.75738969 6.84524595 3.43659468]\n",
      " [4.10505701 0.         6.72631577 ... 7.11318777 3.74041476 2.85085806]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.58954803e-06 4.11167591e-06 2.33415424e-03 ... 2.26597730e-01\n",
      "  3.78980718e-04 2.84574352e-02]\n",
      " [1.55651966e-01 6.29419694e-06 4.81948430e-01 ... 3.29224405e-03\n",
      "  6.36627381e-03 8.42544130e-03]\n",
      " [1.17784681e-03 8.37641849e-05 4.88128185e-02 ... 1.21455913e-02\n",
      "  3.35513698e-01 2.04211693e-02]\n",
      " ...\n",
      " [5.36192872e-01 2.44484770e-01 1.32180593e-02 ... 4.50289574e-03\n",
      "  2.24604353e-04 4.07031138e-02]\n",
      " [2.03113881e-02 5.44359058e-04 2.31115403e-01 ... 1.39359259e-01\n",
      "  4.34387244e-02 4.17154711e-02]\n",
      " [2.52680993e-01 2.37008098e-01 2.58217840e-02 ... 1.34057258e-02\n",
      "  5.21623689e-03 3.29634460e-01]] & cached\n",
      "Re-used Cached Value, runNum =  272\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  272\n",
      "Provided input from cache for runNum = 272\n",
      "Provided input from cache for runNum = 273\n",
      "activation = [[ 5.46956118  5.62862019  6.24426142 ... 10.48540714  4.7482121\n",
      "   3.37615722]\n",
      " [ 0.          3.31542419  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19175707  3.91901701  1.13116841 ...  2.17056038  4.86103103\n",
      "   1.83925386]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.75135931  1.96517698  2.39329799 ...  0.72676027  6.45763005\n",
      "   0.        ]\n",
      " [ 4.10587025  7.62126011  0.         ...  3.25233707  2.14468467\n",
      "   2.29250087]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3183243  0.         0.93210211 ... 0.         0.75202919 0.        ]\n",
      " [1.75883423 6.7205303  2.10421302 ... 6.44091107 2.47476333 5.3281513 ]\n",
      " ...\n",
      " [6.23984958 5.51178202 4.45882636 ... 2.7777549  6.8648466  3.45060952]\n",
      " [4.11012358 0.         6.73739595 ... 7.13514649 3.74741437 2.85342308]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.52040686e-06 3.93069775e-06 2.25214070e-03 ... 2.25060417e-01\n",
      "  3.70815353e-04 2.81161468e-02]\n",
      " [1.54561609e-01 6.14748887e-06 4.84046489e-01 ... 3.22422245e-03\n",
      "  6.26188264e-03 8.28710700e-03]\n",
      " [1.17140530e-03 8.28085367e-05 4.88435491e-02 ... 1.20823303e-02\n",
      "  3.35231858e-01 2.01819243e-02]\n",
      " ...\n",
      " [5.37960661e-01 2.44445908e-01 1.31684784e-02 ... 4.47826123e-03\n",
      "  2.20988850e-04 4.05785954e-02]\n",
      " [2.02168679e-02 5.41708814e-04 2.30497127e-01 ... 1.39505005e-01\n",
      "  4.32007968e-02 4.15513942e-02]\n",
      " [2.52369001e-01 2.37485752e-01 2.57305579e-02 ... 1.34096575e-02\n",
      "  5.17203400e-03 3.31075216e-01]] & cached\n",
      "Re-used Cached Value, runNum =  273\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  273\n",
      "Provided input from cache for runNum = 273\n",
      "Provided input from cache for runNum = 274\n",
      "activation = [[ 5.47428397  5.63058927  6.2461694  ... 10.49858255  4.75276706\n",
      "   3.37862622]\n",
      " [ 0.          3.31589816  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19107115  3.91730451  1.12816992 ...  2.17335188  4.86717745\n",
      "   1.84043612]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.75680666  1.97872441  2.4027618  ...  0.73212568  6.46885457\n",
      "   0.        ]\n",
      " [ 4.11396369  7.63358262  0.         ...  3.25837369  2.15020406\n",
      "   2.29948231]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32107255 0.         0.94113972 ... 0.         0.75402593 0.        ]\n",
      " [1.76158299 6.72888796 2.10248281 ... 6.46633448 2.48424285 5.34422027]\n",
      " ...\n",
      " [6.25628863 5.54004519 4.47662128 ... 2.79818645 6.88444981 3.46462517]\n",
      " [4.11515382 0.         6.74838904 ... 7.15693407 3.75430494 2.85593044]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.45451614e-06 3.75848154e-06 2.17273208e-03 ... 2.23496118e-01\n",
      "  3.62789864e-04 2.77783715e-02]\n",
      " [1.53509206e-01 6.00658649e-06 4.86184691e-01 ... 3.15851638e-03\n",
      "  6.15927028e-03 8.15184557e-03]\n",
      " [1.16517448e-03 8.18796556e-05 4.88722409e-02 ... 1.20196752e-02\n",
      "  3.34932450e-01 1.99448167e-02]\n",
      " ...\n",
      " [5.39669552e-01 2.44397684e-01 1.31171311e-02 ... 4.45424141e-03\n",
      "  2.17391348e-04 4.04514669e-02]\n",
      " [2.01223886e-02 5.39127504e-04 2.29838929e-01 ... 1.39643806e-01\n",
      "  4.29576425e-02 4.13852751e-02]\n",
      " [2.52076231e-01 2.37995818e-01 2.56383162e-02 ... 1.34157977e-02\n",
      "  5.12792185e-03 3.32521235e-01]] & cached\n",
      "Re-used Cached Value, runNum =  274\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  274\n",
      "Provided input from cache for runNum = 274\n",
      "Provided input from cache for runNum = 275\n",
      "activation = [[ 5.47900847  5.63255134  6.24806921 ... 10.51168854  4.75727854\n",
      "   3.38108652]\n",
      " [ 0.          3.3163722   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.19039707  3.9156453   1.12520844 ...  2.17618696  4.87331325\n",
      "   1.84164626]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.76223052  1.99215314  2.41214852 ...  0.73741773  6.48000117\n",
      "   0.        ]\n",
      " [ 4.12202957  7.64587651  0.         ...  3.26446033  2.15570019\n",
      "   2.30646062]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32334424 0.         0.9497339  ... 0.         0.75557883 0.        ]\n",
      " [1.76430625 6.73721806 2.10071587 ... 6.49165258 2.49367681 5.36021823]\n",
      " ...\n",
      " [6.2726142  5.56814067 4.49429857 ... 2.81855046 6.90394531 3.47860688]\n",
      " [4.12021423 0.         6.7593319  ... 7.17859208 3.76118785 2.85841356]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.39226345e-06 3.59466732e-06 2.09694184e-03 ... 2.21958727e-01\n",
      "  3.55105149e-04 2.74471624e-02]\n",
      " [1.52437690e-01 5.86869185e-06 4.88268456e-01 ... 3.09433879e-03\n",
      "  6.05816080e-03 8.01903371e-03]\n",
      " [1.15921101e-03 8.09755484e-05 4.89074890e-02 ... 1.19604434e-02\n",
      "  3.34642345e-01 1.97134866e-02]\n",
      " ...\n",
      " [5.41405887e-01 2.44311414e-01 1.30680964e-02 ... 4.43118953e-03\n",
      "  2.13900701e-04 4.03225894e-02]\n",
      " [2.00316597e-02 5.36492594e-04 2.29207994e-01 ... 1.39776309e-01\n",
      "  4.27256740e-02 4.12187634e-02]\n",
      " [2.51759261e-01 2.38467802e-01 2.55459310e-02 ... 1.34230669e-02\n",
      "  5.08450943e-03 3.33949641e-01]] & cached\n",
      "Re-used Cached Value, runNum =  275\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  275\n",
      "Provided input from cache for runNum = 275\n",
      "Provided input from cache for runNum = 276\n",
      "activation = [[ 5.4837062   5.63445931  6.24993585 ... 10.52467631  4.76170528\n",
      "   3.38351573]\n",
      " [ 0.          3.31682668  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18973275  3.91400562  1.12224869 ...  2.17907642  4.87943266\n",
      "   1.84287774]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.76754752  2.00539422  2.42144596 ...  0.74255882  6.49100462\n",
      "   0.        ]\n",
      " [ 4.13005983  7.65814265  0.         ...  3.27061313  2.16118027\n",
      "   2.31342887]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32547703 0.         0.95822423 ... 0.         0.75702744 0.        ]\n",
      " [1.76708159 6.74558401 2.09893759 ... 6.51694577 2.50310677 5.37616804]\n",
      " ...\n",
      " [6.2887313  5.59599221 4.51181183 ... 2.83870955 6.9232414  3.49253362]\n",
      " [4.12525754 0.         6.77018589 ... 7.20002186 3.76795481 2.86084604]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.33316937e-06 3.43882994e-06 2.02411688e-03 ... 2.20448440e-01\n",
      "  3.47699188e-04 2.71217375e-02]\n",
      " [1.51360011e-01 5.73408354e-06 4.90349231e-01 ... 3.03147156e-03\n",
      "  5.95891720e-03 7.88858390e-03]\n",
      " [1.15328009e-03 8.00800361e-05 4.89430171e-02 ... 1.19016028e-02\n",
      "  3.34344922e-01 1.94859751e-02]\n",
      " ...\n",
      " [5.43150172e-01 2.44220283e-01 1.30206389e-02 ... 4.40967026e-03\n",
      "  2.10524686e-04 4.01924761e-02]\n",
      " [1.99398332e-02 5.33776462e-04 2.28560463e-01 ... 1.39884974e-01\n",
      "  4.24984864e-02 4.10499793e-02]\n",
      " [2.51437142e-01 2.38933058e-01 2.54544769e-02 ... 1.34329079e-02\n",
      "  5.04238135e-03 3.35371413e-01]] & cached\n",
      "Re-used Cached Value, runNum =  276\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  276\n",
      "Provided input from cache for runNum = 276\n",
      "Provided input from cache for runNum = 277\n",
      "activation = [[ 5.48843549  5.63640852  6.25180153 ... 10.53764377  4.76612151\n",
      "   3.3859551 ]\n",
      " [ 0.          3.31727633  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18901212  3.91229898  1.11927578 ...  2.18189437  4.88546105\n",
      "   1.84407621]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.77281914  2.01859208  2.43070114 ...  0.74768505  6.50198245\n",
      "   0.        ]\n",
      " [ 4.1380288   7.67033705  0.         ...  3.27676879  2.16662265\n",
      "   2.32036785]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32761515 0.         0.96670968 ... 0.         0.75854645 0.        ]\n",
      " [1.7698721  6.75388256 2.09715095 ... 6.54209823 2.51247456 5.39206013]\n",
      " ...\n",
      " [6.30467281 5.62363818 4.52919538 ... 2.85872247 6.94239534 3.5063986 ]\n",
      " [4.13039282 0.         6.78101517 ... 7.22140281 3.77476092 2.86331034]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.27701833e-06 3.29121434e-06 1.95393330e-03 ... 2.18947817e-01\n",
      "  3.40511303e-04 2.68032528e-02]\n",
      " [1.50303228e-01 5.60441246e-06 4.92427132e-01 ... 2.97007729e-03\n",
      "  5.86195919e-03 7.76098912e-03]\n",
      " [1.14740740e-03 7.92069730e-05 4.89748156e-02 ... 1.18419716e-02\n",
      "  3.34050144e-01 1.92615484e-02]\n",
      " ...\n",
      " [5.44884463e-01 2.44167501e-01 1.29728943e-02 ... 4.38858764e-03\n",
      "  2.07181034e-04 4.00614776e-02]\n",
      " [1.98513683e-02 5.31240557e-04 2.27914828e-01 ... 1.40002907e-01\n",
      "  4.22731517e-02 4.08859789e-02]\n",
      " [2.51103860e-01 2.39429175e-01 2.53610529e-02 ... 1.34416383e-02\n",
      "  5.00025287e-03 3.36777063e-01]] & cached\n",
      "Re-used Cached Value, runNum =  277\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  277\n",
      "Provided input from cache for runNum = 277\n",
      "Provided input from cache for runNum = 278\n",
      "activation = [[ 5.49319414  5.63839369  6.25367639 ... 10.55059278  4.77052388\n",
      "   3.38840473]\n",
      " [ 0.          3.31772984  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18823086  3.91051964  1.11627908 ...  2.1846284   4.89138482\n",
      "   1.84524421]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.77818811  2.03185479  2.43996208 ...  0.75292346  6.51305728\n",
      "   0.        ]\n",
      " [ 4.14597951  7.68251561  0.         ...  3.28296827  2.17206538\n",
      "   2.32730851]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32972443 0.         0.97512929 ... 0.         0.76006434 0.        ]\n",
      " [1.77256837 6.76202631 2.09528939 ... 6.56701925 2.52170463 5.40787454]\n",
      " ...\n",
      " [6.32069524 5.65141959 4.54664686 ... 2.87897369 6.96175832 3.52032598]\n",
      " [4.13558921 0.         6.79181748 ... 7.2426958  3.78156133 2.86575708]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.22355376e-06 3.15015126e-06 1.88612591e-03 ... 2.17426941e-01\n",
      "  3.33350390e-04 2.64884643e-02]\n",
      " [1.49268429e-01 5.47831196e-06 4.94479582e-01 ... 2.91029171e-03\n",
      "  5.76605234e-03 7.63568094e-03]\n",
      " [1.14205809e-03 7.83741768e-05 4.90149020e-02 ... 1.17877132e-02\n",
      "  3.33746516e-01 1.90424877e-02]\n",
      " ...\n",
      " [5.46591192e-01 2.44095295e-01 1.29243798e-02 ... 4.36792504e-03\n",
      "  2.03824930e-04 3.99305684e-02]\n",
      " [1.97694168e-02 5.28871882e-04 2.27291461e-01 ... 1.40163118e-01\n",
      "  4.20417629e-02 4.07275408e-02]\n",
      " [2.50763284e-01 2.39910283e-01 2.52651160e-02 ... 1.34504586e-02\n",
      "  4.95731315e-03 3.38174532e-01]] & cached\n",
      "Re-used Cached Value, runNum =  278\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  278\n",
      "Provided input from cache for runNum = 278\n",
      "Provided input from cache for runNum = 279\n",
      "activation = [[ 5.49795374  5.64036161  6.25553488 ... 10.56347457  4.77487707\n",
      "   3.39084074]\n",
      " [ 0.          3.31820238  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18743346  3.9087244   1.11328758 ...  2.18731988  4.89725182\n",
      "   1.84640619]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.78348349  2.04500112  2.44915958 ...  0.75805884  6.52402396\n",
      "   0.        ]\n",
      " [ 4.15385354  7.6946034   0.         ...  3.28914201  2.17744407\n",
      "   2.3342043 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33171055 0.         0.98341987 ... 0.         0.7614713  0.        ]\n",
      " [1.77545469 6.77034984 2.09354335 ... 6.59203323 2.53105663 5.42371177]\n",
      " ...\n",
      " [6.33659571 5.67907206 4.56402189 ... 2.89919838 6.98102275 3.53424427]\n",
      " [4.14082977 0.         6.80255737 ... 7.26390592 3.78833927 2.86819857]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.17284003e-06 3.01625905e-06 1.82099593e-03 ... 2.15906549e-01\n",
      "  3.26456956e-04 2.61787479e-02]\n",
      " [1.48238795e-01 5.35576618e-06 4.96536854e-01 ... 2.85194938e-03\n",
      "  5.67157950e-03 7.51266075e-03]\n",
      " [1.13669333e-03 7.75512434e-05 4.90510774e-02 ... 1.17316112e-02\n",
      "  3.33425235e-01 1.88253105e-02]\n",
      " ...\n",
      " [5.48272798e-01 2.44012112e-01 1.28767852e-02 ... 4.34764534e-03\n",
      "  2.00537899e-04 3.97955291e-02]\n",
      " [1.96849207e-02 5.26438674e-04 2.26629466e-01 ... 1.40277798e-01\n",
      "  4.18113169e-02 4.05632593e-02]\n",
      " [2.50442620e-01 2.40427124e-01 2.51720775e-02 ... 1.34611471e-02\n",
      "  4.91531649e-03 3.39567060e-01]] & cached\n",
      "Re-used Cached Value, runNum =  279\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  279\n",
      "Provided input from cache for runNum = 279\n",
      "Provided input from cache for runNum = 280\n",
      "activation = [[ 5.50272605  5.6423493   6.25738776 ... 10.57632101  4.77920361\n",
      "   3.39327465]\n",
      " [ 0.          3.31866191  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18660923  3.90687863  1.11030054 ...  2.18995532  4.90304408\n",
      "   1.84755521]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.78871422  2.058022    2.45828618 ...  0.76310003  6.53491207\n",
      "   0.        ]\n",
      " [ 4.16165907  7.70663925  0.         ...  3.29533855  2.18276521\n",
      "   2.3410727 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3335191  0.         0.99152239 ... 0.         0.76276704 0.        ]\n",
      " [1.77832454 6.77860286 2.09176189 ... 6.61691593 2.54034422 5.43945402]\n",
      " ...\n",
      " [6.35243435 5.70666695 4.58136786 ... 2.91945267 7.00027335 3.54817671]\n",
      " [4.14610625 0.         6.81323541 ... 7.28498075 3.79509217 2.87061601]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.12471755e-06 2.88900695e-06 1.75836937e-03 ... 2.14392831e-01\n",
      "  3.19751128e-04 2.58744964e-02]\n",
      " [1.47228726e-01 5.23764300e-06 4.98587757e-01 ... 2.79517328e-03\n",
      "  5.57874241e-03 7.39257142e-03]\n",
      " [1.13153784e-03 7.67541712e-05 4.90886293e-02 ... 1.16769927e-02\n",
      "  3.33094638e-01 1.86126822e-02]\n",
      " ...\n",
      " [5.49941050e-01 2.43943893e-01 1.28284122e-02 ... 4.32749892e-03\n",
      "  1.97273040e-04 3.96585948e-02]\n",
      " [1.96042289e-02 5.24140050e-04 2.25969234e-01 ... 1.40394952e-01\n",
      "  4.15817474e-02 4.04018347e-02]\n",
      " [2.50109609e-01 2.40958713e-01 2.50768432e-02 ... 1.34717579e-02\n",
      "  4.87315041e-03 3.40944718e-01]] & cached\n",
      "Re-used Cached Value, runNum =  280\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  280\n",
      "Provided input from cache for runNum = 280\n",
      "Provided input from cache for runNum = 281\n",
      "activation = [[ 5.50745998  5.64429512  6.25920588 ... 10.58905262  4.78345605\n",
      "   3.39567512]\n",
      " [ 0.          3.31912178  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18581541  3.90505955  1.1073582  ...  2.19263502  4.90882975\n",
      "   1.84872474]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.7939627   2.07108265  2.46741295 ...  0.76822813  6.54579881\n",
      "   0.        ]\n",
      " [ 4.16944498  7.71864797  0.         ...  3.30156804  2.18806316\n",
      "   2.34794144]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33530842 0.         0.99960105 ... 0.         0.76407304 0.        ]\n",
      " [1.78112187 6.78668196 2.08989632 ... 6.64155193 2.54950633 5.45509627]\n",
      " ...\n",
      " [6.36804344 5.73399753 4.59851586 ... 2.9394758  7.01928708 3.56197254]\n",
      " [4.15129112 0.         6.82379281 ... 7.30580697 3.80171536 2.87293932]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.07897114e-06 2.76810462e-06 1.69826299e-03 ... 2.12932669e-01\n",
      "  3.13269194e-04 2.55775317e-02]\n",
      " [1.46223272e-01 5.12293454e-06 5.00639031e-01 ... 2.73977155e-03\n",
      "  5.48818202e-03 7.27474277e-03]\n",
      " [1.12659258e-03 7.59817912e-05 4.91328057e-02 ... 1.16255009e-02\n",
      "  3.32764487e-01 1.84037231e-02]\n",
      " ...\n",
      " [5.51614866e-01 2.43863405e-01 1.27805145e-02 ... 4.30805428e-03\n",
      "  1.94089237e-04 3.95229636e-02]\n",
      " [1.95227307e-02 5.21839284e-04 2.25289992e-01 ... 1.40502270e-01\n",
      "  4.13542683e-02 4.02388036e-02]\n",
      " [2.49760833e-01 2.41455688e-01 2.49804982e-02 ... 1.34824608e-02\n",
      "  4.83169090e-03 3.42312735e-01]] & cached\n",
      "Re-used Cached Value, runNum =  281\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  281\n",
      "Provided input from cache for runNum = 281\n",
      "Provided input from cache for runNum = 282\n",
      "activation = [[ 5.51216106  5.64619712  6.26099985 ... 10.6016673   4.78763908\n",
      "   3.39804461]\n",
      " [ 0.          3.31958542  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18503514  3.90325023  1.10442864 ...  2.19534939  4.91457923\n",
      "   1.8499002 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.79920303  2.08413384  2.47650515 ...  0.7733846   6.55664489\n",
      "   0.        ]\n",
      " [ 4.17717147  7.7305872   0.         ...  3.30780666  2.19330319\n",
      "   2.35477013]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33707331 0.         1.00763595 ... 0.         0.76542338 0.        ]\n",
      " [1.78388579 6.79466055 2.08799769 ... 6.66598734 2.55856152 5.47065579]\n",
      " ...\n",
      " [6.38347045 5.76113251 4.61553847 ... 2.95937477 7.03811929 3.57568232]\n",
      " [4.15647138 0.         6.83427396 ... 7.32644323 3.8082801  2.87522761]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.03539940e-06 2.65290822e-06 1.64035929e-03 ... 2.11487383e-01\n",
      "  3.06973010e-04 2.52864520e-02]\n",
      " [1.45219628e-01 5.01105571e-06 5.02679820e-01 ... 2.68571849e-03\n",
      "  5.39971290e-03 7.15903660e-03]\n",
      " [1.12177511e-03 7.52268704e-05 4.91784312e-02 ... 1.15761310e-02\n",
      "  3.32437261e-01 1.81981966e-02]\n",
      " ...\n",
      " [5.53293334e-01 2.43777412e-01 1.27339373e-02 ... 4.28990559e-03\n",
      "  1.90982828e-04 3.93878877e-02]\n",
      " [1.94419799e-02 5.19554633e-04 2.24606391e-01 ... 1.40615862e-01\n",
      "  4.11298085e-02 4.00770317e-02]\n",
      " [2.49400740e-01 2.41933267e-01 2.48846150e-02 ... 1.34946163e-02\n",
      "  4.79089390e-03 3.43669766e-01]] & cached\n",
      "Re-used Cached Value, runNum =  282\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  282\n",
      "Provided input from cache for runNum = 282\n",
      "Provided input from cache for runNum = 283\n",
      "activation = [[ 5.51684421  5.64808076  6.26277676 ... 10.61420721  4.79176503\n",
      "   3.40039756]\n",
      " [ 0.          3.3200937   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18422901  3.90140769  1.1015068  ...  2.19802582  4.9202619\n",
      "   1.85106241]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.80452457  2.09728215  2.4856199  ...  0.7786704   6.56755671\n",
      "   0.        ]\n",
      " [ 4.18484695  7.74246317  0.         ...  3.31404915  2.19851138\n",
      "   2.36157015]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33892454 0.         1.01570988 ... 0.         0.766891   0.        ]\n",
      " [1.78657592 6.80251385 2.08603013 ... 6.69019996 2.56748485 5.48615341]\n",
      " ...\n",
      " [6.39892272 5.78832801 4.63257438 ... 2.97944783 7.05701061 3.58940526]\n",
      " [4.16160823 0.         6.84464353 ... 7.34688957 3.81473698 2.87745008]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.93820549e-07 2.54330485e-06 1.58432878e-03 ... 2.10017896e-01\n",
      "  3.00743923e-04 2.49993041e-02]\n",
      " [1.44255125e-01 4.90339526e-06 5.04732050e-01 ... 2.63331804e-03\n",
      "  5.31282490e-03 7.04557101e-03]\n",
      " [1.11730333e-03 7.45107700e-05 4.92261034e-02 ... 1.15295824e-02\n",
      "  3.32083108e-01 1.79956200e-02]\n",
      " ...\n",
      " [5.54907599e-01 2.43665335e-01 1.26850202e-02 ... 4.27154911e-03\n",
      "  1.87867666e-04 3.92491220e-02]\n",
      " [1.93642338e-02 5.17419263e-04 2.23906894e-01 ... 1.40737484e-01\n",
      "  4.09002442e-02 3.99157954e-02]\n",
      " [2.49059819e-01 2.42432007e-01 2.47870883e-02 ... 1.35075113e-02\n",
      "  4.74979735e-03 3.45021352e-01]] & cached\n",
      "Re-used Cached Value, runNum =  283\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  283\n",
      "Provided input from cache for runNum = 283\n",
      "Provided input from cache for runNum = 284\n",
      "activation = [[ 5.52151123  5.6499339   6.26454048 ... 10.62666807  4.79582703\n",
      "   3.40273288]\n",
      " [ 0.          3.32060754  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18342261  3.89956293  1.09858863 ...  2.20070179  4.92590955\n",
      "   1.85222555]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.80984538  2.11044316  2.49470316 ...  0.78401835  6.57847969\n",
      "   0.        ]\n",
      " [ 4.19246865  7.75427386  0.         ...  3.32028414  2.20367296\n",
      "   2.36833595]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34069196 0.         1.02369284 ... 0.         0.76834512 0.        ]\n",
      " [1.78921462 6.81023265 2.08401758 ... 6.71417472 2.57624743 5.50155688]\n",
      " ...\n",
      " [6.41416523 5.81531038 4.64945685 ... 2.99938673 7.07572821 3.60301867]\n",
      " [4.16665702 0.         6.85487963 ... 7.36708568 3.82102906 2.87957814]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.54479533e-07 2.43994331e-06 1.53069107e-03 ... 2.08589451e-01\n",
      "  2.94709858e-04 2.47208986e-02]\n",
      " [1.43298440e-01 4.79948580e-06 5.06745846e-01 ... 2.58178433e-03\n",
      "  5.22813315e-03 6.93405703e-03]\n",
      " [1.11307505e-03 7.38354184e-05 4.92772108e-02 ... 1.14847231e-02\n",
      "  3.31712025e-01 1.77967591e-02]\n",
      " ...\n",
      " [5.56513103e-01 2.43547501e-01 1.26353640e-02 ... 4.25249543e-03\n",
      "  1.84797981e-04 3.91071023e-02]\n",
      " [1.92904466e-02 5.15468860e-04 2.23222804e-01 ... 1.40859719e-01\n",
      "  4.06730849e-02 3.97570104e-02]\n",
      " [2.48712752e-01 2.42930939e-01 2.46876888e-02 ... 1.35177796e-02\n",
      "  4.70912877e-03 3.46346886e-01]] & cached\n",
      "Re-used Cached Value, runNum =  284\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  284\n",
      "Provided input from cache for runNum = 284\n",
      "Provided input from cache for runNum = 285\n",
      "activation = [[ 5.52618957  5.65181065  6.26630093 ... 10.6390946   4.79988052\n",
      "   3.40507549]\n",
      " [ 0.          3.32119694  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.18255139  3.89762226  1.09563194 ...  2.20326622  4.93144595\n",
      "   1.85334141]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.81502596  2.12335277  2.50367131 ...  0.78914213  6.58923234\n",
      "   0.        ]\n",
      " [ 4.200131    7.76614348  0.         ...  3.32665467  2.20887649\n",
      "   2.37513868]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34248968 0.         1.03173595 ... 0.         0.76986609 0.        ]\n",
      " [1.79198835 6.81812105 2.08206662 ... 6.73823292 2.58512044 5.51695492]\n",
      " ...\n",
      " [6.42923413 5.84205059 4.66622453 ... 3.01919358 7.09430345 3.61662833]\n",
      " [4.17162983 0.         6.86504179 ... 7.38706952 3.82724245 2.88165496]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.16522233e-07 2.34049554e-06 1.47878645e-03 ... 2.07143974e-01\n",
      "  2.88854951e-04 2.44431918e-02]\n",
      " [1.42283285e-01 4.69582630e-06 5.08742416e-01 ... 2.53099698e-03\n",
      "  5.14473290e-03 6.82389795e-03]\n",
      " [1.10827294e-03 7.31265883e-05 4.93203182e-02 ... 1.14367350e-02\n",
      "  3.31341527e-01 1.75979806e-02]\n",
      " ...\n",
      " [5.58214751e-01 2.43514214e-01 1.25910995e-02 ... 4.23714967e-03\n",
      "  1.81860919e-04 3.89747585e-02]\n",
      " [1.92116493e-02 5.13442860e-04 2.22542859e-01 ... 1.40996027e-01\n",
      "  4.04492503e-02 3.96002571e-02]\n",
      " [2.48338235e-01 2.43414996e-01 2.45927087e-02 ... 1.35335235e-02\n",
      "  4.67021335e-03 3.47694950e-01]] & cached\n",
      "Re-used Cached Value, runNum =  285\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  285\n",
      "Provided input from cache for runNum = 285\n",
      "Provided input from cache for runNum = 286\n",
      "activation = [[ 5.53089663  5.65372013  6.26807689 ... 10.65149861  4.80392291\n",
      "   3.40742697]\n",
      " [ 0.          3.32179148  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1816674   3.89568151  1.09267764 ...  2.20581354  4.93693906\n",
      "   1.85445012]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.82023988  2.13632589  2.51261788 ...  0.79436484  6.60003693\n",
      "   0.        ]\n",
      " [ 4.20772752  7.77790392  0.         ...  3.33298957  2.2140191\n",
      "   2.38189757]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3441657  0.         1.0396508  ... 0.         0.77138221 0.        ]\n",
      " [1.79474745 6.82592495 2.0801035  ... 6.76209346 2.5938834  5.53229729]\n",
      " ...\n",
      " [6.44417653 5.86865542 4.68286815 ... 3.03894019 7.11278274 3.63015802]\n",
      " [4.1767497  0.         6.87522613 ... 7.40709321 3.83356072 2.88378938]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.80719440e-07 2.24654723e-06 1.42899734e-03 ... 2.05722286e-01\n",
      "  2.83178718e-04 2.41737581e-02]\n",
      " [1.41317279e-01 4.59628816e-06 5.10744057e-01 ... 2.48168728e-03\n",
      "  5.06347089e-03 6.71623402e-03]\n",
      " [1.10394954e-03 7.24591270e-05 4.93646265e-02 ... 1.13904788e-02\n",
      "  3.30983519e-01 1.74032093e-02]\n",
      " ...\n",
      " [5.59845178e-01 2.43440744e-01 1.25442733e-02 ... 4.22081579e-03\n",
      "  1.78923055e-04 3.88362101e-02]\n",
      " [1.91376895e-02 5.11539742e-04 2.21843168e-01 ... 1.41118636e-01\n",
      "  4.02254481e-02 3.94440497e-02]\n",
      " [2.47974965e-01 2.43922765e-01 2.44945291e-02 ... 1.35467148e-02\n",
      "  4.63085804e-03 3.49011380e-01]] & cached\n",
      "Re-used Cached Value, runNum =  286\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  286\n",
      "Provided input from cache for runNum = 286\n",
      "Provided input from cache for runNum = 287\n",
      "activation = [[ 5.53557389  5.65556885  6.26982431 ... 10.66378079  4.80788613\n",
      "   3.40974308]\n",
      " [ 0.          3.3224433   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1807809   3.89372286  1.08974159 ...  2.20837303  4.94239035\n",
      "   1.85556295]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.82542123  2.14921086  2.52151113 ...  0.7995581   6.61078721\n",
      "   0.        ]\n",
      " [ 4.21530287  7.78962909  0.         ...  3.33936663  2.21913927\n",
      "   2.38865671]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34591084 0.         1.04758389 ... 0.         0.77299701 0.        ]\n",
      " [1.79750609 6.83370079 2.07812084 ... 6.78588146 2.60263369 5.54758555]\n",
      " ...\n",
      " [6.45907578 5.89521332 4.69947099 ... 3.05875545 7.13124943 3.6437134 ]\n",
      " [4.18178939 0.         6.88529225 ... 7.42685647 3.83973955 2.88583333]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.46171015e-07 2.15595921e-06 1.38075423e-03 ... 2.04284857e-01\n",
      "  2.77592166e-04 2.39046914e-02]\n",
      " [1.40343275e-01 4.49875799e-06 5.12760999e-01 ... 2.43368278e-03\n",
      "  4.98334476e-03 6.61048126e-03]\n",
      " [1.09944125e-03 7.17833182e-05 4.94073839e-02 ... 1.13459904e-02\n",
      "  3.30606777e-01 1.72100121e-02]\n",
      " ...\n",
      " [5.61490154e-01 2.43380753e-01 1.24996586e-02 ... 4.20632118e-03\n",
      "  1.76041742e-04 3.87010951e-02]\n",
      " [1.90593595e-02 5.09542148e-04 2.21119861e-01 ... 1.41241981e-01\n",
      "  3.99981238e-02 3.92857911e-02]\n",
      " [2.47608915e-01 2.44421754e-01 2.43992025e-02 ... 1.35645578e-02\n",
      "  4.59219963e-03 3.50347966e-01]] & cached\n",
      "Re-used Cached Value, runNum =  287\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  287\n",
      "Provided input from cache for runNum = 287\n",
      "Provided input from cache for runNum = 288\n",
      "activation = [[ 5.54024792  5.65741094  6.27156517 ... 10.67599937  4.81180148\n",
      "   3.41204914]\n",
      " [ 0.          3.32310656  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17992532  3.89180452  1.0868328  ...  2.2109886   4.94785724\n",
      "   1.85670487]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.83051089  2.16193075  2.53031842 ...  0.80464519  6.62140585\n",
      "   0.        ]\n",
      " [ 4.22282978  7.80129218  0.         ...  3.34574458  2.22419935\n",
      "   2.3953862 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34738056 0.         1.05526237 ... 0.         0.77435167 0.        ]\n",
      " [1.80031038 6.84150822 2.07615692 ... 6.80960372 2.61137472 5.56282555]\n",
      " ...\n",
      " [6.47374548 5.92147854 4.71589133 ... 3.07834628 7.14946571 3.65718823]\n",
      " [4.18681991 0.         6.89528954 ... 7.44647839 3.84585753 2.88785624]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.13601649e-07 2.07039963e-06 1.33477437e-03 ... 2.02901640e-01\n",
      "  2.72280597e-04 2.36437694e-02]\n",
      " [1.39370461e-01 4.40380727e-06 5.14729842e-01 ... 2.38636219e-03\n",
      "  4.90489903e-03 6.50653087e-03]\n",
      " [1.09517279e-03 7.11318816e-05 4.94549032e-02 ... 1.13020295e-02\n",
      "  3.30226042e-01 1.70211699e-02]\n",
      " ...\n",
      " [5.63120584e-01 2.43297323e-01 1.24547586e-02 ... 4.19130108e-03\n",
      "  1.73237629e-04 3.85606588e-02]\n",
      " [1.89846385e-02 5.07597821e-04 2.20410973e-01 ... 1.41341391e-01\n",
      "  3.97795185e-02 3.91278461e-02]\n",
      " [2.47245350e-01 2.44921336e-01 2.43030887e-02 ... 1.35805809e-02\n",
      "  4.55445765e-03 3.51654856e-01]] & cached\n",
      "Re-used Cached Value, runNum =  288\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  288\n",
      "Provided input from cache for runNum = 288\n",
      "Provided input from cache for runNum = 289\n",
      "activation = [[ 5.54493268  5.65928079  6.27330823 ... 10.68818426  4.81569744\n",
      "   3.41435796]\n",
      " [ 0.          3.32380204  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17900558  3.88979535  1.08389624 ...  2.21351803  4.95322193\n",
      "   1.85780907]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.83564221  2.174637    2.5391245  ...  0.80979828  6.63204817\n",
      "   0.        ]\n",
      " [ 4.23027734  7.81286239  0.         ...  3.35210052  2.22921423\n",
      "   2.40207993]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34891937 0.         1.0629443  ... 0.         0.77579502 0.        ]\n",
      " [1.80298824 6.84915495 2.07410166 ... 6.83306365 2.61997595 5.57796184]\n",
      " ...\n",
      " [6.48829315 5.94754815 4.73220468 ... 3.09782035 7.16756202 3.67058123]\n",
      " [4.19194463 0.         6.9052671  ... 7.46603385 3.85198477 2.88987954]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.82478160e-07 1.98889053e-06 1.29033626e-03 ... 2.01530949e-01\n",
      "  2.67092060e-04 2.33883614e-02]\n",
      " [1.38425133e-01 4.31182517e-06 5.16696713e-01 ... 2.34022055e-03\n",
      "  4.82842679e-03 6.40484872e-03]\n",
      " [1.09109864e-03 7.04973028e-05 4.95012746e-02 ... 1.12593150e-02\n",
      "  3.29854077e-01 1.68352598e-02]\n",
      " ...\n",
      " [5.64746722e-01 2.43255898e-01 1.24098892e-02 ... 4.17689744e-03\n",
      "  1.70470481e-04 3.84221981e-02]\n",
      " [1.89141343e-02 5.05824155e-04 2.19712459e-01 ... 1.41469048e-01\n",
      "  3.95618596e-02 3.89754187e-02]\n",
      " [2.46855578e-01 2.45423675e-01 2.42041364e-02 ... 1.35952933e-02\n",
      "  4.51674067e-03 3.52947335e-01]] & cached\n",
      "Re-used Cached Value, runNum =  289\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  289\n",
      "Provided input from cache for runNum = 289\n",
      "Provided input from cache for runNum = 290\n",
      "activation = [[ 5.54962334  5.66113072  6.27503016 ... 10.70029297  4.81954407\n",
      "   3.41665114]\n",
      " [ 0.          3.32453993  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17809536  3.88782923  1.08099976 ...  2.21612396  4.95859821\n",
      "   1.8589431 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.84077538  2.1873009   2.54790019 ...  0.81494569  6.64267901\n",
      "   0.        ]\n",
      " [ 4.23765829  7.82433761  0.         ...  3.35843426  2.23415053\n",
      "   2.4087292 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35055776 0.         1.07067666 ... 0.         0.77731296 0.        ]\n",
      " [1.80565656 6.85677119 2.07203057 ... 6.85642324 2.62853168 5.59304311]\n",
      " ...\n",
      " [6.5027798  5.97352435 4.74846192 ... 3.11727442 7.18560451 3.68395207]\n",
      " [4.19719147 0.         6.91521521 ... 7.48554809 3.85813988 2.89193639]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.52886362e-07 1.91135084e-06 1.24748911e-03 ... 2.00192643e-01\n",
      "  2.62026093e-04 2.31389462e-02]\n",
      " [1.37522262e-01 4.22233783e-06 5.18672649e-01 ... 2.29495752e-03\n",
      "  4.75354565e-03 6.30512381e-03]\n",
      " [1.08747755e-03 6.98900106e-05 4.95541682e-02 ... 1.12197156e-02\n",
      "  3.29471839e-01 1.66538614e-02]\n",
      " ...\n",
      " [5.66305605e-01 2.43168920e-01 1.23628492e-02 ... 4.16154181e-03\n",
      "  1.67707706e-04 3.82778665e-02]\n",
      " [1.88459107e-02 5.04075466e-04 2.18983078e-01 ... 1.41578580e-01\n",
      "  3.93412312e-02 3.88222735e-02]\n",
      " [2.46481944e-01 2.45926104e-01 2.41038281e-02 ... 1.36081537e-02\n",
      "  4.47889687e-03 3.54212626e-01]] & cached\n",
      "Re-used Cached Value, runNum =  290\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  290\n",
      "Provided input from cache for runNum = 290\n",
      "Provided input from cache for runNum = 291\n",
      "activation = [[ 5.5543456   5.66303962  6.27676418 ... 10.71242122  4.82339674\n",
      "   3.41896368]\n",
      " [ 0.          3.32525868  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17711988  3.8857575   1.07807995 ...  2.21861418  4.9638646\n",
      "   1.8600284 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.84581613  2.19980889  2.55658295 ...  0.81993985  6.6532017\n",
      "   0.        ]\n",
      " [ 4.24500752  7.83578656  0.         ...  3.36480143  2.23906756\n",
      "   2.41536247]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35186519 0.         1.07809922 ... 0.         0.77857883 0.        ]\n",
      " [1.80836675 6.86440634 2.069979   ... 6.87973405 2.63707506 5.60806315]\n",
      " ...\n",
      " [6.51705654 5.99927395 4.76455751 ... 3.13655124 7.20348175 3.69726824]\n",
      " [4.20235682 0.         6.92503564 ... 7.50485925 3.86417616 2.89393192]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.24643912e-07 1.83755507e-06 1.20639222e-03 ... 1.98847195e-01\n",
      "  2.57168485e-04 2.28927760e-02]\n",
      " [1.36594869e-01 4.13584134e-06 5.20613443e-01 ... 2.25059885e-03\n",
      "  4.68004889e-03 6.20735222e-03]\n",
      " [1.08345720e-03 6.92813244e-05 4.95961559e-02 ... 1.11761746e-02\n",
      "  3.29065044e-01 1.64728069e-02]\n",
      " ...\n",
      " [5.67901446e-01 2.43149926e-01 1.23180800e-02 ... 4.14720042e-03\n",
      "  1.65013511e-04 3.81346270e-02]\n",
      " [1.87785935e-02 5.02470280e-04 2.18277248e-01 ... 1.41684146e-01\n",
      "  3.91271318e-02 3.86713664e-02]\n",
      " [2.46099882e-01 2.46482718e-01 2.40058145e-02 ... 1.36224716e-02\n",
      "  4.44202899e-03 3.55482207e-01]] & cached\n",
      "Re-used Cached Value, runNum =  291\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  291\n",
      "Provided input from cache for runNum = 291\n",
      "Provided input from cache for runNum = 292\n",
      "activation = [[ 5.55904822  5.66492135  6.27848079 ... 10.72446751  4.82719162\n",
      "   3.4212558 ]\n",
      " [ 0.          3.32604422  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17617853  3.8837458   1.07520381 ...  2.22118278  4.9691427\n",
      "   1.86114797]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.85081359  2.21221722  2.56521491 ...  0.82488958  6.66367296\n",
      "   0.        ]\n",
      " [ 4.2522997   7.84716318  0.         ...  3.37114324  2.24391346\n",
      "   2.42196151]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35314318 0.         1.08547682 ... 0.         0.77982827 0.        ]\n",
      " [1.81110042 6.87204514 2.0679317  ... 6.90293892 2.64556643 5.62302601]\n",
      " ...\n",
      " [6.53114692 6.02474738 4.78050559 ... 3.15563081 7.22116814 3.71049606]\n",
      " [4.2075319  0.         6.93479496 ... 7.52404967 3.87018073 2.89590779]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.97819913e-07 1.76744439e-06 1.16695175e-03 ... 1.97550978e-01\n",
      "  2.52476274e-04 2.26534828e-02]\n",
      " [1.35682158e-01 4.05123971e-06 5.22543020e-01 ... 2.20711325e-03\n",
      "  4.60823832e-03 6.11140893e-03]\n",
      " [1.07972681e-03 6.86900336e-05 4.96451938e-02 ... 1.11345543e-02\n",
      "  3.28661586e-01 1.62960391e-02]\n",
      " ...\n",
      " [5.69474227e-01 2.43100970e-01 1.22725673e-02 ... 4.13253008e-03\n",
      "  1.62364594e-04 3.79878920e-02]\n",
      " [1.87122418e-02 5.00845174e-04 2.17557600e-01 ... 1.41768792e-01\n",
      "  3.89140610e-02 3.85199349e-02]\n",
      " [2.45718127e-01 2.47015622e-01 2.39068208e-02 ... 1.36354393e-02\n",
      "  4.40553635e-03 3.56725689e-01]] & cached\n",
      "Re-used Cached Value, runNum =  292\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  292\n",
      "Provided input from cache for runNum = 292\n",
      "Provided input from cache for runNum = 293\n",
      "activation = [[ 5.5637538   5.66679953  6.28019411 ... 10.73643633  4.83095633\n",
      "   3.42353822]\n",
      " [ 0.          3.32680958  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17524663  3.8817381   1.07235431 ...  2.22378749  4.97438032\n",
      "   1.86228542]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.85576821  2.224561    2.57379375 ...  0.82982303  6.67409848\n",
      "   0.        ]\n",
      " [ 4.2595783   7.85852952  0.         ...  3.3775443   2.2487598\n",
      "   2.42856406]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35442373 0.         1.09284436 ... 0.         0.78111842 0.        ]\n",
      " [1.81399208 6.87978265 2.06596692 ... 6.92616802 2.65412936 5.63799741]\n",
      " ...\n",
      " [6.54520245 6.05021115 4.79640377 ... 3.17477448 7.23887948 3.7237439 ]\n",
      " [4.212787   0.         6.94454172 ... 7.54313978 3.87621635 2.89789302]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.72055227e-07 1.69974830e-06 1.12884258e-03 ... 1.96242255e-01\n",
      "  2.47868569e-04 2.24156872e-02]\n",
      " [1.34762230e-01 3.96805277e-06 5.24463491e-01 ... 2.16467130e-03\n",
      "  4.53729863e-03 6.01709724e-03]\n",
      " [1.07594541e-03 6.80896619e-05 4.96948747e-02 ... 1.10945389e-02\n",
      "  3.28252708e-01 1.61213207e-02]\n",
      " ...\n",
      " [5.71050726e-01 2.43051565e-01 1.22284772e-02 ... 4.11925857e-03\n",
      "  1.59758786e-04 3.78419516e-02]\n",
      " [1.86443078e-02 4.99120252e-04 2.16828992e-01 ... 1.41851625e-01\n",
      "  3.87000895e-02 3.83667308e-02]\n",
      " [2.45338382e-01 2.47546874e-01 2.38097538e-02 ... 1.36519112e-02\n",
      "  4.36943017e-03 3.57973041e-01]] & cached\n",
      "Re-used Cached Value, runNum =  293\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  293\n",
      "Provided input from cache for runNum = 293\n",
      "Provided input from cache for runNum = 294\n",
      "activation = [[ 5.56846284  5.66868333  6.28189464 ... 10.74833894  4.83468612\n",
      "   3.42581138]\n",
      " [ 0.          3.32755775  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17431359  3.87972221  1.06953591 ...  2.22640202  4.97957896\n",
      "   1.86342589]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.86068064  2.2368429   2.58232483 ...  0.83473072  6.68447472\n",
      "   0.        ]\n",
      " [ 4.26681772  7.86985218  0.         ...  3.38396613  2.25358202\n",
      "   2.43515156]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3558166  0.         1.10029212 ... 0.         0.78256855 0.        ]\n",
      " [1.81691831 6.88748931 2.06401915 ... 6.94935062 2.66267922 5.65291323]\n",
      " ...\n",
      " [6.55915122 6.07559211 4.81221441 ... 3.1938825  7.25651218 3.73696945]\n",
      " [4.21811292 0.         6.95426068 ... 7.56213739 3.88225465 2.89988047]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.47263852e-07 1.63448476e-06 1.09192520e-03 ... 1.94934254e-01\n",
      "  2.43347598e-04 2.21800637e-02]\n",
      " [1.33849721e-01 3.88680562e-06 5.26397638e-01 ... 2.12321992e-03\n",
      "  4.46771355e-03 5.92475400e-03]\n",
      " [1.07209915e-03 6.74824361e-05 4.97430134e-02 ... 1.10553058e-02\n",
      "  3.27845431e-01 1.59485132e-02]\n",
      " ...\n",
      " [5.72630011e-01 2.43027755e-01 1.21852318e-02 ... 4.10705893e-03\n",
      "  1.57198013e-04 3.76983098e-02]\n",
      " [1.85752962e-02 4.97371713e-04 2.16082532e-01 ... 1.41938845e-01\n",
      "  3.84851395e-02 3.82146104e-02]\n",
      " [2.44949783e-01 2.48081334e-01 2.37133996e-02 ... 1.36702667e-02\n",
      "  4.33370152e-03 3.59223732e-01]] & cached\n",
      "Re-used Cached Value, runNum =  294\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  294\n",
      "Provided input from cache for runNum = 294\n",
      "Provided input from cache for runNum = 295\n",
      "activation = [[ 5.5731651   5.67054841  6.2835909  ... 10.76017033  4.83835524\n",
      "   3.4280696 ]\n",
      " [ 0.          3.32831691  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17337972  3.87771621  1.0667288  ...  2.22900197  4.9847465\n",
      "   1.86457738]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.86555593  2.2490457   2.59079655 ...  0.83958472  6.69479849\n",
      "   0.        ]\n",
      " [ 4.27405646  7.88120476  0.         ...  3.39045508  2.25845158\n",
      "   2.44174673]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35710366 0.         1.10764343 ... 0.         0.78395969 0.        ]\n",
      " [1.81987496 6.89521722 2.06208365 ... 6.97248364 2.67122974 5.66780468]\n",
      " ...\n",
      " [6.57304847 6.10091085 4.82798399 ... 3.2130241  7.274128   3.75020334]\n",
      " [4.2234096  0.         6.96392156 ... 7.58096037 3.88817724 2.90181821]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.23568375e-07 1.57204370e-06 1.05628281e-03 ... 1.93620413e-01\n",
      "  2.38922435e-04 2.19474865e-02]\n",
      " [1.32946321e-01 3.80756710e-06 5.28329854e-01 ... 2.08275491e-03\n",
      "  4.39897504e-03 5.83403083e-03]\n",
      " [1.06834667e-03 6.68872946e-05 4.97892500e-02 ... 1.10168385e-02\n",
      "  3.27415064e-01 1.57781627e-02]\n",
      " ...\n",
      " [5.74179511e-01 2.42978110e-01 1.21413950e-02 ... 4.09512401e-03\n",
      "  1.54675523e-04 3.75513633e-02]\n",
      " [1.85080088e-02 4.95636545e-04 2.15330854e-01 ... 1.42021138e-01\n",
      "  3.82702446e-02 3.80622138e-02]\n",
      " [2.44576315e-01 2.48623253e-01 2.36173533e-02 ... 1.36904021e-02\n",
      "  4.29846393e-03 3.60471393e-01]] & cached\n",
      "Re-used Cached Value, runNum =  295\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  295\n",
      "Provided input from cache for runNum = 295\n",
      "Provided input from cache for runNum = 296\n",
      "activation = [[ 5.57789275  5.6724566   6.28529726 ... 10.77199841  4.84202251\n",
      "   3.43035032]\n",
      " [ 0.          3.32905464  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17237045  3.87559958  1.06388514 ...  2.2314789   4.98979858\n",
      "   1.86567762]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.87043328  2.26125313  2.59923219 ...  0.84450552  6.70514977\n",
      "   0.        ]\n",
      " [ 4.28124563  7.89248975  0.         ...  3.39692126  2.26327031\n",
      "   2.44830752]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35812531 0.         1.11473078 ... 0.         0.7851716  0.        ]\n",
      " [1.82278236 6.90282093 2.06010785 ... 6.99542003 2.67967995 5.68261822]\n",
      " ...\n",
      " [6.58696424 6.12630587 4.84378993 ... 3.23237141 7.29185444 3.76348169]\n",
      " [4.22877903 0.         6.9735671  ... 7.59978709 3.89416442 2.90380179]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.00957203e-07 1.51225351e-06 1.02186637e-03 ... 1.92279305e-01\n",
      "  2.34584354e-04 2.17179247e-02]\n",
      " [1.32066541e-01 3.73172350e-06 5.30257247e-01 ... 2.04372714e-03\n",
      "  4.33125846e-03 5.74575549e-03]\n",
      " [1.06472224e-03 6.63125116e-05 4.98318957e-02 ... 1.09801576e-02\n",
      "  3.26978391e-01 1.56105683e-02]\n",
      " ...\n",
      " [5.75706189e-01 2.42940273e-01 1.20964669e-02 ... 4.08268247e-03\n",
      "  1.52138009e-04 3.74020940e-02]\n",
      " [1.84457935e-02 4.94061106e-04 2.14591934e-01 ... 1.42119435e-01\n",
      "  3.80563385e-02 3.79138838e-02]\n",
      " [2.44196281e-01 2.49202875e-01 2.35192016e-02 ... 1.37098894e-02\n",
      "  4.26244462e-03 3.61704829e-01]] & cached\n",
      "Re-used Cached Value, runNum =  296\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  296\n",
      "Provided input from cache for runNum = 296\n",
      "Provided input from cache for runNum = 297\n",
      "activation = [[ 5.58260872  5.67435625  6.28699257 ... 10.783765    4.84562904\n",
      "   3.43262348]\n",
      " [ 0.          3.32980475  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17136299  3.8734663   1.06106173 ...  2.23395176  4.99482982\n",
      "   1.86677439]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.87531204  2.27342565  2.60763019 ...  0.84942593  6.71551861\n",
      "   0.        ]\n",
      " [ 4.28840827  7.90374851  0.         ...  3.40341032  2.26806319\n",
      "   2.45486556]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35922923 0.         1.12186975 ... 0.         0.78649452 0.        ]\n",
      " [1.82569229 6.91039929 2.05813267 ... 7.01825892 2.68807208 5.69739403]\n",
      " ...\n",
      " [6.60073266 6.15152646 4.85945215 ... 3.25157762 7.3094732  3.7766856 ]\n",
      " [4.23413899 0.         6.98315869 ... 7.61848767 3.9000971  2.90575035]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.79333724e-07 1.45510813e-06 9.88712137e-04 ... 1.90970959e-01\n",
      "  2.30351543e-04 2.14937374e-02]\n",
      " [1.31199423e-01 3.65780892e-06 5.32193994e-01 ... 2.00559487e-03\n",
      "  4.26490704e-03 5.65905090e-03]\n",
      " [1.06120314e-03 6.57436560e-05 4.98748918e-02 ... 1.09439267e-02\n",
      "  3.26539463e-01 1.54452113e-02]\n",
      " ...\n",
      " [5.77224057e-01 2.42901989e-01 1.20517051e-02 ... 4.07061978e-03\n",
      "  1.49634076e-04 3.72531303e-02]\n",
      " [1.83829066e-02 4.92461212e-04 2.13831149e-01 ... 1.42202324e-01\n",
      "  3.78408299e-02 3.77652542e-02]\n",
      " [2.43810314e-01 2.49773225e-01 2.34206995e-02 ... 1.37295521e-02\n",
      "  4.22661380e-03 3.62927137e-01]] & cached\n",
      "Re-used Cached Value, runNum =  297\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  297\n",
      "Provided input from cache for runNum = 297\n",
      "Provided input from cache for runNum = 298\n",
      "activation = [[ 5.58732125  5.67626202  6.28867419 ... 10.79547449  4.84920458\n",
      "   3.43489324]\n",
      " [ 0.          3.33053251  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17033134  3.87130155  1.05825291 ...  2.23641303  4.99982278\n",
      "   1.86786451]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.88017601  2.28554538  2.61598847 ...  0.85436437  6.72586935\n",
      "   0.        ]\n",
      " [ 4.29553866  7.91497794  0.         ...  3.40992509  2.27283303\n",
      "   2.46141109]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36026274 0.         1.12891112 ... 0.         0.78771051 0.        ]\n",
      " [1.82862799 6.91798143 2.05616313 ... 7.04101087 2.69645308 5.71214015]\n",
      " ...\n",
      " [6.61441481 6.17664539 4.87502237 ... 3.27077448 7.32703908 3.78986606]\n",
      " [4.23949159 0.         6.99267743 ... 7.63705712 3.90598562 2.90767209]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.58701454e-07 1.40056968e-06 9.56885100e-04 ... 1.89684094e-01\n",
      "  2.26239840e-04 2.12744024e-02]\n",
      " [1.30323989e-01 3.58522872e-06 5.34084645e-01 ... 1.96810457e-03\n",
      "  4.19949575e-03 5.57356940e-03]\n",
      " [1.05786324e-03 6.51906224e-05 4.99240400e-02 ... 1.09098509e-02\n",
      "  3.26092215e-01 1.52830559e-02]\n",
      " ...\n",
      " [5.78750779e-01 2.42858821e-01 1.20078758e-02 ... 4.05897855e-03\n",
      "  1.47173859e-04 3.71041924e-02]\n",
      " [1.83231494e-02 4.90944936e-04 2.13093892e-01 ... 1.42296152e-01\n",
      "  3.76279054e-02 3.76194382e-02]\n",
      " [2.43416103e-01 2.50327423e-01 2.33225217e-02 ... 1.37493986e-02\n",
      "  4.19114154e-03 3.64135730e-01]] & cached\n",
      "Re-used Cached Value, runNum =  298\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  298\n",
      "Provided input from cache for runNum = 298\n",
      "Provided input from cache for runNum = 299\n",
      "activation = [[ 5.59207055  5.67824439  6.29036767 ... 10.80721337  4.8528016\n",
      "   3.43717878]\n",
      " [ 0.          3.33122044  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16923659  3.86900023  1.05541486 ...  2.23875561  5.00467836\n",
      "   1.86891611]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.8849698   2.29751271  2.62427203 ...  0.85919509  6.73612242\n",
      "   0.        ]\n",
      " [ 4.30269157  7.92622658  0.         ...  3.41650013  2.27761081\n",
      "   2.46797383]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36107255 0.         1.13577135 ... 0.         0.78884502 0.        ]\n",
      " [1.83157883 6.92553364 2.05417724 ... 7.06367316 2.70478503 5.72682279]\n",
      " ...\n",
      " [6.62789794 6.20150503 4.89041423 ... 3.28975389 7.3444068  3.80295798]\n",
      " [4.24478996 0.         7.00211759 ... 7.65547999 3.91180159 2.90953609]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.38876709e-07 1.34834780e-06 9.26243963e-04 ... 1.88396444e-01\n",
      "  2.22278533e-04 2.10581482e-02]\n",
      " [1.29422367e-01 3.51480411e-06 5.35956481e-01 ... 1.93140543e-03\n",
      "  4.13552376e-03 5.48968271e-03]\n",
      " [1.05407894e-03 6.46246968e-05 4.99623023e-02 ... 1.08721631e-02\n",
      "  3.25642768e-01 1.51210155e-02]\n",
      " ...\n",
      " [5.80334890e-01 2.42919137e-01 1.19667113e-02 ... 4.04896460e-03\n",
      "  1.44779561e-04 3.69601881e-02]\n",
      " [1.82632613e-02 4.89560135e-04 2.12376390e-01 ... 1.42400506e-01\n",
      "  3.74187688e-02 3.74766516e-02]\n",
      " [2.42997358e-01 2.50924916e-01 2.32255117e-02 ... 1.37707590e-02\n",
      "  4.15645926e-03 3.65353027e-01]] & cached\n",
      "Re-used Cached Value, runNum =  299\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  299\n",
      "Provided input from cache for runNum = 299\n",
      "Provided input from cache for runNum = 300\n",
      "activation = [[ 5.59681826  5.68022605  6.29205434 ... 10.8189151   4.85635544\n",
      "   3.43946024]\n",
      " [ 0.          3.33191636  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16814118  3.86668069  1.05258254 ...  2.24109272  5.00949479\n",
      "   1.86997235]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.88982621  2.30955705  2.63258574 ...  0.86415424  6.74647884\n",
      "   0.        ]\n",
      " [ 4.30975842  7.9373402   0.         ...  3.42297121  2.28229701\n",
      "   2.47445877]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36195444 0.         1.14265658 ... 0.         0.79013829 0.        ]\n",
      " [1.83440945 6.93289208 2.05209312 ... 7.08608037 2.71293044 5.7413995 ]\n",
      " ...\n",
      " [6.6413372  6.22634106 4.90576288 ... 3.30874745 7.36177265 3.8159994 ]\n",
      " [4.2501512  0.         7.01152976 ... 7.6739164  3.91764001 2.91143109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.20090639e-07 1.29882255e-06 8.96622040e-04 ... 1.87136651e-01\n",
      "  2.18369315e-04 2.08485943e-02]\n",
      " [1.28610517e-01 3.44816005e-06 5.37898592e-01 ... 1.89605192e-03\n",
      "  4.07317737e-03 5.40839928e-03]\n",
      " [1.05096120e-03 6.41014640e-05 5.00033806e-02 ... 1.08370303e-02\n",
      "  3.25183781e-01 1.49630311e-02]\n",
      " ...\n",
      " [5.81781386e-01 2.42895957e-01 1.19192523e-02 ... 4.03600815e-03\n",
      "  1.42331386e-04 3.68049043e-02]\n",
      " [1.82063215e-02 4.88250540e-04 2.11590670e-01 ... 1.42463865e-01\n",
      "  3.72009128e-02 3.73312212e-02]\n",
      " [2.42614878e-01 2.51546428e-01 2.31230526e-02 ... 1.37873854e-02\n",
      "  4.12056590e-03 3.66527909e-01]] & cached\n",
      "Re-used Cached Value, runNum =  300\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  300\n",
      "Provided input from cache for runNum = 300\n",
      "Provided input from cache for runNum = 301\n",
      "activation = [[ 5.60153136  5.68214013  6.29371945 ... 10.83050093  4.85982391\n",
      "   3.44170987]\n",
      " [ 0.          3.33262856  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1670639   3.8644094   1.04976804 ...  2.24347181  5.01431778\n",
      "   1.8710557 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.89468095  2.32155574  2.64085397 ...  0.86909708  6.7568432\n",
      "   0.        ]\n",
      " [ 4.31682964  7.94847229  0.         ...  3.42949772  2.28698664\n",
      "   2.48094198]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36268283 0.         1.14940001 ... 0.         0.7913493  0.        ]\n",
      " [1.83724898 6.94023364 2.05001332 ... 7.10840925 2.72103056 5.75592822]\n",
      " ...\n",
      " [6.65467739 6.25103621 4.92101858 ... 3.32768835 7.37907531 3.8289962 ]\n",
      " [4.25542412 0.         7.02084618 ... 7.69211064 3.92332823 2.91324833]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.02041837e-07 1.25107890e-06 8.68119370e-04 ... 1.85894653e-01\n",
      "  2.14544029e-04 2.06420000e-02]\n",
      " [1.27764459e-01 3.38168868e-06 5.39799785e-01 ... 1.86135403e-03\n",
      "  4.01149383e-03 5.32806259e-03]\n",
      " [1.04783907e-03 6.35788828e-05 5.00490685e-02 ... 1.08045490e-02\n",
      "  3.24717095e-01 1.48077783e-02]\n",
      " ...\n",
      " [5.83274990e-01 2.42840045e-01 1.18750658e-02 ... 4.02474130e-03\n",
      "  1.39962973e-04 3.66530904e-02]\n",
      " [1.81482087e-02 4.86805176e-04 2.10821345e-01 ... 1.42532714e-01\n",
      "  3.69858231e-02 3.71856337e-02]\n",
      " [2.42212803e-01 2.52094488e-01 2.30235369e-02 ... 1.38074024e-02\n",
      "  4.08554259e-03 3.67701526e-01]] & cached\n",
      "Re-used Cached Value, runNum =  301\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  301\n",
      "Provided input from cache for runNum = 301\n",
      "Provided input from cache for runNum = 302\n",
      "activation = [[ 5.60623196  5.68403071  6.29537999 ... 10.84201768  4.86324268\n",
      "   3.44394537]\n",
      " [ 0.          3.33336203  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16600756  3.86217003  1.04699095 ...  2.24587234  5.01912531\n",
      "   1.87215493]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.89953805  2.33355178  2.64907172 ...  0.87408563  6.76720675\n",
      "   0.        ]\n",
      " [ 4.32386675  7.95956685  0.         ...  3.43603505  2.29165472\n",
      "   2.48740306]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36330868 0.         1.15602286 ... 0.         0.79252065 0.        ]\n",
      " [1.84007992 6.94753723 2.04794293 ... 7.13059269 2.72907723 5.77040661]\n",
      " ...\n",
      " [6.66791032 6.27563171 4.936171   ... 3.34658986 7.39631817 3.84194549]\n",
      " [4.26061712 0.         7.03005122 ... 7.71008497 3.92889346 2.91499488]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.84782586e-07 1.20546536e-06 8.40742521e-04 ... 1.84667529e-01\n",
      "  2.10805737e-04 2.04393056e-02]\n",
      " [1.26906229e-01 3.31651813e-06 5.41650705e-01 ... 1.82722544e-03\n",
      "  3.95075294e-03 5.24883812e-03]\n",
      " [1.04482830e-03 6.30744104e-05 5.00983714e-02 ... 1.07739915e-02\n",
      "  3.24233579e-01 1.46547870e-02]\n",
      " ...\n",
      " [5.84775511e-01 2.42767474e-01 1.18323117e-02 ... 4.01398849e-03\n",
      "  1.37639736e-04 3.65012171e-02]\n",
      " [1.80919646e-02 4.85410448e-04 2.10077470e-01 ... 1.42613259e-01\n",
      "  3.67718980e-02 3.70415890e-02]\n",
      " [2.41810628e-01 2.52632280e-01 2.29257749e-02 ... 1.38281803e-02\n",
      "  4.05107063e-03 3.68869888e-01]] & cached\n",
      "Re-used Cached Value, runNum =  302\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  302\n",
      "Provided input from cache for runNum = 302\n",
      "Provided input from cache for runNum = 303\n",
      "activation = [[5.61093555e+00 5.68592935e+00 6.29702719e+00 ... 1.08534853e+01\n",
      "  4.86662825e+00 3.44617507e+00]\n",
      " [0.00000000e+00 3.33406765e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.64873446e-01 3.85983010e+00 1.04417506e+00 ... 2.24819376e+00\n",
      "  5.02382638e+00 1.87321894e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.90436207e+00 2.34546462e+00 2.65723850e+00 ... 8.79055934e-01\n",
      "  6.77752739e+00 8.79152392e-04]\n",
      " [4.33090600e+00 7.97067138e+00 0.00000000e+00 ... 3.44266433e+00\n",
      "  2.29634331e+00 2.49386946e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36398546 0.         1.16265029 ... 0.         0.79379012 0.        ]\n",
      " [1.84288139 6.9547523  2.0458492  ... 7.15266882 2.73707033 5.78439081]\n",
      " ...\n",
      " [6.68100896 6.30006784 4.95121483 ... 3.36540387 7.41345142 3.855492  ]\n",
      " [4.2658463  0.         7.03922746 ... 7.72792132 3.93441045 2.91684949]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.68003654e-07 1.16111312e-06 8.14160392e-04 ... 1.83425047e-01\n",
      "  2.07141894e-04 2.02309859e-02]\n",
      " [1.26036213e-01 3.25265402e-06 5.43515989e-01 ... 1.79401558e-03\n",
      "  3.89118074e-03 5.17350363e-03]\n",
      " [1.04142613e-03 6.25447448e-05 5.01402775e-02 ... 1.07431131e-02\n",
      "  3.23760957e-01 1.45147489e-02]\n",
      " ...\n",
      " [5.86323500e-01 2.42765495e-01 1.17928017e-02 ... 4.00586059e-03\n",
      "  1.35390270e-04 3.63418096e-02]\n",
      " [1.80307902e-02 4.83901029e-04 2.09320487e-01 ... 1.42703362e-01\n",
      "  3.65587439e-02 3.69115861e-02]\n",
      " [2.41382619e-01 2.53175853e-01 2.28303340e-02 ... 1.38536341e-02\n",
      "  4.01745367e-03 3.70002812e-01]] & cached\n",
      "Re-used Cached Value, runNum =  303\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  303\n",
      "Provided input from cache for runNum = 303\n",
      "iterations = 300\n",
      "Accuracy = 0.6399756097560976\n",
      "Provided input from cache for runNum = 304\n",
      "activation = [[5.61565103e+00 5.68783369e+00 6.29867298e+00 ... 1.08649052e+01\n",
      "  4.86999992e+00 3.44840144e+00]\n",
      " [0.00000000e+00 3.33478847e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.63760491e-01 3.85752667e+00 1.04138931e+00 ... 2.25054533e+00\n",
      "  5.02849551e+00 1.87430438e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.90921023e+00 2.35741656e+00 2.66540263e+00 ... 8.84116374e-01\n",
      "  6.78787177e+00 7.30602510e-03]\n",
      " [4.33792464e+00 7.98174832e+00 0.00000000e+00 ... 3.44934364e+00\n",
      "  2.30099851e+00 2.50033125e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36453514 0.         1.16916413 ... 0.         0.79506839 0.        ]\n",
      " [1.84569324 6.96195763 2.04376419 ... 7.17463792 2.74501476 5.79566665]\n",
      " ...\n",
      " [6.69407943 6.3245002  4.96625145 ... 3.38431447 7.43059876 3.87311535]\n",
      " [4.27107799 0.         7.04835647 ... 7.74562098 3.93990262 2.91941811]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.51992959e-07 1.11872573e-06 7.88582764e-04 ... 1.82187342e-01\n",
      "  2.03539790e-04 1.99896421e-02]\n",
      " [1.25168920e-01 3.19016948e-06 5.45338148e-01 ... 1.76136674e-03\n",
      "  3.83244589e-03 5.11386921e-03]\n",
      " [1.03827432e-03 6.20370721e-05 5.01895137e-02 ... 1.07151045e-02\n",
      "  3.23277564e-01 1.44573785e-02]\n",
      " ...\n",
      " [5.87845278e-01 2.42729778e-01 1.17529241e-02 ... 3.99783283e-03\n",
      "  1.33157826e-04 3.60866035e-02]\n",
      " [1.79730476e-02 4.82467531e-04 2.08581429e-01 ... 1.42804184e-01\n",
      "  3.63434700e-02 3.68725199e-02]\n",
      " [2.40969523e-01 2.53726296e-01 2.27353890e-02 ... 1.38797214e-02\n",
      "  3.98388449e-03 3.70774369e-01]] & cached\n",
      "Re-used Cached Value, runNum =  304\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  304\n",
      "Provided input from cache for runNum = 304\n",
      "Provided input from cache for runNum = 305\n",
      "activation = [[ 5.62035569  5.68973306  6.30030397 ... 10.87626037  4.87332989\n",
      "   3.45061726]\n",
      " [ 0.          3.33554228  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16267572  3.85526526  1.03864185 ...  2.25295414  5.03317312\n",
      "   1.87541036]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.91406552  2.36935368  2.67353077 ...  0.88918841  6.79823457\n",
      "   0.0137109 ]\n",
      " [ 4.3449301   7.99278904  0.         ...  3.45603786  2.30560828\n",
      "   2.50678904]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36501385 0.         1.17558892 ... 0.         0.79629434 0.        ]\n",
      " [1.84860657 6.96926187 2.04174314 ... 7.19657557 2.7529601  5.80678395]\n",
      " ...\n",
      " [6.707043   6.34879583 4.981169   ... 3.40312026 7.44765237 3.89068828]\n",
      " [4.27625664 0.         7.05740198 ... 7.76315401 3.94533786 2.92196541]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.36743449e-07 1.07847659e-06 7.64037714e-04 ... 1.80979101e-01\n",
      "  2.00051947e-04 1.97470725e-02]\n",
      " [1.24310808e-01 3.12927885e-06 5.47151084e-01 ... 1.72928758e-03\n",
      "  3.77482548e-03 5.05623211e-03]\n",
      " [1.03525702e-03 6.15472440e-05 5.02398062e-02 ... 1.06872891e-02\n",
      "  3.22773629e-01 1.43976340e-02]\n",
      " ...\n",
      " [5.89329912e-01 2.42660935e-01 1.17124404e-02 ... 3.98933128e-03\n",
      "  1.30951334e-04 3.58392395e-02]\n",
      " [1.79151716e-02 4.81051083e-04 2.07821752e-01 ... 1.42869355e-01\n",
      "  3.61283243e-02 3.68278873e-02]\n",
      " [2.40579950e-01 2.54295759e-01 2.26413722e-02 ... 1.39055159e-02\n",
      "  3.95065870e-03 3.71574961e-01]] & cached\n",
      "Re-used Cached Value, runNum =  305\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  305\n",
      "Provided input from cache for runNum = 305\n",
      "Provided input from cache for runNum = 306\n",
      "activation = [[ 5.62507689  5.69168179  6.30193631 ... 10.88761406  4.87666189\n",
      "   3.45285029]\n",
      " [ 0.          3.33626461  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16159268  3.85297007  1.03590384 ...  2.25534898  5.03781606\n",
      "   1.87650581]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.91885069  2.38113428  2.68158477 ...  0.89415657  6.80850857\n",
      "   0.02003495]\n",
      " [ 4.35185557  8.00371698  0.         ...  3.4626518   2.31013768\n",
      "   2.51318759]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36537005 0.         1.18188253 ... 0.         0.79745164 0.        ]\n",
      " [1.85151719 6.97654569 2.03971831 ... 7.21842836 2.76087247 5.81769429]\n",
      " ...\n",
      " [6.7198584  6.37289772 4.9959865  ... 3.42182766 7.46457391 3.90819603]\n",
      " [4.2815298  0.         7.0664383  ... 7.78071825 3.95086758 2.92460268]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.22202861e-07 1.04011327e-06 7.40415059e-04 ... 1.79793009e-01\n",
      "  1.96694392e-04 1.95037653e-02]\n",
      " [1.23466862e-01 3.07041124e-06 5.48940373e-01 ... 1.69782275e-03\n",
      "  3.71861323e-03 5.00116683e-03]\n",
      " [1.03230934e-03 6.10653977e-05 5.02885052e-02 ... 1.06590428e-02\n",
      "  3.22276789e-01 1.43356785e-02]\n",
      " ...\n",
      " [5.90817131e-01 2.42631617e-01 1.16717266e-02 ... 3.98028893e-03\n",
      "  1.28774534e-04 3.56032405e-02]\n",
      " [1.78611633e-02 4.79758106e-04 2.07081143e-01 ... 1.42938047e-01\n",
      "  3.59185641e-02 3.67851354e-02]\n",
      " [2.40170147e-01 2.54891559e-01 2.25452704e-02 ... 1.39283121e-02\n",
      "  3.91748155e-03 3.72394065e-01]] & cached\n",
      "Re-used Cached Value, runNum =  306\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  306\n",
      "Provided input from cache for runNum = 306\n",
      "Provided input from cache for runNum = 307\n",
      "activation = [[ 5.62976901  5.69356712  6.30354459 ... 10.89886292  4.87991194\n",
      "   3.45505083]\n",
      " [ 0.          3.33699504  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.16052589  3.85072658  1.03318958 ...  2.25779803  5.04246873\n",
      "   1.87762633]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.92357848  2.3928394   2.68958577 ...  0.89909526  6.81873892\n",
      "   0.02632963]\n",
      " [ 4.3587449   8.01461295  0.         ...  3.46928625  2.31464242\n",
      "   2.51956507]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3656072  0.         1.18805144 ... 0.         0.79855793 0.        ]\n",
      " [1.85448321 6.98382666 2.03773722 ... 7.24021905 2.76876348 5.82856042]\n",
      " ...\n",
      " [6.73249543 6.39677875 5.01065652 ... 3.44036171 7.48133454 3.92559392]\n",
      " [4.28672656 0.         7.07536591 ... 7.7980407  3.95625015 2.927157  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.08296854e-07 1.00340389e-06 7.17743262e-04 ... 1.78642350e-01\n",
      "  1.93438754e-04 1.92653667e-02]\n",
      " [1.22612255e-01 3.01240595e-06 5.50703687e-01 ... 1.66689459e-03\n",
      "  3.66347617e-03 4.94674097e-03]\n",
      " [1.02938254e-03 6.05920888e-05 5.03405908e-02 ... 1.06322010e-02\n",
      "  3.21766470e-01 1.42743523e-02]\n",
      " ...\n",
      " [5.92315301e-01 2.42572122e-01 1.16325269e-02 ... 3.97206026e-03\n",
      "  1.26662692e-04 3.53696077e-02]\n",
      " [1.78058771e-02 4.78389076e-04 2.06340067e-01 ... 1.42991463e-01\n",
      "  3.57108993e-02 3.67395345e-02]\n",
      " [2.39755966e-01 2.55449671e-01 2.24510860e-02 ... 1.39527090e-02\n",
      "  3.88523240e-03 3.73207382e-01]] & cached\n",
      "Re-used Cached Value, runNum =  307\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  307\n",
      "Provided input from cache for runNum = 307\n",
      "Provided input from cache for runNum = 308\n",
      "activation = [[ 5.63446989  5.6954666   6.30515159 ... 10.91009141  4.88314316\n",
      "   3.45725126]\n",
      " [ 0.          3.33773889  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15946256  3.84849292  1.03049776 ...  2.26025661  5.04711375\n",
      "   1.87876131]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.9283121   2.40449633  2.69756415 ...  0.90404091  6.82895398\n",
      "   0.0325932 ]\n",
      " [ 4.3656272   8.02552823  0.         ...  3.47594203  2.31914796\n",
      "   2.52594837]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36580006 0.         1.19417067 ... 0.         0.79962556 0.        ]\n",
      " [1.85747523 6.99117527 2.03576413 ... 7.26197588 2.77666972 5.83941864]\n",
      " ...\n",
      " [6.74504779 6.42056009 5.02524001 ... 3.4588702  7.4980155  3.94293031]\n",
      " [4.29195426 0.         7.08426227 ... 7.81532991 3.96161787 2.92971133]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.94956299e-07 9.68201937e-07 6.95835089e-04 ... 1.77500446e-01\n",
      "  1.90272470e-04 1.90311579e-02]\n",
      " [1.21772282e-01 2.95568658e-06 5.52471545e-01 ... 1.63667758e-03\n",
      "  3.60937055e-03 4.89321584e-03]\n",
      " [1.02652727e-03 6.01246279e-05 5.03915976e-02 ... 1.06058697e-02\n",
      "  3.21250643e-01 1.42130744e-02]\n",
      " ...\n",
      " [5.93795422e-01 2.42497951e-01 1.15930131e-02 ... 3.96373786e-03\n",
      "  1.24597370e-04 3.51365825e-02]\n",
      " [1.77508879e-02 4.76987021e-04 2.05588981e-01 ... 1.43034193e-01\n",
      "  3.55060735e-02 3.66924883e-02]\n",
      " [2.39342639e-01 2.56014259e-01 2.23563733e-02 ... 1.39769816e-02\n",
      "  3.85355207e-03 3.74012779e-01]] & cached\n",
      "Re-used Cached Value, runNum =  308\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  308\n",
      "Provided input from cache for runNum = 308\n",
      "Provided input from cache for runNum = 309\n",
      "activation = [[ 5.6391637   5.69734479  6.30675368 ... 10.92126235  4.88632877\n",
      "   3.4594369 ]\n",
      " [ 0.          3.33849248  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15839221  3.84625507  1.02781367 ...  2.26268951  5.05170797\n",
      "   1.87989074]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.93295595  2.41600129  2.70544946 ...  0.90886086  6.83905598\n",
      "   0.03877442]\n",
      " [ 4.37247019  8.03641611  0.         ...  3.48261223  2.32361792\n",
      "   2.53231404]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36588028 0.         1.2001759  ... 0.         0.80061585 0.        ]\n",
      " [1.86051401 6.99854014 2.03381532 ... 7.28367403 2.78454632 5.85024985]\n",
      " ...\n",
      " [6.75741157 6.44408753 5.03965693 ... 3.47717544 7.51451333 3.96014014]\n",
      " [4.29709495 0.         7.09304527 ... 7.83237974 3.96685715 2.93217362]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.82082292e-07 9.34405987e-07 6.74659250e-04 ... 1.76356190e-01\n",
      "  1.87209934e-04 1.87996682e-02]\n",
      " [1.20915960e-01 2.90015755e-06 5.54233498e-01 ... 1.60713684e-03\n",
      "  3.55640272e-03 4.84041249e-03]\n",
      " [1.02333185e-03 5.96508268e-05 5.04326428e-02 ... 1.05772804e-02\n",
      "  3.20726501e-01 1.41498006e-02]\n",
      " ...\n",
      " [5.95311405e-01 2.42470875e-01 1.15563252e-02 ... 3.95727639e-03\n",
      "  1.22600438e-04 3.49105559e-02]\n",
      " [1.76940661e-02 4.75594817e-04 2.04843414e-01 ... 1.43081467e-01\n",
      "  3.53061881e-02 3.66452008e-02]\n",
      " [2.38915158e-01 2.56588637e-01 2.22641562e-02 ... 1.40046066e-02\n",
      "  3.82286807e-03 3.74836859e-01]] & cached\n",
      "Re-used Cached Value, runNum =  309\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  309\n",
      "Provided input from cache for runNum = 309\n",
      "Provided input from cache for runNum = 310\n",
      "activation = [[ 5.64387325  5.6992493   6.30836368 ... 10.93240849  4.88949807\n",
      "   3.46162976]\n",
      " [ 0.          3.33923337  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1572891   3.84397068  1.02511048 ...  2.26507855  5.05623918\n",
      "   1.88100171]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.93760001  2.42749826  2.71331964 ...  0.91370448  6.84916917\n",
      "   0.04495067]\n",
      " [ 4.37927726  8.0472392   0.         ...  3.48928117  2.32804082\n",
      "   2.53865815]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36591636 0.         1.20614928 ... 0.         0.80165559 0.        ]\n",
      " [1.86353248 7.00584472 2.03183717 ... 7.30528791 2.79238356 5.86104246]\n",
      " ...\n",
      " [6.76976946 6.46763178 5.05405399 ... 3.49555131 7.53103515 3.97735673]\n",
      " [4.30232891 0.         7.10184181 ... 7.84942819 3.97214947 2.93467027]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.69689771e-07 9.01838363e-07 6.54071335e-04 ... 1.75197519e-01\n",
      "  1.84193920e-04 1.85711021e-02]\n",
      " [1.20096775e-01 2.84684318e-06 5.56032048e-01 ... 1.57855790e-03\n",
      "  3.50439272e-03 4.78897561e-03]\n",
      " [1.02021718e-03 5.91839260e-05 5.04686795e-02 ... 1.05491785e-02\n",
      "  3.20201047e-01 1.40870471e-02]\n",
      " ...\n",
      " [5.96785158e-01 2.42445855e-01 1.15179057e-02 ... 3.95066068e-03\n",
      "  1.20602724e-04 3.46837339e-02]\n",
      " [1.76382367e-02 4.74231891e-04 2.04074886e-01 ... 1.43124401e-01\n",
      "  3.51044207e-02 3.65981795e-02]\n",
      " [2.38491657e-01 2.57199374e-01 2.21696174e-02 ... 1.40325465e-02\n",
      "  3.79181825e-03 3.75651590e-01]] & cached\n",
      "Re-used Cached Value, runNum =  310\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  310\n",
      "Provided input from cache for runNum = 310\n",
      "Provided input from cache for runNum = 311\n",
      "activation = [[ 5.64857962  5.70114089  6.30996964 ... 10.94349566  4.89262761\n",
      "   3.46380975]\n",
      " [ 0.          3.33994064  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15619787  3.84171797  1.0224247  ...  2.26749353  5.06076748\n",
      "   1.88212564]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.94227574  2.43900651  2.72117375 ...  0.91860269  6.85929816\n",
      "   0.05112246]\n",
      " [ 4.38603303  8.05800772  0.         ...  3.49594994  2.33242436\n",
      "   2.54497844]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36592849 0.         1.21207885 ... 0.         0.80267918 0.        ]\n",
      " [1.86651283 7.01306579 2.02983009 ... 7.32675808 2.80015443 5.87177848]\n",
      " ...\n",
      " [6.78206712 6.49110315 5.06837112 ... 3.51389244 7.54749215 3.99453004]\n",
      " [4.30765898 0.         7.1106468  ... 7.86646099 3.97750178 2.93720109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.57906520e-07 8.70746986e-07 6.34235519e-04 ... 1.74072538e-01\n",
      "  1.81258387e-04 1.83485289e-02]\n",
      " [1.19320390e-01 2.79541651e-06 5.57835572e-01 ... 1.55070303e-03\n",
      "  3.45342453e-03 4.73873897e-03]\n",
      " [1.01758212e-03 5.87407541e-05 5.05100293e-02 ... 1.05237188e-02\n",
      "  3.19677269e-01 1.40265972e-02]\n",
      " ...\n",
      " [5.98196384e-01 2.42360630e-01 1.14767381e-02 ... 3.94268043e-03\n",
      "  1.18610184e-04 3.44526910e-02]\n",
      " [1.75861900e-02 4.72871494e-04 2.03293177e-01 ... 1.43150435e-01\n",
      "  3.49036987e-02 3.65508167e-02]\n",
      " [2.38076557e-01 2.57803047e-01 2.20717109e-02 ... 1.40576528e-02\n",
      "  3.76041480e-03 3.76431366e-01]] & cached\n",
      "Re-used Cached Value, runNum =  311\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  311\n",
      "Provided input from cache for runNum = 311\n",
      "Provided input from cache for runNum = 312\n",
      "activation = [[ 5.65326391  5.70299253  6.31155347 ... 10.95449505  4.89568629\n",
      "   3.46596376]\n",
      " [ 0.          3.34062803  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15514232  3.83953816  1.01977265 ...  2.26998773  5.06532725\n",
      "   1.88328666]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.94698266  2.45056776  2.7290103  ...  0.92357755  6.86946438\n",
      "   0.05730349]\n",
      " [ 4.39277736  8.06876467  0.         ...  3.50265569  2.3368008\n",
      "   2.55129088]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36584651 0.         1.21790151 ... 0.         0.80369569 0.        ]\n",
      " [1.86946614 7.02019028 2.02780797 ... 7.3480566  2.80782145 5.88242834]\n",
      " ...\n",
      " [6.79422887 6.51441641 5.08257241 ... 3.53214356 7.56382643 4.01161222]\n",
      " [4.31291073 0.         7.11934764 ... 7.88327007 3.98271642 2.93965422]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.46665765e-07 8.41132396e-07 6.15206268e-04 ... 1.72983745e-01\n",
      "  1.78389512e-04 1.81316655e-02]\n",
      " [1.18533611e-01 2.74479540e-06 5.59584147e-01 ... 1.52323109e-03\n",
      "  3.40341944e-03 4.68905143e-03]\n",
      " [1.01521060e-03 5.83243745e-05 5.05600152e-02 ... 1.05014302e-02\n",
      "  3.19141294e-01 1.39682665e-02]\n",
      " ...\n",
      " [5.99621148e-01 2.42246131e-01 1.14360781e-02 ... 3.93476597e-03\n",
      "  1.16663477e-04 3.42230191e-02]\n",
      " [1.75378626e-02 4.71606547e-04 2.02542432e-01 ... 1.43190689e-01\n",
      "  3.47056109e-02 3.65062653e-02]\n",
      " [2.37648838e-01 2.58360535e-01 2.19737263e-02 ... 1.40819954e-02\n",
      "  3.72949364e-03 3.77190445e-01]] & cached\n",
      "Re-used Cached Value, runNum =  312\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  312\n",
      "Provided input from cache for runNum = 312\n",
      "Provided input from cache for runNum = 313\n",
      "activation = [[ 5.65794052  5.70484008  6.31312928 ... 10.9654425   4.89871877\n",
      "   3.46811228]\n",
      " [ 0.          3.34132608  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15400724  3.83727367  1.01708333 ...  2.27238206  5.06978939\n",
      "   1.88440271]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.95164734  2.46204365  2.73680844 ...  0.9285343   6.87957288\n",
      "   0.063453  ]\n",
      " [ 4.39951357  8.07952457  0.         ...  3.50943033  2.34118267\n",
      "   2.55761048]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36571059 0.         1.22366163 ... 0.         0.80464336 0.        ]\n",
      " [1.87239954 7.0272879  2.02574917 ... 7.36924081 2.81545125 5.89304   ]\n",
      " ...\n",
      " [6.80625243 6.53754763 5.09666225 ... 3.55030701 7.58003469 4.02862199]\n",
      " [4.31813368 0.         7.12798429 ... 7.8998908  3.98787813 2.94206051]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.35773906e-07 8.12581565e-07 5.96733746e-04 ... 1.71877476e-01\n",
      "  1.75609026e-04 1.79165774e-02]\n",
      " [1.17747634e-01 2.69564497e-06 5.61355998e-01 ... 1.49654173e-03\n",
      "  3.35448037e-03 4.64026204e-03]\n",
      " [1.01252122e-03 5.78987079e-05 5.05994828e-02 ... 1.04775092e-02\n",
      "  3.18596765e-01 1.39080785e-02]\n",
      " ...\n",
      " [6.01063023e-01 2.42179934e-01 1.13963703e-02 ... 3.92815000e-03\n",
      "  1.14764699e-04 3.39977183e-02]\n",
      " [1.74867676e-02 4.70315262e-04 2.01775573e-01 ... 1.43226566e-01\n",
      "  3.45115355e-02 3.64598881e-02]\n",
      " [2.37210612e-01 2.58949540e-01 2.18763871e-02 ... 1.41093994e-02\n",
      "  3.69923603e-03 3.77963728e-01]] & cached\n",
      "Re-used Cached Value, runNum =  313\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  313\n",
      "Provided input from cache for runNum = 313\n",
      "Provided input from cache for runNum = 314\n",
      "activation = [[ 5.66262789  5.70669809  6.31470455 ... 10.97635749  4.90173496\n",
      "   3.47026443]\n",
      " [ 0.          3.34199965  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15284876  3.83498878  1.01438738 ...  2.27475969  5.07419221\n",
      "   1.88551552]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.95632266  2.4735174   2.74459413 ...  0.93352337  6.88968564\n",
      "   0.06960913]\n",
      " [ 4.40619911  8.0902058   0.         ...  3.51615695  2.34549902\n",
      "   2.56388999]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36540732 0.         1.22925946 ... 0.         0.80548024 0.        ]\n",
      " [1.87537628 7.03437155 2.02372133 ... 7.39036062 2.82307211 5.90361312]\n",
      " ...\n",
      " [6.81822282 6.56060637 5.11070529 ... 3.56846309 7.59620211 4.04559755]\n",
      " [4.32344469 0.         7.13661508 ... 7.91651165 3.99310399 2.94450824]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.25337552e-07 7.85170921e-07 5.78889290e-04 ... 1.70772886e-01\n",
      "  1.72913942e-04 1.77055019e-02]\n",
      " [1.16981898e-01 2.64825000e-06 5.63135435e-01 ... 1.47064433e-03\n",
      "  3.30639275e-03 4.59266639e-03]\n",
      " [1.00985735e-03 5.74790174e-05 5.06340852e-02 ... 1.04532754e-02\n",
      "  3.18049415e-01 1.38483559e-02]\n",
      " ...\n",
      " [6.02474899e-01 2.42096848e-01 1.13558869e-02 ... 3.92090728e-03\n",
      "  1.12882851e-04 3.37700652e-02]\n",
      " [1.74364400e-02 4.69003338e-04 2.00996100e-01 ... 1.43238403e-01\n",
      "  3.43207940e-02 3.64121881e-02]\n",
      " [2.36779347e-01 2.59564919e-01 2.17788220e-02 ... 1.41363265e-02\n",
      "  3.66896749e-03 3.78718296e-01]] & cached\n",
      "Re-used Cached Value, runNum =  314\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  314\n",
      "Provided input from cache for runNum = 314\n",
      "Provided input from cache for runNum = 315\n",
      "activation = [[ 5.66731051  5.70855537  6.31627263 ... 10.98723111  4.90472842\n",
      "   3.47240271]\n",
      " [ 0.          3.34266787  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15169099  3.83270105  1.0117181  ...  2.27713991  5.07856724\n",
      "   1.88663611]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.96097827  2.48494613  2.7523397  ...  0.93850373  6.89975951\n",
      "   0.07574532]\n",
      " [ 4.41284159  8.10083512  0.         ...  3.52287107  2.34977196\n",
      "   2.57015207]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36500582 0.         1.23473524 ... 0.         0.80623264 0.        ]\n",
      " [1.87831937 7.04136466 2.02167223 ... 7.41133819 2.83063671 5.91411969]\n",
      " ...\n",
      " [6.8300374  6.58346584 5.12460679 ... 3.5864659  7.61222071 4.06247248]\n",
      " [4.32875001 0.         7.14519126 ... 7.93304003 3.99833723 2.9469446 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.15308601e-07 7.58805878e-07 5.61737895e-04 ... 1.69700047e-01\n",
      "  1.70307977e-04 1.74991917e-02]\n",
      " [1.16200299e-01 2.60156765e-06 5.64861516e-01 ... 1.44515747e-03\n",
      "  3.25921388e-03 4.54567942e-03]\n",
      " [1.00720857e-03 5.70628922e-05 5.06736524e-02 ... 1.04307197e-02\n",
      "  3.17516328e-01 1.37898558e-02]\n",
      " ...\n",
      " [6.03928228e-01 2.42038133e-01 1.13174962e-02 ... 3.91448825e-03\n",
      "  1.11054724e-04 3.35472781e-02]\n",
      " [1.73882736e-02 4.67730588e-04 2.00255546e-01 ... 1.43274125e-01\n",
      "  3.41361830e-02 3.63683676e-02]\n",
      " [2.36318977e-01 2.60149551e-01 2.16817241e-02 ... 1.41629649e-02\n",
      "  3.63918950e-03 3.79461892e-01]] & cached\n",
      "Re-used Cached Value, runNum =  315\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  315\n",
      "Provided input from cache for runNum = 315\n",
      "Provided input from cache for runNum = 316\n",
      "activation = [[ 5.67200818  5.710436    6.31784852 ... 10.998089    4.90772325\n",
      "   3.47454114]\n",
      " [ 0.          3.34334187  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.15045981  3.83031531  1.00900935 ...  2.27942203  5.08280483\n",
      "   1.88771701]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.965565    2.49622884  2.76001625 ...  0.94338921  6.90974549\n",
      "   0.08180943]\n",
      " [ 4.41945182  8.11143646  0.         ...  3.52959319  2.35403424\n",
      "   2.57640061]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36460622 0.         1.24018335 ... 0.         0.80699715 0.        ]\n",
      " [1.8813604  7.04848625 2.01968831 ... 7.43233905 2.83825311 5.92464097]\n",
      " ...\n",
      " [6.84179407 6.60624176 5.13845203 ... 3.60445761 7.62821739 4.0793021 ]\n",
      " [4.33402199 0.         7.15369715 ... 7.94943899 4.00351529 2.94933361]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.05584065e-07 7.33460209e-07 5.45062475e-04 ... 1.68592668e-01\n",
      "  1.67757680e-04 1.72938753e-02]\n",
      " [1.15421430e-01 2.55637677e-06 5.66609048e-01 ... 1.42035770e-03\n",
      "  3.21267421e-03 4.49950329e-03]\n",
      " [1.00421555e-03 5.66375767e-05 5.06982366e-02 ... 1.04039312e-02\n",
      "  3.16957071e-01 1.37282340e-02]\n",
      " ...\n",
      " [6.05373157e-01 2.42037551e-01 1.12796132e-02 ... 3.90881669e-03\n",
      "  1.09245192e-04 3.33277575e-02]\n",
      " [1.73385573e-02 4.66535814e-04 1.99497894e-01 ... 1.43297616e-01\n",
      "  3.39502225e-02 3.63224876e-02]\n",
      " [2.35872311e-01 2.60812368e-01 2.15867533e-02 ... 1.41925136e-02\n",
      "  3.60974884e-03 3.80229352e-01]] & cached\n",
      "Re-used Cached Value, runNum =  316\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  316\n",
      "Provided input from cache for runNum = 316\n",
      "Provided input from cache for runNum = 317\n",
      "activation = [[ 5.67668852  5.71229107  6.31941027 ... 11.00887933  4.91066829\n",
      "   3.47666162]\n",
      " [ 0.          3.34401731  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14921822  3.8279329   1.00630721 ...  2.28172204  5.08701128\n",
      "   1.88880263]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.97018028  2.50752953  2.76768648 ...  0.94833662  6.91975222\n",
      "   0.08787828]\n",
      " [ 4.42599832  8.12195681  0.         ...  3.53626696  2.35823372\n",
      "   2.58260655]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36431593 0.         1.24566441 ... 0.         0.80788956 0.        ]\n",
      " [1.88424613 7.055398   2.01759492 ... 7.45305041 2.84566012 5.93503086]\n",
      " ...\n",
      " [6.853497   6.62894657 5.15224003 ... 3.6224193  7.64416569 4.09608915]\n",
      " [4.33931556 0.         7.16216502 ... 7.96576631 4.00867953 2.95171559]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.96268751e-07 7.09175230e-07 5.28948716e-04 ... 1.67520203e-01\n",
      "  1.65225661e-04 1.70938333e-02]\n",
      " [1.14667039e-01 2.51204263e-06 5.68342781e-01 ... 1.39615828e-03\n",
      "  3.16714171e-03 4.45424416e-03]\n",
      " [1.00177031e-03 5.62412679e-05 5.07361418e-02 ... 1.03822526e-02\n",
      "  3.16417811e-01 1.36703944e-02]\n",
      " ...\n",
      " [6.06798148e-01 2.42000349e-01 1.12405346e-02 ... 3.90311430e-03\n",
      "  1.07448946e-04 3.31095716e-02]\n",
      " [1.72935186e-02 4.65452951e-04 1.98748632e-01 ... 1.43351215e-01\n",
      "  3.37613254e-02 3.62819117e-02]\n",
      " [2.35409402e-01 2.61412829e-01 2.14887780e-02 ... 1.42202192e-02\n",
      "  3.57990910e-03 3.80967354e-01]] & cached\n",
      "Re-used Cached Value, runNum =  317\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  317\n",
      "Provided input from cache for runNum = 317\n",
      "Provided input from cache for runNum = 318\n",
      "activation = [[ 5.68133174  5.71408467  6.32094735 ... 11.01956973  4.91353414\n",
      "   3.47874659]\n",
      " [ 0.          3.34466835  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14800934  3.82561493  1.00365392 ...  2.28408025  5.09123518\n",
      "   1.88991732]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.9747752   2.5187698   2.77531191 ...  0.95323083  6.92976637\n",
      "   0.0939027 ]\n",
      " [ 4.43253248  8.13247464  0.         ...  3.54300403  2.36244753\n",
      "   2.5888073 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36418481 0.         1.25126165 ... 0.         0.80898057 0.        ]\n",
      " [1.8872293  7.0623861  2.01558983 ... 7.47375714 2.85307157 5.94541344]\n",
      " ...\n",
      " [6.86508287 6.65148033 5.1659188  ... 3.64023083 7.66001396 4.11277868]\n",
      " [4.3445269  0.         7.17053232 ... 7.9818406  4.01369443 2.95401931]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.87300515e-07 6.85882600e-07 5.13414653e-04 ... 1.66474318e-01\n",
      "  1.62727808e-04 1.68972977e-02]\n",
      " [1.13903672e-01 2.46795523e-06 5.70056358e-01 ... 1.37224055e-03\n",
      "  3.12230593e-03 4.40916808e-03]\n",
      " [9.99419776e-04 5.58497857e-05 5.07801636e-02 ... 1.03611953e-02\n",
      "  3.15859856e-01 1.36123327e-02]\n",
      " ...\n",
      " [6.08210626e-01 2.41928706e-01 1.12025622e-02 ... 3.89847678e-03\n",
      "  1.05694879e-04 3.28933091e-02]\n",
      " [1.72458830e-02 4.64290339e-04 1.97983304e-01 ... 1.43383116e-01\n",
      "  3.35683798e-02 3.62368330e-02]\n",
      " [2.34965847e-01 2.61990231e-01 2.13942938e-02 ... 1.42509670e-02\n",
      "  3.55084235e-03 3.81709318e-01]] & cached\n",
      "Re-used Cached Value, runNum =  318\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  318\n",
      "Provided input from cache for runNum = 318\n",
      "Provided input from cache for runNum = 319\n",
      "activation = [[ 5.68598805  5.71591177  6.32248512 ... 11.03026316  4.91639444\n",
      "   3.48084382]\n",
      " [ 0.          3.34526762  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14675894  3.82321663  1.0009957  ...  2.28634951  5.09538546\n",
      "   1.89098873]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.97934551  2.5299846   2.78292121 ...  0.95814096  6.93976465\n",
      "   0.09992031]\n",
      " [ 4.43900764  8.14292814  0.         ...  3.54972886  2.36661728\n",
      "   2.59497314]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36418225 0.         1.25692577 ... 0.         0.81022039 0.        ]\n",
      " [1.89016347 7.06923472 2.0135535  ... 7.49429462 2.86039862 5.95571593]\n",
      " ...\n",
      " [6.87649185 6.67381398 5.17946776 ... 3.65788227 7.67570002 4.12937185]\n",
      " [4.34969333 0.         7.1788101  ... 7.99777514 4.01863248 2.95628759]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.78627066e-07 6.63560332e-07 4.98343817e-04 ... 1.65429990e-01\n",
      "  1.60287703e-04 1.67039666e-02]\n",
      " [1.13144608e-01 2.42534695e-06 5.71762979e-01 ... 1.34875521e-03\n",
      "  3.07861840e-03 4.36486279e-03]\n",
      " [9.96905784e-04 5.54626009e-05 5.08167169e-02 ... 1.03383581e-02\n",
      "  3.15290791e-01 1.35534043e-02]\n",
      " ...\n",
      " [6.09633427e-01 2.41936666e-01 1.11651439e-02 ... 3.89452865e-03\n",
      "  1.03966433e-04 3.26816870e-02]\n",
      " [1.71997520e-02 4.63341201e-04 1.97229437e-01 ... 1.43441711e-01\n",
      "  3.33755155e-02 3.61970305e-02]\n",
      " [2.34512633e-01 2.62610105e-01 2.12999003e-02 ... 1.42814836e-02\n",
      "  3.52212413e-03 3.82454018e-01]] & cached\n",
      "Re-used Cached Value, runNum =  319\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  319\n",
      "Provided input from cache for runNum = 319\n",
      "Provided input from cache for runNum = 320\n",
      "activation = [[ 5.69061908  5.71772402  6.32399766 ... 11.04089325  4.91919845\n",
      "   3.48291733]\n",
      " [ 0.          3.34582835  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14546497  3.82074117  0.99833052 ...  2.28853978  5.09945284\n",
      "   1.89203467]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.98389012  2.54114081  2.79048633 ...  0.96302182  6.94973295\n",
      "   0.10590339]\n",
      " [ 4.44541925  8.15329682  0.         ...  3.55640308  2.37070994\n",
      "   2.60109755]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36442884 0.         1.26277032 ... 0.         0.81175249 0.        ]\n",
      " [1.89310395 7.07603295 2.01154169 ... 7.51473845 2.86769307 5.96597725]\n",
      " ...\n",
      " [6.88788287 6.69612043 5.1929887  ... 3.67554747 7.69138049 4.14593335]\n",
      " [4.35485571 0.         7.1870197  ... 8.01361824 4.02353477 2.9585294 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.70228819e-07 6.42040833e-07 4.83659941e-04 ... 1.64378490e-01\n",
      "  1.57858781e-04 1.65130676e-02]\n",
      " [1.12415153e-01 2.38403827e-06 5.73507237e-01 ... 1.32591228e-03\n",
      "  3.03575193e-03 4.32136990e-03]\n",
      " [9.94495310e-04 5.50804957e-05 5.08502148e-02 ... 1.03154931e-02\n",
      "  3.14707412e-01 1.34943098e-02]\n",
      " ...\n",
      " [6.11004407e-01 2.41942803e-01 1.11262565e-02 ... 3.89049640e-03\n",
      "  1.02234843e-04 3.24696710e-02]\n",
      " [1.71526986e-02 4.62434513e-04 1.96433309e-01 ... 1.43485577e-01\n",
      "  3.31753102e-02 3.61553012e-02]\n",
      " [2.34081701e-01 2.63254059e-01 2.12059010e-02 ... 1.43128386e-02\n",
      "  3.49324112e-03 3.83194812e-01]] & cached\n",
      "Re-used Cached Value, runNum =  320\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  320\n",
      "Provided input from cache for runNum = 320\n",
      "Provided input from cache for runNum = 321\n",
      "activation = [[ 5.69525946  5.71955539  6.32551875 ... 11.05150938  4.92198796\n",
      "   3.48500051]\n",
      " [ 0.          3.34638489  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14414046  3.81820489  0.99565054 ...  2.29066254  5.10344369\n",
      "   1.89304942]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.98837368  2.55222108  2.79800071 ...  0.96787202  6.95964467\n",
      "   0.11184331]\n",
      " [ 4.45184272  8.16371877  0.         ...  3.56317131  2.37487424\n",
      "   2.60724615]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36467605 0.         1.26862265 ... 0.         0.81332961 0.        ]\n",
      " [1.89608303 7.08284212 2.00954481 ... 7.53513256 2.8750027  5.97622757]\n",
      " ...\n",
      " [6.89917879 6.71835819 5.20642578 ... 3.69317415 7.70700018 4.1624491 ]\n",
      " [4.35997707 0.         7.19518594 ... 8.02931582 4.02833876 2.96073111]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.62077875e-07 6.21122347e-07 4.69420872e-04 ... 1.63318135e-01\n",
      "  1.55474807e-04 1.63238968e-02]\n",
      " [1.11668977e-01 2.34322059e-06 5.75240356e-01 ... 1.30354729e-03\n",
      "  2.99334925e-03 4.27831021e-03]\n",
      " [9.91864663e-04 5.46867816e-05 5.08792919e-02 ... 1.02920772e-02\n",
      "  3.14118679e-01 1.34341521e-02]\n",
      " ...\n",
      " [6.12404735e-01 2.41978777e-01 1.10890527e-02 ... 3.88810702e-03\n",
      "  1.00545626e-04 3.22629391e-02]\n",
      " [1.71045887e-02 4.61507423e-04 1.95643973e-01 ... 1.43543706e-01\n",
      "  3.29751218e-02 3.61145009e-02]\n",
      " [2.33639906e-01 2.63891629e-01 2.11134131e-02 ... 1.43475695e-02\n",
      "  3.46493838e-03 3.83947657e-01]] & cached\n",
      "Re-used Cached Value, runNum =  321\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  321\n",
      "Provided input from cache for runNum = 321\n",
      "Provided input from cache for runNum = 322\n",
      "activation = [[ 5.69990429  5.72141082  6.32704019 ... 11.06210706  4.92476527\n",
      "   3.48708314]\n",
      " [ 0.          3.34691432  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14278498  3.81560095  0.99296142 ...  2.29271693  5.10736921\n",
      "   1.89403651]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.99282315  2.56323167  2.8054791  ...  0.97268596  6.96947755\n",
      "   0.11775534]\n",
      " [ 4.45822415  8.17409035  0.         ...  3.56992503  2.37899867\n",
      "   2.61336549]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36480732 0.         1.27435174 ... 0.         0.81479916 0.        ]\n",
      " [1.89908083 7.08963917 2.0075582  ... 7.55543715 2.88229575 5.98642877]\n",
      " ...\n",
      " [6.91041752 6.74054732 5.21980507 ... 3.71077993 7.72255139 4.17892915]\n",
      " [4.36511513 0.         7.2033198  ... 8.04497195 4.03313781 2.96293459]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.54315676e-07 6.01128163e-07 4.55726055e-04 ... 1.62270647e-01\n",
      "  1.53166222e-04 1.61385746e-02]\n",
      " [1.10945135e-01 2.30384319e-06 5.76955669e-01 ... 1.28166942e-03\n",
      "  2.95157103e-03 4.23615895e-03]\n",
      " [9.89507890e-04 5.43078734e-05 5.09112896e-02 ... 1.02686997e-02\n",
      "  3.13518873e-01 1.33746357e-02]\n",
      " ...\n",
      " [6.13752595e-01 2.41990850e-01 1.10497258e-02 ... 3.88473473e-03\n",
      "  9.88630185e-05 3.20535611e-02]\n",
      " [1.70604333e-02 4.60676707e-04 1.94856092e-01 ... 1.43589053e-01\n",
      "  3.27752284e-02 3.60735608e-02]\n",
      " [2.33219424e-01 2.64561398e-01 2.10196280e-02 ... 1.43805253e-02\n",
      "  3.43653422e-03 3.84681794e-01]] & cached\n",
      "Re-used Cached Value, runNum =  322\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  322\n",
      "Provided input from cache for runNum = 322\n",
      "Provided input from cache for runNum = 323\n",
      "activation = [[ 5.70454869  5.72327631  6.32855537 ... 11.07267141  4.92751892\n",
      "   3.48916225]\n",
      " [ 0.          3.34745261  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14148469  3.81306519  0.99033939 ...  2.29483466  5.11131888\n",
      "   1.89505692]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 2.99727273  2.57419389  2.81292512 ...  0.97749837  6.97929318\n",
      "   0.12363769]\n",
      " [ 4.46452867  8.18435608  0.         ...  3.57660859  2.38303147\n",
      "   2.6194354 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36475255 0.         1.2798927  ... 0.         0.81612583 0.        ]\n",
      " [1.90207633 7.09642172 2.00558798 ... 7.57561291 2.88952896 5.99657351]\n",
      " ...\n",
      " [6.92153656 6.76256178 5.23306339 ... 3.72825158 7.73798798 4.19531202]\n",
      " [4.37027754 0.         7.21139961 ... 8.06058753 4.03795114 2.96513878]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.46971712e-07 5.82142436e-07 4.42634147e-04 ... 1.61271229e-01\n",
      "  1.50941659e-04 1.59589398e-02]\n",
      " [1.10239252e-01 2.26548749e-06 5.78634573e-01 ... 1.26019123e-03\n",
      "  2.91057597e-03 4.19480104e-03]\n",
      " [9.87553175e-04 5.39511014e-05 5.09509753e-02 ... 1.02471283e-02\n",
      "  3.12917493e-01 1.33168929e-02]\n",
      " ...\n",
      " [6.15066130e-01 2.41949800e-01 1.10091899e-02 ... 3.88004892e-03\n",
      "  9.71969141e-05 3.18420812e-02]\n",
      " [1.70208074e-02 4.59897871e-04 1.94078699e-01 ... 1.43616293e-01\n",
      "  3.25779307e-02 3.60324319e-02]\n",
      " [2.32802400e-01 2.65211198e-01 2.09246311e-02 ... 1.44098344e-02\n",
      "  3.40804016e-03 3.85381507e-01]] & cached\n",
      "Re-used Cached Value, runNum =  323\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  323\n",
      "Provided input from cache for runNum = 323\n",
      "Provided input from cache for runNum = 324\n",
      "activation = [[ 5.70917238  5.72509771  6.33006031 ... 11.08314546  4.93020856\n",
      "   3.49121718]\n",
      " [ 0.          3.34805189  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.14026083  3.81065268  0.9877924  ...  2.2970668   5.11533659\n",
      "   1.89613273]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.00173518  2.58514632  2.82034754 ...  0.98230673  6.9890926\n",
      "   0.1295038 ]\n",
      " [ 4.47083788  8.19462572  0.         ...  3.58334046  2.38708859\n",
      "   2.62551171]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36438166 0.         1.28515721 ... 0.         0.81719567 0.        ]\n",
      " [1.90512479 7.10325624 2.00365728 ... 7.59576341 2.89679093 6.00670699]\n",
      " ...\n",
      " [6.93257789 6.7844523  5.24623044 ... 3.74564778 7.75333484 4.21163179]\n",
      " [4.37535163 0.         7.21938395 ... 8.07597362 4.04261382 2.96726105]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.39932410e-07 5.63869731e-07 4.30097264e-04 ... 1.60302647e-01\n",
      "  1.48791541e-04 1.57827407e-02]\n",
      " [1.09508760e-01 2.22718038e-06 5.80260697e-01 ... 1.23902208e-03\n",
      "  2.86996666e-03 4.15370179e-03]\n",
      " [9.85656033e-04 5.36024352e-05 5.09982976e-02 ... 1.02279505e-02\n",
      "  3.12299382e-01 1.32601592e-02]\n",
      " ...\n",
      " [6.16404298e-01 2.41850026e-01 1.09706909e-02 ... 3.87594024e-03\n",
      "  9.55894713e-05 3.16317718e-02]\n",
      " [1.69808909e-02 4.59001993e-04 1.93323611e-01 ... 1.43630013e-01\n",
      "  3.23854406e-02 3.59882838e-02]\n",
      " [2.32377469e-01 2.65798516e-01 2.08320647e-02 ... 1.44410516e-02\n",
      "  3.38047365e-03 3.86074290e-01]] & cached\n",
      "Re-used Cached Value, runNum =  324\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  324\n",
      "Provided input from cache for runNum = 324\n",
      "Provided input from cache for runNum = 325\n",
      "activation = [[ 5.71379031  5.72692298  6.33156021 ... 11.09358675  4.93287262\n",
      "   3.49326875]\n",
      " [ 0.          3.34864893  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13902107  3.80819568  0.98525507 ...  2.29926303  5.11929056\n",
      "   1.89718762]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.00626082  2.59621439  2.82778641 ...  0.98728971  6.99900725\n",
      "   0.13542506]\n",
      " [ 4.47706761  8.20478175  0.         ...  3.58999934  2.39104734\n",
      "   2.63153028]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36410148 0.         1.29043563 ... 0.         0.81841868 0.        ]\n",
      " [1.90809028 7.10993025 2.00168079 ... 7.61567642 2.9038893  6.01673518]\n",
      " ...\n",
      " [6.9436251  6.8063897  5.25940508 ... 3.76322475 7.7687499  4.22798324]\n",
      " [4.38040329 0.         7.22728302 ... 8.09127513 4.04726885 2.96937074]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.33124419e-07 5.46347190e-07 4.17876489e-04 ... 1.59308600e-01\n",
      "  1.46643562e-04 1.56086175e-02]\n",
      " [1.08814568e-01 2.19074722e-06 5.81914266e-01 ... 1.21861548e-03\n",
      "  2.83036683e-03 4.11388489e-03]\n",
      " [9.83848786e-04 5.32743803e-05 5.10390263e-02 ... 1.02097033e-02\n",
      "  3.11664043e-01 1.32040743e-02]\n",
      " ...\n",
      " [6.17699148e-01 2.41766887e-01 1.09301424e-02 ... 3.87082282e-03\n",
      "  9.39660175e-05 3.14200670e-02]\n",
      " [1.69433080e-02 4.58301356e-04 1.92555719e-01 ... 1.43656111e-01\n",
      "  3.21896331e-02 3.59474277e-02]\n",
      " [2.31960307e-01 2.66436536e-01 2.07376805e-02 ... 1.44710050e-02\n",
      "  3.35230401e-03 3.86756072e-01]] & cached\n",
      "Re-used Cached Value, runNum =  325\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  325\n",
      "Provided input from cache for runNum = 325\n",
      "Provided input from cache for runNum = 326\n",
      "activation = [[ 5.71835753  5.72866961  6.33302141 ... 11.10391261  4.93544532\n",
      "   3.49527907]\n",
      " [ 0.          3.34927307  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13779907  3.80578321  0.98275125 ...  2.3015092   5.12325891\n",
      "   1.89826493]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.01072665  2.60716282  2.83516607 ...  0.99222421  7.00884104\n",
      "   0.14130966]\n",
      " [ 4.48331236  8.21496815  0.         ...  3.59671081  2.3950293\n",
      "   2.637555  ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36382541 0.         1.2957061  ... 0.         0.81965358 0.        ]\n",
      " [1.91106014 7.11659491 1.99970226 ... 7.63547966 2.91095375 6.02671227]\n",
      " ...\n",
      " [6.95441205 6.82795787 5.27236037 ... 3.78048933 7.78386902 4.244165  ]\n",
      " [4.38525572 0.         7.23502586 ... 8.10624625 4.05167936 2.97134466]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.26559370e-07 5.29604599e-07 4.06123901e-04 ... 1.58361972e-01\n",
      "  1.44581550e-04 1.54389071e-02]\n",
      " [1.08099933e-01 2.15467832e-06 5.83550406e-01 ... 1.19844942e-03\n",
      "  2.79172526e-03 4.07436745e-03]\n",
      " [9.81877143e-04 5.29478955e-05 5.10798026e-02 ... 1.01913187e-02\n",
      "  3.11018546e-01 1.31478538e-02]\n",
      " ...\n",
      " [6.19033807e-01 2.41702975e-01 1.08911263e-02 ... 3.86641417e-03\n",
      "  9.24076220e-05 3.12123647e-02]\n",
      " [1.69032990e-02 4.57586504e-04 1.91788960e-01 ... 1.43671931e-01\n",
      "  3.19980970e-02 3.59054839e-02]\n",
      " [2.31525861e-01 2.67038648e-01 2.06446149e-02 ... 1.45016277e-02\n",
      "  3.32535572e-03 3.87435075e-01]] & cached\n",
      "Re-used Cached Value, runNum =  326\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  326\n",
      "Provided input from cache for runNum = 326\n",
      "Provided input from cache for runNum = 327\n",
      "activation = [[ 5.72291365  5.73039957  6.33447299 ... 11.11418336  4.93797433\n",
      "   3.49728374]\n",
      " [ 0.          3.34993565  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13657605  3.80337943  0.98025745 ...  2.30377653  5.12719561\n",
      "   1.89934144]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.01518454  2.6181034   2.84252458 ...  0.9971883   7.0186713\n",
      "   0.1471845 ]\n",
      " [ 4.48952162  8.22510823  0.         ...  3.60342843  2.39896334\n",
      "   2.6435632 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36359147 0.         1.30098645 ... 0.         0.8209803  0.        ]\n",
      " [1.91402078 7.12322006 1.99771398 ... 7.65520317 2.91799817 6.03665724]\n",
      " ...\n",
      " [6.96511674 6.84942805 5.28523649 ... 3.79771559 7.79894544 4.26029832]\n",
      " [4.39009267 0.         7.24270997 ... 8.12108982 4.0560608  2.97329276]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.20195820e-07 5.13432312e-07 3.94694161e-04 ... 1.57414197e-01\n",
      "  1.42555011e-04 1.52714147e-02]\n",
      " [1.07399090e-01 2.11955305e-06 5.85208158e-01 ... 1.17879183e-03\n",
      "  2.75384387e-03 4.03556266e-03]\n",
      " [9.79832628e-04 5.26229118e-05 5.11142937e-02 ... 1.01726890e-02\n",
      "  3.10369364e-01 1.30913649e-02]\n",
      " ...\n",
      " [6.20355005e-01 2.41642045e-01 1.08517714e-02 ... 3.86202193e-03\n",
      "  9.08700986e-05 3.10055468e-02]\n",
      " [1.68624533e-02 4.56865353e-04 1.91004839e-01 ... 1.43677694e-01\n",
      "  3.18072085e-02 3.58629345e-02]\n",
      " [2.31092598e-01 2.67649325e-01 2.05514747e-02 ... 1.45330355e-02\n",
      "  3.29864647e-03 3.88111242e-01]] & cached\n",
      "Re-used Cached Value, runNum =  327\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  327\n",
      "Provided input from cache for runNum = 327\n",
      "Provided input from cache for runNum = 328\n",
      "activation = [[ 5.72744489  5.73210041  6.33590634 ... 11.12437635  4.94044988\n",
      "   3.49926525]\n",
      " [ 0.          3.35059017  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13536066  3.80099842  0.97777597 ...  2.30606785  5.13111174\n",
      "   1.90042877]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.01954742  2.6288755   2.8498077  ...  1.00202907  7.02838541\n",
      "   0.15297647]\n",
      " [ 4.49571331  8.23523536  0.         ...  3.61019252  2.40288851\n",
      "   2.64956411]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36325519 0.         1.30618618 ... 0.         0.82225306 0.        ]\n",
      " [1.9170272  7.12989812 1.99574772 ... 7.67489667 2.92504348 6.04658901]\n",
      " ...\n",
      " [6.97565066 6.87065485 5.29798041 ... 3.81476519 7.81385904 4.27630904]\n",
      " [4.39484758 0.         7.25030502 ... 8.13572149 4.06033554 2.97517027]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.14039787e-07 4.97853538e-07 3.83637376e-04 ... 1.56473094e-01\n",
      "  1.40596337e-04 1.51061417e-02]\n",
      " [1.06683553e-01 2.08496528e-06 5.86857374e-01 ... 1.15954232e-03\n",
      "  2.71662210e-03 3.99717920e-03]\n",
      " [9.77568086e-04 5.22920444e-05 5.11440807e-02 ... 1.01528898e-02\n",
      "  3.09718728e-01 1.30336368e-02]\n",
      " ...\n",
      " [6.21699351e-01 2.41592593e-01 1.08138760e-02 ... 3.85890619e-03\n",
      "  8.93791893e-05 3.08027108e-02]\n",
      " [1.68199654e-02 4.56095582e-04 1.90222120e-01 ... 1.43673930e-01\n",
      "  3.16212784e-02 3.58183942e-02]\n",
      " [2.30652384e-01 2.68258217e-01 2.04602253e-02 ... 1.45677104e-02\n",
      "  3.27271707e-03 3.88798124e-01]] & cached\n",
      "Re-used Cached Value, runNum =  328\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  328\n",
      "Provided input from cache for runNum = 328\n",
      "Provided input from cache for runNum = 329\n",
      "activation = [[ 5.73195572  5.73377763  6.33732181 ... 11.13450755  4.94288024\n",
      "   3.50122627]\n",
      " [ 0.          3.35127388  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1341402   3.79861643  0.97530574 ...  2.30835189  5.13500669\n",
      "   1.90151832]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.02383094  2.63948561  2.85703584 ...  1.00675159  7.03799582\n",
      "   0.1586942 ]\n",
      " [ 4.50189632  8.24537446  0.         ...  3.61699248  2.40682147\n",
      "   2.65557613]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36298144 0.         1.31144008 ... 0.         0.82360018 0.        ]\n",
      " [1.92002379 7.13657834 1.99376094 ... 7.69450698 2.93205383 6.05648351]\n",
      " ...\n",
      " [6.98602642 6.89165007 5.31060435 ... 3.83163616 7.82860074 4.29220209]\n",
      " [4.39954793 0.         7.25783168 ... 8.15018554 4.06451409 2.97697901]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.08129801e-07 4.82956835e-07 3.72963257e-04 ... 1.55565543e-01\n",
      "  1.38690362e-04 1.49450121e-02]\n",
      " [1.05968550e-01 2.05080067e-06 5.88474331e-01 ... 1.14047279e-03\n",
      "  2.68015235e-03 3.95910941e-03]\n",
      " [9.75501546e-04 5.19726217e-05 5.11806784e-02 ... 1.01339906e-02\n",
      "  3.09069637e-01 1.29768891e-02]\n",
      " ...\n",
      " [6.23041565e-01 2.41539503e-01 1.07753426e-02 ... 3.85571167e-03\n",
      "  8.79210234e-05 3.06014789e-02]\n",
      " [1.67804611e-02 4.55400197e-04 1.89459780e-01 ... 1.43681823e-01\n",
      "  3.14379432e-02 3.57765042e-02]\n",
      " [2.30208202e-01 2.68847133e-01 2.03678941e-02 ... 1.46011717e-02\n",
      "  3.24725837e-03 3.89473393e-01]] & cached\n",
      "Re-used Cached Value, runNum =  329\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  329\n",
      "Provided input from cache for runNum = 329\n",
      "Provided input from cache for runNum = 330\n",
      "activation = [[ 5.73648468  5.735486    6.33874996 ... 11.14462041  4.94532004\n",
      "   3.50319832]\n",
      " [ 0.          3.35198001  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13290409  3.79622849  0.97283753 ...  2.3106153   5.13886926\n",
      "   1.90260192]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.02812541  2.65014018  2.8642636  ...  1.01154183  7.04763905\n",
      "   0.16443363]\n",
      " [ 4.50800053  8.25539873  0.         ...  3.62372762  2.41063606\n",
      "   2.66152991]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36276373 0.         1.31671542 ... 0.         0.82502628 0.        ]\n",
      " [1.92299317 7.14318849 1.9917585  ... 7.71398777 2.938987   6.06631087]\n",
      " ...\n",
      " [6.99635415 6.91261498 5.32319402 ... 3.84856405 7.84333254 4.30808494]\n",
      " [4.40444151 0.         7.2654183  ... 8.16476839 4.06892881 2.97890395]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.02491611e-07 4.68721836e-07 3.62610172e-04 ... 1.54664057e-01\n",
      "  1.36825299e-04 1.47878133e-02]\n",
      " [1.05300200e-01 2.01792346e-06 5.90103664e-01 ... 1.12188856e-03\n",
      "  2.64464655e-03 3.92211400e-03]\n",
      " [9.73838873e-04 5.16758195e-05 5.12174374e-02 ... 1.01163746e-02\n",
      "  3.08435389e-01 1.29222786e-02]\n",
      " ...\n",
      " [6.24318754e-01 2.41449304e-01 1.07335406e-02 ... 3.85080332e-03\n",
      "  8.64487650e-05 3.03964349e-02]\n",
      " [1.67459371e-02 4.54810563e-04 1.88686704e-01 ... 1.43685780e-01\n",
      "  3.12553354e-02 3.57374941e-02]\n",
      " [2.29774859e-01 2.69458179e-01 2.02718787e-02 ... 1.46305200e-02\n",
      "  3.22106394e-03 3.90108724e-01]] & cached\n",
      "Re-used Cached Value, runNum =  330\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  330\n",
      "Provided input from cache for runNum = 330\n",
      "Provided input from cache for runNum = 331\n",
      "activation = [[ 5.74104731  5.73725187  6.3402069  ... 11.15475177  4.94777901\n",
      "   3.50519225]\n",
      " [ 0.          3.35266731  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13159762  3.79373882  0.97033621 ...  2.31275764  5.14262085\n",
      "   1.90363344]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.03234899  2.66066499  2.87140857 ...  1.01623953  7.05719996\n",
      "   0.1701077 ]\n",
      " [ 4.51407978  8.26540007  0.         ...  3.63047246  2.41442777\n",
      "   2.66746552]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36248077 0.         1.32190366 ... 0.         0.82642518 0.        ]\n",
      " [1.92598213 7.14979311 1.98978928 ... 7.73339231 2.94590812 6.07610657]\n",
      " ...\n",
      " [7.00654078 6.93338982 5.33566578 ... 3.86537374 7.85795237 4.32387118]\n",
      " [4.409272   0.         7.27292084 ... 8.17919745 4.07327009 2.9807839 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.96986316e-07 4.54969165e-07 3.52566366e-04 ... 1.53744147e-01\n",
      "  1.35014265e-04 1.46319277e-02]\n",
      " [1.04613284e-01 1.98598435e-06 5.91714956e-01 ... 1.10369867e-03\n",
      "  2.60979764e-03 3.88569233e-03]\n",
      " [9.71671180e-04 5.13675298e-05 5.12394200e-02 ... 1.00951928e-02\n",
      "  3.07782815e-01 1.28651909e-02]\n",
      " ...\n",
      " [6.25645316e-01 2.41465643e-01 1.06938955e-02 ... 3.84711209e-03\n",
      "  8.50057108e-05 3.01978876e-02]\n",
      " [1.67108122e-02 4.54361289e-04 1.87936512e-01 ... 1.43709599e-01\n",
      "  3.10753445e-02 3.57018025e-02]\n",
      " [2.29320558e-01 2.70127571e-01 2.01778647e-02 ... 1.46618299e-02\n",
      "  3.19533509e-03 3.90764087e-01]] & cached\n",
      "Re-used Cached Value, runNum =  331\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  331\n",
      "Provided input from cache for runNum = 331\n",
      "Provided input from cache for runNum = 332\n",
      "activation = [[ 5.7455974   5.73899488  6.34165458 ... 11.16481948  4.95019142\n",
      "   3.50717123]\n",
      " [ 0.          3.35339204  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13030853  3.79126988  0.96787343 ...  2.31492089  5.14636616\n",
      "   1.90467952]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.03657509  2.67115027  2.87853536 ...  1.02095075  7.06673011\n",
      "   0.17576934]\n",
      " [ 4.52019714  8.27547226  0.         ...  3.63731505  2.41827686\n",
      "   2.67343022]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3620504  0.         1.32695545 ... 0.         0.82771157 0.        ]\n",
      " [1.92899509 7.15642647 1.98783126 ... 7.75273351 2.95284208 6.08586896]\n",
      " ...\n",
      " [7.01673126 6.95415869 5.34811619 ... 3.88225851 7.87258173 4.33965127]\n",
      " [4.41402806 0.         7.28035909 ... 8.1934462  4.07749435 2.98260109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.91685811e-07 4.41633014e-07 3.42882165e-04 ... 1.52832433e-01\n",
      "  1.33231893e-04 1.44780242e-02]\n",
      " [1.03924195e-01 1.95425348e-06 5.93310574e-01 ... 1.08593706e-03\n",
      "  2.57517427e-03 3.84973172e-03]\n",
      " [9.69706931e-04 5.10655567e-05 5.12699834e-02 ... 1.00768228e-02\n",
      "  3.07119532e-01 1.28094239e-02]\n",
      " ...\n",
      " [6.26957225e-01 2.41420972e-01 1.06539833e-02 ... 3.84360429e-03\n",
      "  8.35913908e-05 2.99993291e-02]\n",
      " [1.66752350e-02 4.53800727e-04 1.87181748e-01 ... 1.43721826e-01\n",
      "  3.08937989e-02 3.56627670e-02]\n",
      " [2.28875459e-01 2.70750269e-01 2.00845303e-02 ... 1.46952431e-02\n",
      "  3.16998743e-03 3.91414859e-01]] & cached\n",
      "Re-used Cached Value, runNum =  332\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  332\n",
      "Provided input from cache for runNum = 332\n",
      "Provided input from cache for runNum = 333\n",
      "activation = [[ 5.75012932  5.7407099   6.34309327 ... 11.17482132  4.95256096\n",
      "   3.50913594]\n",
      " [ 0.          3.35416058  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12901924  3.78881333  0.96543154 ...  2.31710333  5.15008477\n",
      "   1.90572798]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.04077899  2.68159777  2.88561666 ...  1.02562595  7.07621821\n",
      "   0.18140213]\n",
      " [ 4.52628978  8.28548662  0.         ...  3.6441233   2.42206508\n",
      "   2.67936466]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36169887 0.         1.33202942 ... 0.         0.82908393 0.        ]\n",
      " [1.9319643  7.16298612 1.98585932 ... 7.7719433  2.9597074  6.09557481]\n",
      " ...\n",
      " [7.02681302 6.97478335 5.36047092 ... 3.89901042 7.88710466 4.35535236]\n",
      " [4.41874239 0.         7.28773394 ... 8.20756488 4.08167524 2.98438713]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.86556018e-07 4.28803505e-07 3.33463615e-04 ... 1.51933189e-01\n",
      "  1.31489327e-04 1.43269208e-02]\n",
      " [1.03259959e-01 1.92357598e-06 5.94954854e-01 ... 1.06872809e-03\n",
      "  2.54135063e-03 3.81465149e-03]\n",
      " [9.67741075e-04 5.07694819e-05 5.12924589e-02 ... 1.00576158e-02\n",
      "  3.06449549e-01 1.27535199e-02]\n",
      " ...\n",
      " [6.28239941e-01 2.41367732e-01 1.06128034e-02 ... 3.83971233e-03\n",
      "  8.21899669e-05 2.98004534e-02]\n",
      " [1.66378968e-02 4.53228953e-04 1.86386325e-01 ... 1.43704812e-01\n",
      "  3.07114325e-02 3.56213313e-02]\n",
      " [2.28436261e-01 2.71381472e-01 1.99905022e-02 ... 1.47285625e-02\n",
      "  3.14468228e-03 3.92057114e-01]] & cached\n",
      "Re-used Cached Value, runNum =  333\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  333\n",
      "Provided input from cache for runNum = 333\n",
      "Provided input from cache for runNum = 334\n",
      "activation = [[ 5.75469341  5.74245611  6.34454884 ... 11.18479266  4.95494912\n",
      "   3.51110786]\n",
      " [ 0.          3.35494248  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.1277511   3.78638345  0.96302815 ...  2.31932875  5.15381265\n",
      "   1.90679661]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.04503439  2.6920222   2.89269847 ...  1.03034824  7.08573085\n",
      "   0.18703273]\n",
      " [ 4.53234434  8.29543719  0.         ...  3.65092761  2.42579286\n",
      "   2.68527431]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36121568 0.         1.3369439  ... 0.         0.83036554 0.        ]\n",
      " [1.9350336  7.16961465 1.98400546 ... 7.79116743 2.96665149 6.105277  ]\n",
      " ...\n",
      " [7.03684859 6.99528671 5.37275836 ... 3.9156949  7.90156299 4.37099172]\n",
      " [4.42364486 0.         7.2951739  ... 8.22173521 4.08602744 2.9862573 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.81652594e-07 4.16439751e-07 3.24451390e-04 ... 1.51073647e-01\n",
      "  1.29813577e-04 1.41802546e-02]\n",
      " [1.02583017e-01 1.89294092e-06 5.96516098e-01 ... 1.05161477e-03\n",
      "  2.50795851e-03 3.77986919e-03]\n",
      " [9.66021406e-04 5.04782947e-05 5.13256592e-02 ... 1.00408082e-02\n",
      "  3.05798143e-01 1.26997738e-02]\n",
      " ...\n",
      " [6.29545690e-01 2.41309233e-01 1.05730748e-02 ... 3.83587681e-03\n",
      "  8.08124896e-05 2.96033450e-02]\n",
      " [1.66054934e-02 4.52680332e-04 1.85641339e-01 ... 1.43705610e-01\n",
      "  3.05347102e-02 3.55842112e-02]\n",
      " [2.27977247e-01 2.71980053e-01 1.98974745e-02 ... 1.47602871e-02\n",
      "  3.11945771e-03 3.92672484e-01]] & cached\n",
      "Re-used Cached Value, runNum =  334\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  334\n",
      "Provided input from cache for runNum = 334\n",
      "Provided input from cache for runNum = 335\n",
      "activation = [[ 5.7592269   5.74416187  6.34598795 ... 11.1946835   4.95727435\n",
      "   3.51305743]\n",
      " [ 0.          3.3557425   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12645382  3.78391901  0.96061365 ...  2.32151183  5.15746788\n",
      "   1.90785034]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.04931863  2.70249492  2.89976709 ...  1.03514525  7.09529581\n",
      "   0.19269179]\n",
      " [ 4.53840559  8.30540888  0.         ...  3.65781635  2.42954492\n",
      "   2.69120152]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36083345 0.         1.34196423 ... 0.         0.83185853 0.        ]\n",
      " [1.93805174 7.17615885 1.98211787 ... 7.8102689  2.9735111  6.11492166]\n",
      " ...\n",
      " [7.04684718 7.01576852 5.38499417 ... 3.93241436 7.91602653 4.38662391]\n",
      " [4.42840665 0.         7.30249164 ... 8.235622   4.09020398 2.98802263]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.76814106e-07 4.04329555e-07 3.15608043e-04 ... 1.50183968e-01\n",
      "  1.28124504e-04 1.40329229e-02]\n",
      " [1.01903039e-01 1.86292189e-06 5.98117695e-01 ... 1.03505216e-03\n",
      "  2.47509553e-03 3.74559074e-03]\n",
      " [9.63983327e-04 5.01789201e-05 5.13490463e-02 ... 1.00239627e-02\n",
      "  3.05127773e-01 1.26448704e-02]\n",
      " ...\n",
      " [6.30860134e-01 2.41267010e-01 1.05348970e-02 ... 3.83396388e-03\n",
      "  7.94591175e-05 2.94106850e-02]\n",
      " [1.65680353e-02 4.52057282e-04 1.84868980e-01 ... 1.43711228e-01\n",
      "  3.03536154e-02 3.55446855e-02]\n",
      " [2.27520548e-01 2.72585423e-01 1.98070376e-02 ... 1.47984118e-02\n",
      "  3.09464016e-03 3.93320515e-01]] & cached\n",
      "Re-used Cached Value, runNum =  335\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  335\n",
      "Provided input from cache for runNum = 335\n",
      "Provided input from cache for runNum = 336\n",
      "activation = [[ 5.76373068  5.74583087  6.34740968 ... 11.20449626  4.95954105\n",
      "   3.51498375]\n",
      " [ 0.          3.35654342  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12515693  3.78144825  0.95820854 ...  2.32369772  5.16110147\n",
      "   1.9089048 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.05359923  2.71293743  2.90680347 ...  1.03993634  7.10484549\n",
      "   0.19834136]\n",
      " [ 4.54443647  8.31534859  0.         ...  3.66470232  2.43327488\n",
      "   2.69710964]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36050438 0.         1.34700833 ... 0.         0.83343351 0.        ]\n",
      " [1.94102844 7.18261131 1.98021263 ... 7.82925739 2.98030639 6.12450744]\n",
      " ...\n",
      " [7.05677378 7.03615439 5.3971603  ... 3.94907021 7.93042255 4.40220144]\n",
      " [4.43307563 0.         7.30971499 ... 8.24931176 4.09425329 2.98971515]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.72107625e-07 3.92576146e-07 3.07007285e-04 ... 1.49299508e-01\n",
      "  1.26457261e-04 1.38873765e-02]\n",
      " [1.01229238e-01 1.83363233e-06 5.99729004e-01 ... 1.01887657e-03\n",
      "  2.44275319e-03 3.71181962e-03]\n",
      " [9.61897527e-04 4.98792593e-05 5.13696395e-02 ... 1.00075555e-02\n",
      "  3.04447566e-01 1.25899989e-02]\n",
      " ...\n",
      " [6.32161730e-01 2.41221273e-01 1.04971279e-02 ... 3.83255945e-03\n",
      "  7.81287301e-05 2.92192080e-02]\n",
      " [1.65290198e-02 4.51395636e-04 1.84085025e-01 ... 1.43711079e-01\n",
      "  3.01718458e-02 3.55036995e-02]\n",
      " [2.27071203e-01 2.73192029e-01 1.97178332e-02 ... 1.48386319e-02\n",
      "  3.07014088e-03 3.93972238e-01]] & cached\n",
      "Re-used Cached Value, runNum =  336\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  336\n",
      "Provided input from cache for runNum = 336\n",
      "Provided input from cache for runNum = 337\n",
      "activation = [[ 5.76823184  5.74749467  6.34882767 ... 11.21427036  4.96178399\n",
      "   3.51690325]\n",
      " [ 0.          3.35734767  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12385729  3.77898049  0.95581321 ...  2.32588453  5.16471027\n",
      "   1.90996022]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.05779655  2.72322961  2.91376613 ...  1.04458752  7.11427168\n",
      "   0.20390718]\n",
      " [ 4.55045569  8.32527312  0.         ...  3.67162134  2.43699584\n",
      "   2.70301384]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.36017899 0.         1.35202844 ... 0.         0.83499392 0.        ]\n",
      " [1.94413049 7.1891471  1.97841577 ... 7.84833198 2.98721118 6.13412043]\n",
      " ...\n",
      " [7.06656284 7.05632921 5.40921708 ... 3.96554852 7.94467794 4.41766505]\n",
      " [4.43770459 0.         7.31688537 ... 8.26285196 4.09823889 2.99136341]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.67562700e-07 3.81256652e-07 2.98709552e-04 ... 1.48436501e-01\n",
      "  1.24851525e-04 1.37446941e-02]\n",
      " [1.00540866e-01 1.80467102e-06 6.01308288e-01 ... 1.00279365e-03\n",
      "  2.41076667e-03 3.67820287e-03]\n",
      " [9.59699736e-04 4.95760802e-05 5.13882904e-02 ... 9.98936776e-03\n",
      "  3.03758261e-01 1.25342283e-02]\n",
      " ...\n",
      " [6.33476123e-01 2.41194311e-01 1.04605529e-02 ... 3.83163940e-03\n",
      "  7.68313397e-05 2.90302048e-02]\n",
      " [1.64898406e-02 4.50736103e-04 1.83312739e-01 ... 1.43699552e-01\n",
      "  2.99932657e-02 3.54620731e-02]\n",
      " [2.26623721e-01 2.73803974e-01 1.96308525e-02 ... 1.48798068e-02\n",
      "  3.04626437e-03 3.94624627e-01]] & cached\n",
      "Re-used Cached Value, runNum =  337\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  337\n",
      "Provided input from cache for runNum = 337\n",
      "Provided input from cache for runNum = 338\n",
      "activation = [[ 5.77273614  5.74918232  6.35024673 ... 11.22402539  4.96402264\n",
      "   3.51882095]\n",
      " [ 0.          3.35814082  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12250817  3.77643499  0.95339904 ...  2.32800755  5.16823997\n",
      "   1.91098942]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.06200487  2.73353209  2.9207079  ...  1.04926376  7.12370172\n",
      "   0.2094892 ]\n",
      " [ 4.55643052  8.33513237  0.         ...  3.6785084   2.4406624\n",
      "   2.70888834]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35986285 0.         1.35702712 ... 0.         0.83658641 0.        ]\n",
      " [1.94715565 7.19553986 1.97656632 ... 7.86722463 2.99400599 6.14364421]\n",
      " ...\n",
      " [7.07632584 7.07647924 5.42122336 ... 3.98199684 7.95890582 4.4331095 ]\n",
      " [4.44232059 0.         7.32400534 ... 8.276307   4.10220107 2.99298828]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.63148323e-07 3.70298103e-07 2.90646333e-04 ... 1.47576299e-01\n",
      "  1.23263702e-04 1.36040564e-02]\n",
      " [9.98544947e-02 1.77646093e-06 6.02856816e-01 ... 9.87008889e-04\n",
      "  2.37932229e-03 3.64514589e-03]\n",
      " [9.57577745e-04 4.92833505e-05 5.14085622e-02 ... 9.97265088e-03\n",
      "  3.03075987e-01 1.24799194e-02]\n",
      " ...\n",
      " [6.34801735e-01 2.41207649e-01 1.04247472e-02 ... 3.83123918e-03\n",
      "  7.55457421e-05 2.88442714e-02]\n",
      " [1.64544273e-02 4.50260643e-04 1.82573070e-01 ... 1.43731077e-01\n",
      "  2.98149264e-02 3.54272139e-02]\n",
      " [2.26160690e-01 2.74425586e-01 1.95433287e-02 ... 1.49204615e-02\n",
      "  3.02224182e-03 3.95270130e-01]] & cached\n",
      "Re-used Cached Value, runNum =  338\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  338\n",
      "Provided input from cache for runNum = 338\n",
      "Provided input from cache for runNum = 339\n",
      "activation = [[ 5.77723171  5.7508621   6.35165354 ... 11.23372708  4.96623095\n",
      "   3.52072441]\n",
      " [ 0.          3.35893497  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.12116023  3.77388702  0.9509974  ...  2.33013857  5.17174712\n",
      "   1.91202383]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.06627701  2.7439425   2.9276725  ...  1.05409913  7.13320986\n",
      "   0.2151203 ]\n",
      " [ 4.56241717  8.34500145  0.         ...  3.68545289  2.44436346\n",
      "   2.71478061]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3594895  0.         1.36198172 ... 0.         0.83818737 0.        ]\n",
      " [1.95019492 7.20189446 1.97471529 ... 7.88600293 3.00076759 6.15312335]\n",
      " ...\n",
      " [7.08610432 7.09667006 5.43319857 ... 3.99851988 7.97317302 4.44856509]\n",
      " [4.4469204  0.         7.33109365 ... 8.28967088 4.10613051 2.9945888 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.58899007e-07 3.59692086e-07 2.82843792e-04 ... 1.46720583e-01\n",
      "  1.21687306e-04 1.34649297e-02]\n",
      " [9.91799865e-02 1.74874301e-06 6.04406158e-01 ... 9.71605526e-04\n",
      "  2.34820534e-03 3.61253292e-03]\n",
      " [9.55711474e-04 4.90031941e-05 5.14349828e-02 ... 9.95877404e-03\n",
      "  3.02388195e-01 1.24268869e-02]\n",
      " ...\n",
      " [6.36089314e-01 2.41161761e-01 1.03881970e-02 ... 3.83083075e-03\n",
      "  7.42726005e-05 2.86576812e-02]\n",
      " [1.64190966e-02 4.49742515e-04 1.81815028e-01 ... 1.43748290e-01\n",
      "  2.96332140e-02 3.53889790e-02]\n",
      " [2.25717071e-01 2.75028160e-01 1.94559204e-02 ... 1.49621602e-02\n",
      "  2.99821618e-03 3.95906446e-01]] & cached\n",
      "Re-used Cached Value, runNum =  339\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  339\n",
      "Provided input from cache for runNum = 339\n",
      "Provided input from cache for runNum = 340\n",
      "activation = [[ 5.78168707  5.75249141  6.3530331  ... 11.24334111  4.96837437\n",
      "   3.52259882]\n",
      " [ 0.          3.35971198  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11981261  3.77135106  0.94860275 ...  2.3322889   5.17525002\n",
      "   1.91306168]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.07048167  2.75421824  2.93457685 ...  1.05883905  7.14263348\n",
      "   0.2206947 ]\n",
      " [ 4.56839887  8.35485955  0.         ...  3.6924302   2.44805612\n",
      "   2.72065609]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35928386 0.         1.36706335 ... 0.         0.83994461 0.        ]\n",
      " [1.95326507 7.20825947 1.97288832 ... 7.90476042 3.00753955 6.1625828 ]\n",
      " ...\n",
      " [7.09578867 7.11670505 5.44511152 ... 4.01496574 7.98734583 4.46394772]\n",
      " [4.45145028 0.         7.33811203 ... 8.30284925 4.10995826 2.99613953]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.54782253e-07 3.49481210e-07 2.75274822e-04 ... 1.45886894e-01\n",
      "  1.20137199e-04 1.33288106e-02]\n",
      " [9.85064634e-02 1.72137988e-06 6.05945508e-01 ... 9.56335482e-04\n",
      "  2.31759581e-03 3.58015146e-03]\n",
      " [9.53899974e-04 4.87269356e-05 5.14644239e-02 ... 9.94561998e-03\n",
      "  3.01686342e-01 1.23742661e-02]\n",
      " ...\n",
      " [6.37368316e-01 2.41108264e-01 1.03516981e-02 ... 3.83071925e-03\n",
      "  7.30240742e-05 2.84715179e-02]\n",
      " [1.63829306e-02 4.49217437e-04 1.81052598e-01 ... 1.43759991e-01\n",
      "  2.94504123e-02 3.53500077e-02]\n",
      " [2.25280101e-01 2.75621886e-01 1.93692300e-02 ... 1.50046288e-02\n",
      "  2.97461743e-03 3.96531579e-01]] & cached\n",
      "Re-used Cached Value, runNum =  340\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  340\n",
      "Provided input from cache for runNum = 340\n",
      "Provided input from cache for runNum = 341\n",
      "activation = [[ 5.78612833  5.7540931   6.35440228 ... 11.25288923  4.97047521\n",
      "   3.52445685]\n",
      " [ 0.          3.36057306  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11852454  3.76891672  0.94625698 ...  2.33453959  5.1788107\n",
      "   1.91414036]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.07463525  2.76440781  2.94142196 ...  1.06352455  7.15201215\n",
      "   0.22621704]\n",
      " [ 4.57433843  8.36464487  0.         ...  3.69935919  2.45168573\n",
      "   2.72649877]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3588559  0.         1.37193975 ... 0.         0.84156801 0.        ]\n",
      " [1.9564074  7.21470908 1.97112677 ... 7.92347623 3.01430492 6.17203066]\n",
      " ...\n",
      " [7.10527026 7.13643716 5.45685642 ... 4.03116566 8.00130238 4.47918211]\n",
      " [4.45594486 0.         7.34506169 ... 8.31590213 4.11372837 2.99765324]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.50893201e-07 3.39857407e-07 2.68075211e-04 ... 1.45113628e-01\n",
      "  1.18662403e-04 1.31979294e-02]\n",
      " [9.78214107e-02 1.69416239e-06 6.07403985e-01 ... 9.41022181e-04\n",
      "  2.28766395e-03 3.54783274e-03]\n",
      " [9.52337109e-04 4.84723953e-05 5.15037171e-02 ... 9.93381652e-03\n",
      "  3.00974546e-01 1.23230176e-02]\n",
      " ...\n",
      " [6.38647429e-01 2.41015038e-01 1.03157890e-02 ... 3.82979760e-03\n",
      "  7.18056991e-05 2.82843438e-02]\n",
      " [1.63512557e-02 4.48787020e-04 1.80331210e-01 ... 1.43765565e-01\n",
      "  2.92730581e-02 3.53120807e-02]\n",
      " [2.24844387e-01 2.76189892e-01 1.92834246e-02 ... 1.50435538e-02\n",
      "  2.95154804e-03 3.97124753e-01]] & cached\n",
      "Re-used Cached Value, runNum =  341\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  341\n",
      "Provided input from cache for runNum = 341\n",
      "Provided input from cache for runNum = 342\n",
      "activation = [[ 5.7905404   5.7556436   6.35574771 ... 11.262351    4.97251166\n",
      "   3.52628613]\n",
      " [ 0.          3.36143398  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11721051  3.76647246  0.94390337 ...  2.33678139  5.18233454\n",
      "   1.91520998]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.07870998  2.77445292  2.94820501 ...  1.06809441  7.1613037\n",
      "   0.23166119]\n",
      " [ 4.58031862  8.37450944  0.         ...  3.70641922  2.455387\n",
      "   2.73240215]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35854133 0.         1.37692431 ... 0.         0.84330994 0.        ]\n",
      " [1.95958396 7.22121188 1.96936943 ... 7.94219234 3.02108406 6.18148768]\n",
      " ...\n",
      " [7.11461693 7.15595774 5.46849792 ... 4.04721108 8.01512801 4.49430826]\n",
      " [4.46029747 0.         7.35190724 ... 8.32864785 4.11731476 2.99903209]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.47083185e-07 3.30553126e-07 2.61027540e-04 ... 1.44333207e-01\n",
      "  1.17213668e-04 1.30676218e-02]\n",
      " [9.71429850e-02 1.66755993e-06 6.08915159e-01 ... 9.26060199e-04\n",
      "  2.25837355e-03 3.51584991e-03]\n",
      " [9.50461445e-04 4.82110042e-05 5.15308116e-02 ... 9.91960914e-03\n",
      "  3.00242635e-01 1.22693954e-02]\n",
      " ...\n",
      " [6.39919753e-01 2.40940411e-01 1.02796299e-02 ... 3.82987506e-03\n",
      "  7.06227548e-05 2.80998978e-02]\n",
      " [1.63145853e-02 4.48287150e-04 1.79568235e-01 ... 1.43740244e-01\n",
      "  2.90957801e-02 3.52682239e-02]\n",
      " [2.24416902e-01 2.76782074e-01 1.91981156e-02 ... 1.50865162e-02\n",
      "  2.92930387e-03 3.97740660e-01]] & cached\n",
      "Re-used Cached Value, runNum =  342\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  342\n",
      "Provided input from cache for runNum = 342\n",
      "Provided input from cache for runNum = 343\n",
      "activation = [[ 5.79494065  5.75718609  6.35708166 ... 11.27175474  4.97451883\n",
      "   3.52809978]\n",
      " [ 0.          3.36229729  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11592975  3.76408876  0.94159113 ...  2.33910113  5.18588397\n",
      "   1.91631187]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.08285217  2.78454328  2.95498231 ...  1.07274067  7.17064196\n",
      "   0.23711243]\n",
      " [ 4.58622699  8.38427955  0.         ...  3.71342038  2.45897214\n",
      "   2.73826374]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35813204 0.         1.38177272 ... 0.         0.84497168 0.        ]\n",
      " [1.96262793 7.22754528 1.96754579 ... 7.96065272 3.02767812 6.19082724]\n",
      " ...\n",
      " [7.12381624 7.17523812 5.47998882 ... 4.06305604 8.02877009 4.50929709]\n",
      " [4.4646369  0.         7.35868339 ... 8.34128016 4.12089944 3.00037474]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.43453296e-07 3.21738842e-07 2.54245129e-04 ... 1.43590807e-01\n",
      "  1.15817693e-04 1.29416751e-02]\n",
      " [9.64927531e-02 1.64200829e-06 6.10400751e-01 ... 9.11402802e-04\n",
      "  2.23006342e-03 3.48463040e-03]\n",
      " [9.48928745e-04 4.79757814e-05 5.15612009e-02 ... 9.90732521e-03\n",
      "  2.99518159e-01 1.22177910e-02]\n",
      " ...\n",
      " [6.41162357e-01 2.40851434e-01 1.02417363e-02 ... 3.82867828e-03\n",
      "  6.94505848e-05 2.79143306e-02]\n",
      " [1.62842262e-02 4.47977076e-04 1.78828305e-01 ... 1.43725431e-01\n",
      "  2.89240918e-02 3.52286970e-02]\n",
      " [2.23981866e-01 2.77373006e-01 1.91096948e-02 ... 1.51248592e-02\n",
      "  2.90691666e-03 3.98320303e-01]] & cached\n",
      "Re-used Cached Value, runNum =  343\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  343\n",
      "Provided input from cache for runNum = 343\n",
      "Provided input from cache for runNum = 344\n",
      "activation = [[ 5.79935988  5.75878867  6.35842589 ... 11.28119533  4.97655623\n",
      "   3.52993755]\n",
      " [ 0.          3.36312147  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11459908  3.76160422  0.9392593  ...  2.34130499  5.18932702\n",
      "   1.91736279]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.08694517  2.79449765  2.96170965 ...  1.07729573  7.17990077\n",
      "   0.2424978 ]\n",
      " [ 4.59213118  8.39404809  0.         ...  3.720456    2.46256648\n",
      "   2.74412953]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35775462 0.         1.38662998 ... 0.         0.84668458 0.        ]\n",
      " [1.96566814 7.23385413 1.965725   ... 7.97908585 3.03429081 6.20015926]\n",
      " ...\n",
      " [7.13292398 7.19437221 5.49140577 ... 4.07884161 8.04234037 4.52421577]\n",
      " [4.46902991 0.         7.36546818 ... 8.35390453 4.12456586 3.00176139]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.39846587e-07 3.13018072e-07 2.47569504e-04 ... 1.42804017e-01\n",
      "  1.14451452e-04 1.28149555e-02]\n",
      " [9.58362923e-02 1.61704818e-06 6.11924786e-01 ... 8.97221607e-04\n",
      "  2.20231980e-03 3.45406730e-03]\n",
      " [9.46822623e-04 4.77124515e-05 5.15731503e-02 ... 9.89181604e-03\n",
      "  2.98814238e-01 1.21635415e-02]\n",
      " ...\n",
      " [6.42455925e-01 2.40869948e-01 1.02061914e-02 ... 3.82983765e-03\n",
      "  6.83120886e-05 2.77363887e-02]\n",
      " [1.62493143e-02 4.47608189e-04 1.78073078e-01 ... 1.43723176e-01\n",
      "  2.87560003e-02 3.51895550e-02]\n",
      " [2.23514651e-01 2.78003397e-01 1.90224577e-02 ... 1.51679489e-02\n",
      "  2.88497341e-03 3.98925602e-01]] & cached\n",
      "Re-used Cached Value, runNum =  344\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  344\n",
      "Provided input from cache for runNum = 344\n",
      "Provided input from cache for runNum = 345\n",
      "activation = [[ 5.80379499  5.76042398  6.35977961 ... 11.29062707  4.9785995\n",
      "   3.53178558]\n",
      " [ 0.          3.36391952  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11323631  3.7590533   0.93693337 ...  2.34345426  5.19270342\n",
      "   1.91838933]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.09106694  2.80444913  2.96842758 ...  1.08189926  7.18917261\n",
      "   0.24789337]\n",
      " [ 4.59799697  8.40376567  0.         ...  3.72747605  2.46611314\n",
      "   2.74996972]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3573978  0.         1.39147811 ... 0.         0.84846502 0.        ]\n",
      " [1.96866885 7.24009106 1.963884   ... 7.99740658 3.0408406  6.20943869]\n",
      " ...\n",
      " [7.14208426 7.21356917 5.50282179 ... 4.09473272 8.05596913 4.53916046]\n",
      " [4.4734714  0.         7.37224191 ... 8.36652655 4.12827752 3.0031787 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.36325513e-07 3.04482634e-07 2.41043350e-04 ... 1.41994187e-01\n",
      "  1.13086725e-04 1.26887409e-02]\n",
      " [9.52016509e-02 1.59292992e-06 6.13466936e-01 ... 8.83496085e-04\n",
      "  2.17492222e-03 3.42427613e-03]\n",
      " [9.44737905e-04 4.74471339e-05 5.15801995e-02 ... 9.87673991e-03\n",
      "  2.98111311e-01 1.21095059e-02]\n",
      " ...\n",
      " [6.43715010e-01 2.40892498e-01 1.01699844e-02 ... 3.83111416e-03\n",
      "  6.71740898e-05 2.75583586e-02]\n",
      " [1.62153736e-02 4.47263996e-04 1.77308682e-01 ... 1.43729562e-01\n",
      "  2.85857286e-02 3.51512591e-02]\n",
      " [2.23058713e-01 2.78667505e-01 1.89349481e-02 ... 1.52122692e-02\n",
      "  2.86269728e-03 3.99528496e-01]] & cached\n",
      "Re-used Cached Value, runNum =  345\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  345\n",
      "Provided input from cache for runNum = 345\n",
      "Provided input from cache for runNum = 346\n",
      "activation = [[ 5.80820955  5.76203382  6.36112513 ... 11.30000149  4.98059928\n",
      "   3.53362014]\n",
      " [ 0.          3.36474744  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11188668  3.75652754  0.93464189 ...  2.34562917  5.19608066\n",
      "   1.9194337 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.09522395  2.81441558  2.97514206 ...  1.08656426  7.19848909\n",
      "   0.25329887]\n",
      " [ 4.6038841   8.4135246   0.         ...  3.73457866  2.46969088\n",
      "   2.75584082]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35693621 0.         1.39621628 ... 0.         0.85020084 0.        ]\n",
      " [1.97170403 7.24634096 1.96208156 ... 8.01567338 3.04737054 6.2186911 ]\n",
      " ...\n",
      " [7.15125907 7.2327669  5.51421829 ... 4.11066843 8.06963102 4.55409727]\n",
      " [4.47785411 0.         7.37896788 ... 8.37899149 4.13190069 3.00453974]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.32943153e-07 2.96211543e-07 2.34767922e-04 ... 1.41206868e-01\n",
      "  1.11731729e-04 1.25647485e-02]\n",
      " [9.45632362e-02 1.56883246e-06 6.14962351e-01 ... 8.69938070e-04\n",
      "  2.14763156e-03 3.39463497e-03]\n",
      " [9.42963464e-04 4.71922488e-05 5.16005709e-02 ... 9.86522129e-03\n",
      "  2.97398543e-01 1.20572816e-02]\n",
      " ...\n",
      " [6.44955024e-01 2.40839585e-01 1.01338815e-02 ... 3.83248908e-03\n",
      "  6.60538640e-05 2.73798584e-02]\n",
      " [1.61831150e-02 4.46846538e-04 1.76560519e-01 ... 1.43736082e-01\n",
      "  2.84142857e-02 3.51116238e-02]\n",
      " [2.22614780e-01 2.79271834e-01 1.88484952e-02 ... 1.52575695e-02\n",
      "  2.84052952e-03 4.00116221e-01]] & cached\n",
      "Re-used Cached Value, runNum =  346\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  346\n",
      "Provided input from cache for runNum = 346\n",
      "Provided input from cache for runNum = 347\n",
      "activation = [[ 5.812654    5.76369693  6.36248839 ... 11.30940212  4.98262215\n",
      "   3.53547536]\n",
      " [ 0.          3.36556489  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.11046617  3.7539155   0.9323153  ...  2.34771429  5.1993635\n",
      "   1.92043509]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.09937091  2.82431517  2.98184298 ...  1.09121663  7.20775573\n",
      "   0.25867592]\n",
      " [ 4.6097079   8.42319265  0.         ...  3.74161724  2.47319181\n",
      "   2.76166481]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35663018 0.         1.40104037 ... 0.         0.85207049 0.        ]\n",
      " [1.97468302 7.2524952  1.96025865 ... 8.03383099 3.05384941 6.22789459]\n",
      " ...\n",
      " [7.1603611  7.25182819 5.52555745 ... 4.12652167 8.08319335 4.56895992]\n",
      " [4.48228469 0.         7.38567475 ... 8.39145326 4.13555674 3.00592212]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.29667591e-07 2.88281643e-07 2.28662438e-04 ... 1.40428489e-01\n",
      "  1.10409349e-04 1.24439063e-02]\n",
      " [9.39463808e-02 1.54566907e-06 6.16448833e-01 ... 8.56537240e-04\n",
      "  2.12101899e-03 3.36554153e-03]\n",
      " [9.41236167e-04 4.69459239e-05 5.16156959e-02 ... 9.85214923e-03\n",
      "  2.96674519e-01 1.20051553e-02]\n",
      " ...\n",
      " [6.46177820e-01 2.40850955e-01 1.00964760e-02 ... 3.83302527e-03\n",
      "  6.49342675e-05 2.72021392e-02]\n",
      " [1.61546544e-02 4.46658502e-04 1.75826165e-01 ... 1.43763974e-01\n",
      "  2.82425089e-02 3.50781337e-02]\n",
      " [2.22167301e-01 2.79933643e-01 1.87600407e-02 ... 1.52988882e-02\n",
      "  2.81815534e-03 4.00684232e-01]] & cached\n",
      "Re-used Cached Value, runNum =  347\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  347\n",
      "Provided input from cache for runNum = 347\n",
      "Provided input from cache for runNum = 348\n",
      "activation = [[ 5.81709583  5.76535779  6.36385796 ... 11.31877293  4.98462247\n",
      "   3.53732527]\n",
      " [ 0.          3.36644158  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10904257  3.75128954  0.92999637 ...  2.34977312  5.20261052\n",
      "   1.92143304]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.10351188  2.83422955  2.98851641 ...  1.09590711  7.21702098\n",
      "   0.26405309]\n",
      " [ 4.61553395  8.43286599  0.         ...  3.74869401  2.47669242\n",
      "   2.76750242]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.356198   0.         1.40574986 ... 0.         0.85387586 0.        ]\n",
      " [1.97777826 7.25873738 1.95853598 ... 8.05202042 3.06039742 6.23711127]\n",
      " ...\n",
      " [7.16945427 7.27091908 5.53687971 ... 4.14245872 8.09679614 4.58383014]\n",
      " [4.48667732 0.         7.39234787 ... 8.40382061 4.13916942 3.00727132]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.26514166e-07 2.80597404e-07 2.22770795e-04 ... 1.39653402e-01\n",
      "  1.09109206e-04 1.23244296e-02]\n",
      " [9.33296261e-02 1.52280398e-06 6.17920138e-01 ... 8.43382857e-04\n",
      "  2.09456033e-03 3.33675761e-03]\n",
      " [9.39569081e-04 4.67038276e-05 5.16321124e-02 ... 9.83953074e-03\n",
      "  2.95930789e-01 1.19531423e-02]\n",
      " ...\n",
      " [6.47378741e-01 2.40813573e-01 1.00589713e-02 ... 3.83305447e-03\n",
      "  6.38274044e-05 2.70236465e-02]\n",
      " [1.61255009e-02 4.46404957e-04 1.75085230e-01 ... 1.43765906e-01\n",
      "  2.80689111e-02 3.50409594e-02]\n",
      " [2.21737309e-01 2.80583909e-01 1.86730138e-02 ... 1.53407045e-02\n",
      "  2.79591676e-03 4.01244678e-01]] & cached\n",
      "Re-used Cached Value, runNum =  348\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  348\n",
      "Provided input from cache for runNum = 348\n",
      "Provided input from cache for runNum = 349\n",
      "activation = [[ 5.82152082  5.76700293  6.36521112 ... 11.32809139  4.98658905\n",
      "   3.53915724]\n",
      " [ 0.          3.36735597  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10762392  3.7486792   0.92769074 ...  2.35185195  5.20585148\n",
      "   1.92244049]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.1076212   2.84410352  2.99516317 ...  1.10057853  7.22625013\n",
      "   0.26941334]\n",
      " [ 4.62132014  8.44248007  0.         ...  3.75575372  2.4801377\n",
      "   2.77331167]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35583635 0.         1.41050137 ... 0.         0.85576834 0.        ]\n",
      " [1.98085807 7.26495433 1.95679974 ... 8.0701106  3.06689699 6.24627211]\n",
      " ...\n",
      " [7.17843296 7.28986909 5.54811203 ... 4.15830053 8.11030447 4.59862934]\n",
      " [4.49106308 0.         7.39897843 ... 8.41611656 4.14278223 3.00861477]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.23478632e-07 2.73222801e-07 2.17069426e-04 ... 1.38904976e-01\n",
      "  1.07837159e-04 1.22078894e-02]\n",
      " [9.27205115e-02 1.50028456e-06 6.19381085e-01 ... 8.30458495e-04\n",
      "  2.06866570e-03 3.30844736e-03]\n",
      " [9.38081966e-04 4.64730749e-05 5.16529367e-02 ... 9.82819369e-03\n",
      "  2.95193105e-01 1.19024095e-02]\n",
      " ...\n",
      " [6.48578166e-01 2.40766394e-01 1.00211722e-02 ... 3.83307659e-03\n",
      "  6.27373694e-05 2.68465503e-02]\n",
      " [1.60980229e-02 4.46208303e-04 1.74347745e-01 ... 1.43770840e-01\n",
      "  2.78957988e-02 3.50051313e-02]\n",
      " [2.21297273e-01 2.81208844e-01 1.85850060e-02 ... 1.53814377e-02\n",
      "  2.77374708e-03 4.01785718e-01]] & cached\n",
      "Re-used Cached Value, runNum =  349\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  349\n",
      "Provided input from cache for runNum = 349\n",
      "Provided input from cache for runNum = 350\n",
      "activation = [[ 5.82594762  5.76866003  6.36656232 ... 11.33738589  4.9885338\n",
      "   3.54098703]\n",
      " [ 0.          3.36829981  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10620098  3.74606327  0.92539492 ...  2.35392601  5.20907302\n",
      "   1.92344889]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.11164448  2.8538519   3.00174248 ...  1.10513067  7.23538424\n",
      "   0.27470895]\n",
      " [ 4.62708282  8.4520583   0.         ...  3.76281431  2.48356316\n",
      "   2.77910301]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35540283 0.         1.41517406 ... 0.         0.85761067 0.        ]\n",
      " [1.98397667 7.27120597 1.95509477 ... 8.08820136 3.07342161 6.25542851]\n",
      " ...\n",
      " [7.18727305 7.3086272  5.55923886 ... 4.17398934 8.12368627 4.6133278 ]\n",
      " [4.49542154 0.         7.40555752 ... 8.42830319 4.14634803 3.00993423]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.20513793e-07 2.66052663e-07 2.11536216e-04 ... 1.38162451e-01\n",
      "  1.06606518e-04 1.20928381e-02]\n",
      " [9.20983371e-02 1.47803633e-06 6.20837343e-01 ... 8.17759591e-04\n",
      "  2.04318494e-03 3.28043033e-03]\n",
      " [9.36271233e-04 4.62321380e-05 5.16668717e-02 ... 9.81524246e-03\n",
      "  2.94454397e-01 1.18505330e-02]\n",
      " ...\n",
      " [6.49810344e-01 2.40756701e-01 9.98545160e-03 ... 3.83431261e-03\n",
      "  6.16807123e-05 2.66733969e-02]\n",
      " [1.60677460e-02 4.45967995e-04 1.73611563e-01 ... 1.43768499e-01\n",
      "  2.77256089e-02 3.49680335e-02]\n",
      " [2.20842798e-01 2.81841047e-01 1.84991748e-02 ... 1.54247552e-02\n",
      "  2.75219452e-03 4.02336267e-01]] & cached\n",
      "Re-used Cached Value, runNum =  350\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  350\n",
      "Provided input from cache for runNum = 350\n",
      "Provided input from cache for runNum = 351\n",
      "activation = [[ 5.83035147  5.77027788  6.36789568 ... 11.34659955  4.99041979\n",
      "   3.54279514]\n",
      " [ 0.          3.36923656  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10479457  3.74348592  0.92311766 ...  2.35604787  5.21231148\n",
      "   1.9244744 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.11567965  2.86360523  3.00832904 ...  1.10971319  7.24452815\n",
      "   0.28001361]\n",
      " [ 4.63286166  8.46166186  0.         ...  3.76995452  2.48701818\n",
      "   2.78491404]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35491381 0.         1.41981715 ... 0.         0.8594451  0.        ]\n",
      " [1.98706598 7.27738838 1.95335028 ... 8.10618602 3.07989708 6.26452972]\n",
      " ...\n",
      " [7.19608372 7.32734079 5.57033874 ... 4.18965504 8.13705589 4.62799749]\n",
      " [4.4997225  0.         7.41208548 ... 8.4403017  4.14981146 3.01119555]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.17627713e-07 2.59046006e-07 2.06157414e-04 ... 1.37430830e-01\n",
      "  1.05382646e-04 1.19788993e-02]\n",
      " [9.14693726e-02 1.45589968e-06 6.22285522e-01 ... 8.05318959e-04\n",
      "  2.01795880e-03 3.25266475e-03]\n",
      " [9.34526289e-04 4.59935002e-05 5.16873518e-02 ... 9.80517944e-03\n",
      "  2.93716163e-01 1.17997554e-02]\n",
      " ...\n",
      " [6.51047971e-01 2.40712007e-01 9.95062948e-03 ... 3.83678486e-03\n",
      "  6.06516100e-05 2.65022454e-02]\n",
      " [1.60365107e-02 4.45636785e-04 1.72875923e-01 ... 1.43771310e-01\n",
      "  2.75552380e-02 3.49292127e-02]\n",
      " [2.20386216e-01 2.82428256e-01 1.84144246e-02 ... 1.54717446e-02\n",
      "  2.73104859e-03 4.02889986e-01]] & cached\n",
      "Re-used Cached Value, runNum =  351\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  351\n",
      "Provided input from cache for runNum = 351\n",
      "Provided input from cache for runNum = 352\n",
      "activation = [[ 5.8347295   5.7718463   6.36920635 ... 11.35574402  4.99225666\n",
      "   3.54457521]\n",
      " [ 0.          3.37018332  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10340155  3.74096553  0.92085596 ...  2.35822183  5.21556903\n",
      "   1.92552205]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.11967992  2.87328567  3.01487634 ...  1.11425385  7.25363262\n",
      "   0.28527833]\n",
      " [ 4.638562    8.47115719  0.         ...  3.7770097   2.49037337\n",
      "   2.79067116]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35446818 0.         1.42447309 ... 0.         0.86133201 0.        ]\n",
      " [1.99014707 7.28354912 1.9516108  ... 8.12409066 3.08633175 6.27359029]\n",
      " ...\n",
      " [7.20476889 7.34585255 5.58132588 ... 4.20513888 8.15029296 4.64254808]\n",
      " [4.50404866 0.         7.41856457 ... 8.452259   4.15329613 3.01246386]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.14880343e-07 2.52406346e-07 2.00962084e-04 ... 1.36737841e-01\n",
      "  1.04198226e-04 1.18690290e-02]\n",
      " [9.08755915e-02 1.43458205e-06 6.23743216e-01 ... 7.93063894e-04\n",
      "  1.99337337e-03 3.22543920e-03]\n",
      " [9.33070645e-04 4.57721776e-05 5.17068374e-02 ... 9.79460642e-03\n",
      "  2.92962897e-01 1.17497282e-02]\n",
      " ...\n",
      " [6.52212307e-01 2.40612276e-01 9.91310689e-03 ... 3.83708810e-03\n",
      "  5.96266209e-05 2.63262931e-02]\n",
      " [1.60079646e-02 4.45352500e-04 1.72121942e-01 ... 1.43733131e-01\n",
      "  2.73858392e-02 3.48882183e-02]\n",
      " [2.19961435e-01 2.83043543e-01 1.83282393e-02 ... 1.55140894e-02\n",
      "  2.70986905e-03 4.03407336e-01]] & cached\n",
      "Re-used Cached Value, runNum =  352\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  352\n",
      "Provided input from cache for runNum = 352\n",
      "Provided input from cache for runNum = 353\n",
      "activation = [[ 5.83909307  5.77338605  6.37051005 ... 11.36483206  4.99404444\n",
      "   3.5463449 ]\n",
      " [ 0.          3.37113169  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10200971  3.73845644  0.91860838 ...  2.36040099  5.21881235\n",
      "   1.92656892]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.12369936  2.88300541  3.02140481 ...  1.11885529  7.26275874\n",
      "   0.29055138]\n",
      " [ 4.6442305   8.48060108  0.         ...  3.78403412  2.49370376\n",
      "   2.79640301]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35410482 0.         1.42915141 ... 0.         0.86334487 0.        ]\n",
      " [1.9931772  7.28959951 1.94985176 ... 8.14183212 3.09268046 6.28257904]\n",
      " ...\n",
      " [7.21334405 7.36422518 5.59220366 ... 4.22051484 8.16343191 4.65701655]\n",
      " [4.50830846 0.         7.42496094 ... 8.464072   4.15668183 3.01368495]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.12220734e-07 2.46046592e-07 1.95914789e-04 ... 1.36054686e-01\n",
      "  1.03032673e-04 1.17615150e-02]\n",
      " [9.02989405e-02 1.41411681e-06 6.25200239e-01 ... 7.81076523e-04\n",
      "  1.96939451e-03 3.19882602e-03]\n",
      " [9.31628208e-04 4.55627710e-05 5.17193874e-02 ... 9.78320180e-03\n",
      "  2.92189178e-01 1.16997414e-02]\n",
      " ...\n",
      " [6.53358111e-01 2.40526892e-01 9.87429657e-03 ... 3.83643658e-03\n",
      "  5.86101976e-05 2.61500459e-02]\n",
      " [1.59819736e-02 4.45218600e-04 1.71375218e-01 ... 1.43700195e-01\n",
      "  2.72179511e-02 3.48504409e-02]\n",
      " [2.19537259e-01 2.83685860e-01 1.82405795e-02 ... 1.55539481e-02\n",
      "  2.68873385e-03 4.03908361e-01]] & cached\n",
      "Re-used Cached Value, runNum =  353\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  353\n",
      "Provided input from cache for runNum = 353\n",
      "iterations = 350\n",
      "Accuracy = 0.6677317073170732\n",
      "Provided input from cache for runNum = 354\n",
      "activation = [[ 5.84345947  5.77492943  6.37181816 ... 11.37389852  4.99581674\n",
      "   3.54811453]\n",
      " [ 0.          3.37210445  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10062801  3.73596359  0.91637998 ...  2.36259121  5.22204226\n",
      "   1.92762516]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.12764667  2.89263835  3.02786307 ...  1.12337073  7.27180517\n",
      "   0.2957637 ]\n",
      " [ 4.64988634  8.49003604  0.         ...  3.7910816   2.49702852\n",
      "   2.80213722]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35342001 0.         1.433525   ... 0.         0.86510524 0.        ]\n",
      " [1.99627403 7.29569532 1.94814628 ... 8.15955648 3.09905631 6.29156697]\n",
      " ...\n",
      " [7.22181554 7.38246401 5.60300505 ... 4.23583171 8.17649101 4.67141241]\n",
      " [4.51253343 0.         7.43130842 ... 8.47577087 4.16002556 3.01487216]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.09642429e-07 2.39873041e-07 1.91045239e-04 ... 1.35366753e-01\n",
      "  1.01913606e-04 1.16551445e-02]\n",
      " [8.97018652e-02 1.39390225e-06 6.26622672e-01 ... 7.69355382e-04\n",
      "  1.94568667e-03 3.17251466e-03]\n",
      " [9.29937039e-04 4.53503854e-05 5.17270579e-02 ... 9.77086947e-03\n",
      "  2.91417799e-01 1.16491727e-02]\n",
      " ...\n",
      " [6.54544951e-01 2.40451970e-01 9.83748362e-03 ... 3.83647337e-03\n",
      "  5.76254341e-05 2.59769433e-02]\n",
      " [1.59556565e-02 4.45057061e-04 1.70653466e-01 ... 1.43664305e-01\n",
      "  2.70568489e-02 3.48120756e-02]\n",
      " [2.19092510e-01 2.84315635e-01 1.81548010e-02 ... 1.55956662e-02\n",
      "  2.66812643e-03 4.04411244e-01]] & cached\n",
      "Re-used Cached Value, runNum =  354\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  354\n",
      "Provided input from cache for runNum = 354\n",
      "Provided input from cache for runNum = 355\n",
      "activation = [[ 5.84782277  5.7764736   6.37312647 ... 11.38293079  4.99757187\n",
      "   3.54987679]\n",
      " [ 0.          3.37304626  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09918703  3.73339681  0.91412546 ...  2.36471144  5.2251877\n",
      "   1.92864405]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.1315093   2.90214181  3.03425198 ...  1.12778232  7.2807599\n",
      "   0.30090774]\n",
      " [ 4.65551645  8.49944279  0.         ...  3.79812928  2.5003257\n",
      "   2.80785648]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35276262 0.         1.43789842 ... 0.         0.86691838 0.        ]\n",
      " [1.99931049 7.30169287 1.94640523 ... 8.17714494 3.10536001 6.30049336]\n",
      " ...\n",
      " [7.23014431 7.40052749 5.613694   ... 4.25097682 8.18942648 4.6857012 ]\n",
      " [4.51670774 0.         7.43759586 ... 8.48732802 4.16331742 3.016012  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.07108966e-07 2.33852345e-07 1.86297173e-04 ... 1.34680821e-01\n",
      "  1.00817739e-04 1.15498966e-02]\n",
      " [8.90967668e-02 1.37414030e-06 6.28038099e-01 ... 7.57890890e-04\n",
      "  1.92249008e-03 3.14663078e-03]\n",
      " [9.28014029e-04 4.51336437e-05 5.17303493e-02 ... 9.75809775e-03\n",
      "  2.90660302e-01 1.15982345e-02]\n",
      " ...\n",
      " [6.55775413e-01 2.40448759e-01 9.80226855e-03 ... 3.83793294e-03\n",
      "  5.66633464e-05 2.58096785e-02]\n",
      " [1.59285279e-02 4.44960852e-04 1.69946525e-01 ... 1.43654691e-01\n",
      "  2.68977360e-02 3.47770945e-02]\n",
      " [2.18617831e-01 2.84955614e-01 1.80693765e-02 ... 1.56392242e-02\n",
      "  2.64779600e-03 4.04926858e-01]] & cached\n",
      "Re-used Cached Value, runNum =  355\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  355\n",
      "Provided input from cache for runNum = 355\n",
      "Provided input from cache for runNum = 356\n",
      "activation = [[ 5.85214101  5.77795331  6.37440519 ... 11.39186359  4.99925573\n",
      "   3.55160694]\n",
      " [ 0.          3.37402999  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09774562  3.73086071  0.91187709 ...  2.36686709  5.22833746\n",
      "   1.92966942]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.13539409  2.91166747  3.04064371 ...  1.13226085  7.28973974\n",
      "   0.30606402]\n",
      " [ 4.66109521  8.5087877   0.         ...  3.80514007  2.50357431\n",
      "   2.81353935]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35236192 0.         1.44248106 ... 0.         0.8689818  0.        ]\n",
      " [2.00225458 7.30758171 1.94460299 ... 8.19453544 3.11153847 6.30932247]\n",
      " ...\n",
      " [7.23842684 7.4185106  5.62432202 ... 4.26606323 8.20230316 4.69993669]\n",
      " [4.52084861 0.         7.44381965 ... 8.49876834 4.16654944 3.01712499]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.04666158e-07 2.28068851e-07 1.81672823e-04 ... 1.34021669e-01\n",
      "  9.97180221e-05 1.14472564e-02]\n",
      " [8.85234103e-02 1.35479085e-06 6.29471448e-01 ... 7.46664489e-04\n",
      "  1.89981963e-03 3.12125320e-03]\n",
      " [9.26557375e-04 4.49362474e-05 5.17430949e-02 ... 9.74856591e-03\n",
      "  2.89903011e-01 1.15495407e-02]\n",
      " ...\n",
      " [6.56945725e-01 2.40378898e-01 9.76465274e-03 ... 3.83851460e-03\n",
      "  5.57043709e-05 2.56402808e-02]\n",
      " [1.59029216e-02 4.44879948e-04 1.69215591e-01 ... 1.43638605e-01\n",
      "  2.67341255e-02 3.47418465e-02]\n",
      " [2.18164455e-01 2.85563178e-01 1.79821207e-02 ... 1.56809467e-02\n",
      "  2.62729519e-03 4.05415011e-01]] & cached\n",
      "Re-used Cached Value, runNum =  356\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  356\n",
      "Provided input from cache for runNum = 356\n",
      "Provided input from cache for runNum = 357\n",
      "activation = [[ 5.85647257  5.77946898  6.37569759 ... 11.40078713  5.00094186\n",
      "   3.55335148]\n",
      " [ 0.          3.37501372  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09629342  3.72829293  0.90963799 ...  2.36899723  5.23144313\n",
      "   1.93068165]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.13925406  2.92119022  3.04698111 ...  1.13676217  7.29874728\n",
      "   0.31120738]\n",
      " [ 4.66668247  8.51815213  0.         ...  3.81220464  2.50683999\n",
      "   2.81923975]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35187831 0.         1.44696616 ... 0.         0.87102373 0.        ]\n",
      " [2.00532284 7.31351923 1.9429274  ... 8.21198916 3.11779251 6.31818997]\n",
      " ...\n",
      " [7.24664837 7.43644186 5.63487786 ... 4.28114098 8.21517365 4.7141399 ]\n",
      " [4.52492738 0.         7.44998545 ... 8.51005558 4.16970678 3.01819094]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.02244259e-07 2.22337172e-07 1.77176079e-04 ... 1.33338345e-01\n",
      "  9.86342982e-05 1.13439557e-02]\n",
      " [8.79163145e-02 1.33555013e-06 6.30874811e-01 ... 7.35598539e-04\n",
      "  1.87727055e-03 3.09589286e-03]\n",
      " [9.24526875e-04 4.47174753e-05 5.17454441e-02 ... 9.73692012e-03\n",
      "  2.89134024e-01 1.14985954e-02]\n",
      " ...\n",
      " [6.58173896e-01 2.40384598e-01 9.73092630e-03 ... 3.84101983e-03\n",
      "  5.47741244e-05 2.54765049e-02]\n",
      " [1.58731643e-02 4.44751300e-04 1.68502053e-01 ... 1.43630982e-01\n",
      "  2.65722608e-02 3.47057663e-02]\n",
      " [2.17696051e-01 2.86195711e-01 1.78998279e-02 ... 1.57279312e-02\n",
      "  2.60740983e-03 4.05934712e-01]] & cached\n",
      "Re-used Cached Value, runNum =  357\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  357\n",
      "Provided input from cache for runNum = 357\n",
      "Provided input from cache for runNum = 358\n",
      "activation = [[ 5.86081129  5.78098417  6.37700291 ... 11.40968838  5.00262185\n",
      "   3.55509408]\n",
      " [ 0.          3.37600736  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09480902  3.72570609  0.90737959 ...  2.37109021  5.23449885\n",
      "   1.93167241]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.14310365  2.93066935  3.05328949 ...  1.14125757  7.30772176\n",
      "   0.31632426]\n",
      " [ 4.67226672  8.52753316  0.         ...  3.81932279  2.51011074\n",
      "   2.82493908]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35137508 0.         1.45141546 ... 0.         0.87309002 0.        ]\n",
      " [2.00848891 7.31955569 1.94132461 ... 8.22942748 3.12406878 6.32704155]\n",
      " ...\n",
      " [7.25481974 7.45428266 5.64536683 ... 4.29616493 8.22797354 4.72828433]\n",
      " [4.52896688 0.         7.4561141  ... 8.52122489 4.17280402 3.019224  ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.99223729e-08 2.16848777e-07 1.72830789e-04 ... 1.32663124e-01\n",
      "  9.75756022e-05 1.12424839e-02]\n",
      " [8.73224266e-02 1.31674822e-06 6.32260329e-01 ... 7.24700116e-04\n",
      "  1.85504358e-03 3.07090514e-03]\n",
      " [9.22662204e-04 4.45083445e-05 5.17472857e-02 ... 9.72467206e-03\n",
      "  2.88348950e-01 1.14477467e-02]\n",
      " ...\n",
      " [6.59355360e-01 2.40356867e-01 9.69574296e-03 ... 3.84293537e-03\n",
      "  5.38504818e-05 2.53119449e-02]\n",
      " [1.58461542e-02 4.44684841e-04 1.67794149e-01 ... 1.43613991e-01\n",
      "  2.64096115e-02 3.46693711e-02]\n",
      " [2.17255575e-01 2.86850865e-01 1.78172375e-02 ... 1.57742778e-02\n",
      "  2.58758476e-03 4.06444384e-01]] & cached\n",
      "Re-used Cached Value, runNum =  358\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  358\n",
      "Provided input from cache for runNum = 358\n",
      "Provided input from cache for runNum = 359\n",
      "activation = [[ 5.8651505   5.78250866  6.37831014 ... 11.41857372  5.00428598\n",
      "   3.55683795]\n",
      " [ 0.          3.37700942  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09334632  3.72314648  0.90514623 ...  2.37320642  5.23756937\n",
      "   1.93268362]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.14697597  2.94018173  3.05959986 ...  1.14581403  7.31675717\n",
      "   0.32145248]\n",
      " [ 4.67787805  8.53696482  0.         ...  3.82651262  2.51343173\n",
      "   2.83067793]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35086116 0.         1.45585161 ... 0.         0.87515211 0.        ]\n",
      " [2.01159105 7.32548455 1.93967082 ... 8.24674037 3.13026226 6.33584531]\n",
      " ...\n",
      " [7.26294376 7.47205673 5.65580824 ... 4.31117192 8.2407678  4.7423941 ]\n",
      " [4.53306781 0.         7.46227778 ... 8.53240542 4.17593951 3.02029133]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.76669291e-08 2.11488084e-07 1.68603763e-04 ... 1.32004697e-01\n",
      "  9.65262980e-05 1.11431024e-02]\n",
      " [8.67345260e-02 1.29812033e-06 6.33648737e-01 ... 7.14041768e-04\n",
      "  1.83316662e-03 3.04631600e-03]\n",
      " [9.20973282e-04 4.43042793e-05 5.17549693e-02 ... 9.71562403e-03\n",
      "  2.87584751e-01 1.13989567e-02]\n",
      " ...\n",
      " [6.60550141e-01 2.40314102e-01 9.66047157e-03 ... 3.84528934e-03\n",
      "  5.29429254e-05 2.51490263e-02]\n",
      " [1.58195808e-02 4.44575886e-04 1.67083918e-01 ... 1.43608783e-01\n",
      "  2.62473469e-02 3.46341348e-02]\n",
      " [2.16791780e-01 2.87448529e-01 1.77329323e-02 ... 1.58202344e-02\n",
      "  2.56770281e-03 4.06930846e-01]] & cached\n",
      "Re-used Cached Value, runNum =  359\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  359\n",
      "Provided input from cache for runNum = 359\n",
      "Provided input from cache for runNum = 360\n",
      "activation = [[ 5.86947812  5.78401219  6.37961099 ... 11.42741003  5.0059133\n",
      "   3.55856703]\n",
      " [ 0.          3.37801961  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09190802  3.72062806  0.902938   ...  2.37537228  5.24066432\n",
      "   1.93371633]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.15087365  2.94968092  3.06590169 ...  1.15037304  7.32578976\n",
      "   0.32656874]\n",
      " [ 4.68347159  8.54637483  0.         ...  3.83371182  2.51675487\n",
      "   2.83640653]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.35039586 0.         1.46031065 ... 0.         0.87723371 0.        ]\n",
      " [2.01473062 7.33144939 1.93805646 ... 8.26406087 3.13648615 6.34464988]\n",
      " ...\n",
      " [7.27099422 7.48970788 5.66615613 ... 4.32605626 8.25347028 4.75642935]\n",
      " [4.53711007 0.         7.46837037 ... 8.54343891 4.17897603 3.02130732]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.54966588e-08 2.06343694e-07 1.64506898e-04 ... 1.31361940e-01\n",
      "  9.55039424e-05 1.10456997e-02]\n",
      " [8.61681384e-02 1.28005694e-06 6.35049139e-01 ... 7.03541325e-04\n",
      "  1.81162628e-03 3.02204908e-03]\n",
      " [9.19386229e-04 4.41057133e-05 5.17581315e-02 ... 9.70469376e-03\n",
      "  2.86789801e-01 1.13495050e-02]\n",
      " ...\n",
      " [6.61685932e-01 2.40238727e-01 9.62351321e-03 ... 3.84655566e-03\n",
      "  5.20462503e-05 2.49835088e-02]\n",
      " [1.57933742e-02 4.44464055e-04 1.66354385e-01 ... 1.43564838e-01\n",
      "  2.60847463e-02 3.45949380e-02]\n",
      " [2.16362813e-01 2.88081326e-01 1.76487272e-02 ... 1.58653518e-02\n",
      "  2.54810464e-03 4.07408207e-01]] & cached\n",
      "Re-used Cached Value, runNum =  360\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  360\n",
      "Provided input from cache for runNum = 360\n",
      "Provided input from cache for runNum = 361\n",
      "activation = [[ 5.87381501  5.78554023  6.38092449 ... 11.43625519  5.00754504\n",
      "   3.56029981]\n",
      " [ 0.          3.37905236  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.09043767  3.71806156  0.90069634 ...  2.37746286  5.24369998\n",
      "   1.9347229 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.15469841  2.95909621  3.07213317 ...  1.15484625  7.33474475\n",
      "   0.33163748]\n",
      " [ 4.68904906  8.55576137  0.         ...  3.84093193  2.52006281\n",
      "   2.84212921]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34992341 0.         1.46475847 ... 0.         0.87933405 0.        ]\n",
      " [2.01784657 7.33734306 1.9364299  ... 8.28127817 3.14265549 6.35340143]\n",
      " ...\n",
      " [7.27886023 7.50711106 5.67636725 ... 4.34073733 8.26599405 4.77034382]\n",
      " [4.54111663 0.         7.47442641 ... 8.55437306 4.18196905 3.02228745]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[9.33562060e-08 2.01328076e-07 1.60507527e-04 ... 1.30714185e-01\n",
      "  9.45118809e-05 1.09492284e-02]\n",
      " [8.55777088e-02 1.26215020e-06 6.36423402e-01 ... 6.93206054e-04\n",
      "  1.79054483e-03 2.99801443e-03]\n",
      " [9.17420312e-04 4.39005117e-05 5.17531498e-02 ... 9.69242468e-03\n",
      "  2.86012546e-01 1.12994712e-02]\n",
      " ...\n",
      " [6.62898598e-01 2.40262688e-01 9.58979253e-03 ... 3.85020146e-03\n",
      "  5.11797264e-05 2.48259485e-02]\n",
      " [1.57668215e-02 4.44470510e-04 1.65659428e-01 ... 1.43572495e-01\n",
      "  2.59267166e-02 3.45622922e-02]\n",
      " [2.15887643e-01 2.88710544e-01 1.75657982e-02 ... 1.59131067e-02\n",
      "  2.52897770e-03 4.07901307e-01]] & cached\n",
      "Re-used Cached Value, runNum =  361\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  361\n",
      "Provided input from cache for runNum = 361\n",
      "Provided input from cache for runNum = 362\n",
      "activation = [[ 5.87814295  5.78707003  6.38223459 ... 11.44506915  5.00915682\n",
      "   3.56202789]\n",
      " [ 0.          3.38008041  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08898974  3.71551578  0.89848232 ...  2.37956893  5.24673872\n",
      "   1.93574235]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.15863745  2.96870885  3.07842263 ...  1.15953361  7.3438247\n",
      "   0.33680262]\n",
      " [ 4.69461062  8.56512445  0.         ...  3.84816269  2.52335938\n",
      "   2.84784052]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34946143e+00 0.00000000e+00 1.46919173e+00 ... 0.00000000e+00\n",
      "  8.81456981e-01 0.00000000e+00]\n",
      " [2.02089872e+00 7.34311324e+00 1.93476376e+00 ... 8.29833601e+00\n",
      "  3.14872275e+00 6.36207212e+00]\n",
      " ...\n",
      " [7.28685648e+00 7.52474565e+00 5.68665283e+00 ... 4.35567338e+00\n",
      "  8.27868536e+00 4.78435985e+00]\n",
      " [4.54513540e+00 1.54160266e-04 7.48047085e+00 ... 8.56528922e+00\n",
      "  4.18496954e+00 3.02328230e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[9.12922152e-08 1.96460831e-07 1.56610967e-04 ... 1.30061094e-01\n",
      "  9.34997693e-05 1.08535594e-02]\n",
      " [8.50167224e-02 1.24482724e-06 6.37808587e-01 ... 6.83168262e-04\n",
      "  1.76961827e-03 2.97452375e-03]\n",
      " [9.15965812e-04 4.37210908e-05 5.17585658e-02 ... 9.68474773e-03\n",
      "  2.85227947e-01 1.12520700e-02]\n",
      " ...\n",
      " [6.64035839e-01 2.40187410e-01 9.55368518e-03 ... 3.85266941e-03\n",
      "  5.03083886e-05 2.46639032e-02]\n",
      " [1.57424693e-02 4.44512754e-04 1.64943657e-01 ... 1.43571938e-01\n",
      "  2.57633555e-02 3.45277939e-02]\n",
      " [2.15447749e-01 2.89314897e-01 1.74818320e-02 ... 1.59603286e-02\n",
      "  2.50940746e-03 4.08368126e-01]] & cached\n",
      "Re-used Cached Value, runNum =  362\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  362\n",
      "Provided input from cache for runNum = 362\n",
      "Provided input from cache for runNum = 363\n",
      "activation = [[ 5.88248975  5.78862407  6.38355861 ... 11.45387535  5.01076967\n",
      "   3.56376769]\n",
      " [ 0.          3.38112678  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08754757  3.71299499  0.89628592 ...  2.38168938  5.24978446\n",
      "   1.93677704]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.16260262  2.9783547   3.08471691 ...  1.16430804  7.35297228\n",
      "   0.34199174]\n",
      " [ 4.70019464  8.57452044  0.         ...  3.85545327  2.52670989\n",
      "   2.85357126]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34885659e+00 0.00000000e+00 1.47350812e+00 ... 0.00000000e+00\n",
      "  8.83485855e-01 0.00000000e+00]\n",
      " [2.02409450e+00 7.34904025e+00 1.93321552e+00 ... 8.31546288e+00\n",
      "  3.15489576e+00 6.37078627e+00]\n",
      " ...\n",
      " [7.29481774e+00 7.54233946e+00 5.69689625e+00 ... 4.37063016e+00\n",
      "  8.29137940e+00 4.79836228e+00]\n",
      " [4.54922134e+00 1.67251048e-03 7.48654350e+00 ... 8.57620158e+00\n",
      "  4.18801215e+00 3.02431151e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[8.93058486e-08 1.91987079e-07 1.52848000e-04 ... 1.29410877e-01\n",
      "  9.25192356e-05 1.07593885e-02]\n",
      " [8.44594762e-02 1.22897581e-06 6.39180401e-01 ... 6.73280063e-04\n",
      "  1.74886933e-03 2.95132694e-03]\n",
      " [9.14435643e-04 4.35887890e-05 5.17578446e-02 ... 9.67535977e-03\n",
      "  2.84424139e-01 1.12039365e-02]\n",
      " ...\n",
      " [6.65154439e-01 2.40105035e-01 9.51719947e-03 ... 3.85436386e-03\n",
      "  4.94489436e-05 2.45006801e-02]\n",
      " [1.57184364e-02 4.44929257e-04 1.64228283e-01 ... 1.43544661e-01\n",
      "  2.56019572e-02 3.44905083e-02]\n",
      " [2.15021289e-01 2.89904645e-01 1.73985554e-02 ... 1.60067164e-02\n",
      "  2.49001920e-03 4.08822940e-01]] & cached\n",
      "Re-used Cached Value, runNum =  363\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  363\n",
      "Provided input from cache for runNum = 363\n",
      "Provided input from cache for runNum = 364\n",
      "activation = [[ 5.88683241  5.7901859   6.38487983 ... 11.46265138  5.01236383\n",
      "   3.56550594]\n",
      " [ 0.          3.38217522  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0861133   3.71048825  0.89410596 ...  2.38384506  5.25283336\n",
      "   1.93781948]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.16653166  2.98792078  3.0909732  ...  1.16904511  7.36206401\n",
      "   0.34714022]\n",
      " [ 4.70574939  8.58387782  0.         ...  3.86273223  2.53002444\n",
      "   2.85928615]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34814712e+00 0.00000000e+00 1.47772333e+00 ... 0.00000000e+00\n",
      "  8.85417549e-01 0.00000000e+00]\n",
      " [2.02728419e+00 7.35496201e+00 1.93166547e+00 ... 8.33253795e+00\n",
      "  3.16105197e+00 6.37947512e+00]\n",
      " ...\n",
      " [7.30263986e+00 7.55973749e+00 5.70701239e+00 ... 4.38542281e+00\n",
      "  8.30392299e+00 4.81225255e+00]\n",
      " [4.55331236e+00 3.21179709e-03 7.49259013e+00 ... 8.58706748e+00\n",
      "  4.19106403e+00 3.02534718e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[8.73864103e-08 1.87672987e-07 1.49216694e-04 ... 1.28787981e-01\n",
      "  9.15805987e-05 1.06678307e-02]\n",
      " [8.38989568e-02 1.21341638e-06 6.40526316e-01 ... 6.63529804e-04\n",
      "  1.72854529e-03 2.92847143e-03]\n",
      " [9.12862246e-04 4.34582798e-05 5.17574824e-02 ... 9.66590678e-03\n",
      "  2.83628569e-01 1.11562172e-02]\n",
      " ...\n",
      " [6.66297943e-01 2.40048776e-01 9.48147507e-03 ... 3.85608697e-03\n",
      "  4.86114806e-05 2.43397994e-02]\n",
      " [1.56957627e-02 4.45393080e-04 1.63533808e-01 ... 1.43521295e-01\n",
      "  2.54454343e-02 3.44552987e-02]\n",
      " [2.14572244e-01 2.90488193e-01 1.73149398e-02 ... 1.60516361e-02\n",
      "  2.47092815e-03 4.09262837e-01]] & cached\n",
      "Re-used Cached Value, runNum =  364\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  364\n",
      "Provided input from cache for runNum = 364\n",
      "Provided input from cache for runNum = 365\n",
      "activation = [[ 5.89117271  5.79175879  6.38619964 ... 11.47141897  5.01395124\n",
      "   3.56724894]\n",
      " [ 0.          3.38325228  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08462711  3.70790287  0.89189612 ...  2.3859323   5.25582193\n",
      "   1.93882043]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 3.1703976   2.99738683  3.09717062 ...  1.17369292  7.37107853\n",
      "   0.35223177]\n",
      " [ 4.71132772  8.5932771   0.         ...  3.87009072  2.53338196\n",
      "   2.86502004]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34740385e+00 0.00000000e+00 1.48189869e+00 ... 0.00000000e+00\n",
      "  8.87326791e-01 0.00000000e+00]\n",
      " [2.03046830e+00 7.36087039e+00 1.93010590e+00 ... 8.34957263e+00\n",
      "  3.16719805e+00 6.38814306e+00]\n",
      " ...\n",
      " [7.31031557e+00 7.57690587e+00 5.71698341e+00 ... 4.40000584e+00\n",
      "  8.31628598e+00 4.82602654e+00]\n",
      " [4.55730963e+00 4.61158564e-03 7.49856688e+00 ... 8.59776034e+00\n",
      "  4.19400210e+00 3.02632465e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[8.54830755e-08 1.83401304e-07 1.45679818e-04 ... 1.28166416e-01\n",
      "  9.06694683e-05 1.05765930e-02]\n",
      " [8.33082738e-02 1.19762608e-06 6.41849259e-01 ... 6.53897158e-04\n",
      "  1.70855673e-03 2.90565247e-03]\n",
      " [9.10888228e-04 4.33090410e-05 5.17534287e-02 ... 9.65589832e-03\n",
      "  2.82843081e-01 1.11075030e-02]\n",
      " ...\n",
      " [6.67513394e-01 2.40067371e-01 9.44962200e-03 ... 3.86070504e-03\n",
      "  4.78100067e-05 2.41867089e-02]\n",
      " [1.56696051e-02 4.45786337e-04 1.62860326e-01 ... 1.43527045e-01\n",
      "  2.52919591e-02 3.44215763e-02]\n",
      " [2.14088942e-01 2.91058941e-01 1.72342429e-02 ... 1.61018896e-02\n",
      "  2.45263394e-03 4.09732083e-01]] & cached\n",
      "Re-used Cached Value, runNum =  365\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  365\n",
      "Provided input from cache for runNum = 365\n",
      "Provided input from cache for runNum = 366\n",
      "activation = [[5.89550944e+00 5.79334717e+00 6.38751473e+00 ... 1.14801594e+01\n",
      "  5.01553015e+00 3.56898794e+00]\n",
      " [0.00000000e+00 3.38431191e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.31242918e-02 3.70529979e+00 8.89690315e-01 ... 2.38801702e+00\n",
      "  5.25878592e+00 1.93981792e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.05938013e-03]\n",
      " [3.17429387e+00 3.00688347e+00 3.10337033e+00 ... 1.17838302e+00\n",
      "  7.38012796e+00 3.57346866e-01]\n",
      " [4.71689967e+00 8.60267534e+00 0.00000000e+00 ... 3.87748414e+00\n",
      "  2.53675987e+00 2.87075550e+00]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34661021e+00 0.00000000e+00 1.48602368e+00 ... 0.00000000e+00\n",
      "  8.89173561e-01 0.00000000e+00]\n",
      " [2.03363625e+00 7.36673689e+00 1.92853670e+00 ... 8.36654009e+00\n",
      "  3.17331660e+00 6.39684546e+00]\n",
      " ...\n",
      " [7.31806046e+00 7.59415633e+00 5.72698075e+00 ... 4.41467103e+00\n",
      "  8.32872861e+00 4.83994342e+00]\n",
      " [4.56129341e+00 6.00658113e-03 7.50451553e+00 ... 8.60838001e+00\n",
      "  4.19691523e+00 3.02747873e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[8.36317816e-08 1.79225937e-07 1.42226889e-04 ... 1.27538018e-01\n",
      "  8.97571753e-05 1.04856865e-02]\n",
      " [8.27342254e-02 1.18221140e-06 6.43185886e-01 ... 6.44475431e-04\n",
      "  1.68871852e-03 2.88340847e-03]\n",
      " [9.09078624e-04 4.31667946e-05 5.17508470e-02 ... 9.64735090e-03\n",
      "  2.82042379e-01 1.10603677e-02]\n",
      " ...\n",
      " [6.68679955e-01 2.40052451e-01 9.41658367e-03 ... 3.86506138e-03\n",
      "  4.70124622e-05 2.40280441e-02]\n",
      " [1.56441036e-02 4.46196075e-04 1.62169395e-01 ... 1.43523007e-01\n",
      "  2.51354048e-02 3.43810742e-02]\n",
      " [2.13633855e-01 2.91651514e-01 1.71537633e-02 ... 1.61532989e-02\n",
      "  2.43427493e-03 4.10173046e-01]] & cached\n",
      "Re-used Cached Value, runNum =  366\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  366\n",
      "Provided input from cache for runNum = 366\n",
      "Provided input from cache for runNum = 367\n",
      "activation = [[5.89987386e+00 5.79499923e+00 6.38884044e+00 ... 1.14889291e+01\n",
      "  5.01714174e+00 3.57074817e+00]\n",
      " [0.00000000e+00 3.38533929e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.16012930e-02 3.70264240e+00 8.87479764e-01 ... 2.39004489e+00\n",
      "  5.26169142e+00 1.94079380e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.78313322e-03]\n",
      " [3.17817252e+00 3.01633754e+00 3.10954431e+00 ... 1.18305058e+00\n",
      "  7.38915184e+00 3.62437448e-01]\n",
      " [4.72243814e+00 8.61200024e+00 0.00000000e+00 ... 3.88484094e+00\n",
      "  2.54009079e+00 2.87645920e+00]] & cached\n",
      "activation = [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34577943e+00 0.00000000e+00 1.49009264e+00 ... 0.00000000e+00\n",
      "  8.91026650e-01 0.00000000e+00]\n",
      " [2.03687949e+00 7.37265698e+00 1.92701463e+00 ... 8.38353823e+00\n",
      "  3.17949050e+00 6.40561038e+00]\n",
      " ...\n",
      " [7.32584128e+00 7.61144243e+00 5.73696668e+00 ... 4.42939111e+00\n",
      "  8.34121594e+00 4.85393032e+00]\n",
      " [4.56546403e+00 7.71156015e-03 7.51054439e+00 ... 8.61918233e+00\n",
      "  4.20004004e+00 3.02887501e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]] & cached\n",
      "activation = [[8.18386530e-08 1.75191595e-07 1.38861995e-04 ... 1.26900019e-01\n",
      "  8.88623898e-05 1.03959421e-02]\n",
      " [8.21805227e-02 1.16747465e-06 6.44533330e-01 ... 6.35274849e-04\n",
      "  1.66913217e-03 2.86180742e-03]\n",
      " [9.07334025e-04 4.30325939e-05 5.17429875e-02 ... 9.63770253e-03\n",
      "  2.81250223e-01 1.10136646e-02]\n",
      " ...\n",
      " [6.69814373e-01 2.40044225e-01 9.38268685e-03 ... 3.86898714e-03\n",
      "  4.62179212e-05 2.38667113e-02]\n",
      " [1.56202115e-02 4.46714981e-04 1.61469701e-01 ... 1.43508843e-01\n",
      "  2.49790326e-02 3.43381612e-02]\n",
      " [2.13187877e-01 2.92273276e-01 1.70724234e-02 ... 1.62034199e-02\n",
      "  2.41565685e-03 4.10581306e-01]] & cached\n",
      "Re-used Cached Value, runNum =  367\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  367\n",
      "Provided input from cache for runNum = 367\n",
      "Provided input from cache for runNum = 368\n",
      "activation = [[5.90428381e+00 5.79673204e+00 6.39020200e+00 ... 1.14977472e+01\n",
      "  5.01879931e+00 3.57254469e+00]\n",
      " [0.00000000e+00 3.38634779e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.00559294e-02 3.69994703e+00 8.85265983e-01 ... 2.39204543e+00\n",
      "  5.26455697e+00 1.94175317e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 4.52047755e-03]\n",
      " [3.18200088e+00 3.02568441e+00 3.11566954e+00 ... 1.18763184e+00\n",
      "  7.39809434e+00 3.67471079e-01]\n",
      " [4.72794126e+00 8.62125074e+00 0.00000000e+00 ... 3.89215524e+00\n",
      "  2.54336835e+00 2.88212657e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34472371 0.         1.49393667 ... 0.         0.89265755 0.        ]\n",
      " [2.0402577  7.37866767 1.92561243 ... 8.40062337 3.18577289 6.41441299]\n",
      " ...\n",
      " [7.3335609  7.62861461 5.74688154 ... 4.44399142 8.35361694 4.8678462 ]\n",
      " [4.56977514 0.00964754 7.51664145 ... 8.63011157 4.20333601 3.03036817]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[8.01077495e-08 1.71294500e-07 1.35625402e-04 ... 1.26280352e-01\n",
      "  8.80168487e-05 1.03083871e-02]\n",
      " [8.16175650e-02 1.15311721e-06 6.45832724e-01 ... 6.26151337e-04\n",
      "  1.64973768e-03 2.84049131e-03]\n",
      " [9.05470374e-04 4.28987419e-05 5.17334787e-02 ... 9.62662743e-03\n",
      "  2.80463661e-01 1.09668249e-02]\n",
      " ...\n",
      " [6.70968483e-01 2.40077375e-01 9.35023922e-03 ... 3.87290031e-03\n",
      "  4.54381420e-05 2.37071539e-02]\n",
      " [1.55984242e-02 4.47341500e-04 1.60803034e-01 ... 1.43498149e-01\n",
      "  2.48272086e-02 3.42980342e-02]\n",
      " [2.12729765e-01 2.92912118e-01 1.69924152e-02 ... 1.62518390e-02\n",
      "  2.39714064e-03 4.10975386e-01]] & cached\n",
      "Re-used Cached Value, runNum =  368\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  368\n",
      "Provided input from cache for runNum = 368\n",
      "Provided input from cache for runNum = 369\n",
      "activation = [[5.90866516e+00 5.79843485e+00 6.39154611e+00 ... 1.15064941e+01\n",
      "  5.02040477e+00 3.57432112e+00]\n",
      " [0.00000000e+00 3.38735690e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.85382274e-02 3.69728049e+00 8.83081509e-01 ... 2.39410054e+00\n",
      "  5.26744226e+00 1.94273425e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 6.25236649e-03]\n",
      " [3.18590965e+00 3.03511842e+00 3.12181845e+00 ... 1.19232793e+00\n",
      "  7.40712690e+00 3.72541231e-01]\n",
      " [4.73344714e+00 8.63050454e+00 0.00000000e+00 ... 3.89951269e+00\n",
      "  2.54666361e+00 2.88779502e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34373169 0.         1.49779773 ... 0.         0.8943555  0.        ]\n",
      " [2.04353844 7.38455787 1.92414694 ... 8.41753809 3.19192401 6.42313735]\n",
      " ...\n",
      " [7.34139469 7.64593236 5.75685036 ... 4.45874023 8.36614809 4.88181081]\n",
      " [4.57405305 0.01153358 7.52270013 ... 8.6409212  4.20656208 3.03183278]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.84242581e-08 1.67465692e-07 1.32458311e-04 ... 1.25662855e-01\n",
      "  8.71472890e-05 1.02214760e-02]\n",
      " [8.10851427e-02 1.13907649e-06 6.47167229e-01 ... 6.17339203e-04\n",
      "  1.63052281e-03 2.81969311e-03]\n",
      " [9.03981143e-04 4.27742337e-05 5.17297297e-02 ... 9.61941239e-03\n",
      "  2.79660904e-01 1.09216707e-02]\n",
      " ...\n",
      " [6.72059377e-01 2.40025126e-01 9.31556316e-03 ... 3.87643036e-03\n",
      "  4.46579724e-05 2.35458210e-02]\n",
      " [1.55770549e-02 4.47889419e-04 1.60106804e-01 ... 1.43479198e-01\n",
      "  2.46699208e-02 3.42552311e-02]\n",
      " [2.12296604e-01 2.93517159e-01 1.69109972e-02 ... 1.63017140e-02\n",
      "  2.37828635e-03 4.11356386e-01]] & cached\n",
      "Re-used Cached Value, runNum =  369\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  369\n",
      "Provided input from cache for runNum = 369\n",
      "Provided input from cache for runNum = 370\n",
      "activation = [[5.91307075e+00 5.80018910e+00 6.39290345e+00 ... 1.15152559e+01\n",
      "  5.02202957e+00 3.57610479e+00]\n",
      " [0.00000000e+00 3.38832023e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.69844252e-02 3.69455125e+00 8.80886450e-01 ... 2.39610512e+00\n",
      "  5.27026886e+00 1.94369690e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 7.99306889e-03]\n",
      " [3.18981998e+00 3.04454780e+00 3.12796647e+00 ... 1.19707254e+00\n",
      "  7.41614190e+00 3.77607407e-01]\n",
      " [4.73892291e+00 8.63971240e+00 0.00000000e+00 ... 3.90685910e+00\n",
      "  2.54992804e+00 2.89344457e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34268892 0.         1.50159515 ... 0.         0.89601284 0.        ]\n",
      " [2.04680061 7.39037489 1.92267603 ... 8.4343409  3.19803282 6.43181876]\n",
      " ...\n",
      " [7.34913386 7.66312438 5.76673976 ... 4.47337743 8.37856127 4.89569325]\n",
      " [4.57834918 0.01346788 7.52874018 ... 8.6516819  4.20979903 3.03328467]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.67970427e-08 1.63788629e-07 1.29390418e-04 ... 1.25059657e-01\n",
      "  8.63079111e-05 1.01367176e-02]\n",
      " [8.05543651e-02 1.12548638e-06 6.48469993e-01 ... 6.08625442e-04\n",
      "  1.61170225e-03 2.79917364e-03]\n",
      " [9.02477047e-04 4.26583544e-05 5.17228304e-02 ... 9.61139687e-03\n",
      "  2.78847628e-01 1.08765843e-02]\n",
      " ...\n",
      " [6.73156447e-01 2.40020592e-01 9.28089132e-03 ... 3.87954853e-03\n",
      "  4.38885954e-05 2.33858560e-02]\n",
      " [1.55594360e-02 4.48653207e-04 1.59439760e-01 ... 1.43482857e-01\n",
      "  2.45150967e-02 3.42168802e-02]\n",
      " [2.11854046e-01 2.94155814e-01 1.68290976e-02 ... 1.63490001e-02\n",
      "  2.35950767e-03 4.11725076e-01]] & cached\n",
      "Re-used Cached Value, runNum =  370\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  370\n",
      "Provided input from cache for runNum = 370\n",
      "Provided input from cache for runNum = 371\n",
      "activation = [[5.91745134e+00 5.80192926e+00 6.39424503e+00 ... 1.15239703e+01\n",
      "  5.02361686e+00 3.57787115e+00]\n",
      " [0.00000000e+00 3.38929745e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.54266897e-02 3.69181324e+00 8.78704429e-01 ... 2.39811043e+00\n",
      "  5.27308112e+00 1.94465766e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.73642820e-03]\n",
      " [3.19366969e+00 3.05385582e+00 3.13405743e+00 ... 1.20172240e+00\n",
      "  7.42507339e+00 3.82608036e-01]\n",
      " [4.74434537e+00 8.64884488e+00 0.00000000e+00 ... 3.91414877e+00\n",
      "  2.55313755e+00 2.89905186e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34165741 0.         1.50536825 ... 0.         0.89768642 0.        ]\n",
      " [2.0500564  7.3961851  1.92121451 ... 8.45108312 3.20411385 6.44047257]\n",
      " ...\n",
      " [7.35678712 7.68016777 5.77654681 ... 4.48788179 8.39087313 4.90948405]\n",
      " [4.58262111 0.01536916 7.53472208 ... 8.66237469 4.21299775 3.03472316]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.52250796e-08 1.60236793e-07 1.26418439e-04 ... 1.24478354e-01\n",
      "  8.54882470e-05 1.00540568e-02]\n",
      " [8.00342481e-02 1.11211453e-06 6.49773765e-01 ... 6.00055280e-04\n",
      "  1.59322797e-03 2.77898572e-03]\n",
      " [9.01082701e-04 4.25454753e-05 5.17174299e-02 ... 9.60331675e-03\n",
      "  2.78024900e-01 1.08317160e-02]\n",
      " ...\n",
      " [6.74232986e-01 2.40000821e-01 9.24574182e-03 ... 3.88205345e-03\n",
      "  4.31311915e-05 2.32253027e-02]\n",
      " [1.55416345e-02 4.49405644e-04 1.58762150e-01 ... 1.43464234e-01\n",
      "  2.43593891e-02 3.41762371e-02]\n",
      " [2.11418905e-01 2.94790224e-01 1.67474063e-02 ... 1.63948055e-02\n",
      "  2.34088192e-03 4.12075007e-01]] & cached\n",
      "Re-used Cached Value, runNum =  371\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  371\n",
      "Provided input from cache for runNum = 371\n",
      "Provided input from cache for runNum = 372\n",
      "activation = [[5.92180192e+00 5.80361943e+00 6.39557183e+00 ... 1.15326038e+01\n",
      "  5.02514919e+00 3.57961286e+00]\n",
      " [0.00000000e+00 3.39033438e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.38850233e-02 3.68913211e+00 8.76536275e-01 ... 2.40016265e+00\n",
      "  5.27591470e+00 1.94563970e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.14925921e-02]\n",
      " [3.19753040e+00 3.06318874e+00 3.14013361e+00 ... 1.20642224e+00\n",
      "  7.43401856e+00 3.87623819e-01]\n",
      " [4.74975876e+00 8.65796312e+00 0.00000000e+00 ... 3.92145552e+00\n",
      "  2.55632942e+00 2.90465504e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.34060227 0.         1.50910775 ... 0.         0.89935777 0.        ]\n",
      " [2.05336262 7.40203867 1.91980158 ... 8.46777645 3.2101991  6.44910218]\n",
      " ...\n",
      " [7.36439905 7.69714164 5.7863078  ... 4.50234612 8.40313554 4.92323761]\n",
      " [4.58680871 0.01711314 7.5406291  ... 8.67288905 4.21609333 3.03610209]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.36971415e-08 1.56764957e-07 1.23534317e-04 ... 1.23912784e-01\n",
      "  8.46799513e-05 9.97259518e-03]\n",
      " [7.95122416e-02 1.09862164e-06 6.51071600e-01 ... 5.91634753e-04\n",
      "  1.57496890e-03 2.75897572e-03]\n",
      " [8.99722926e-04 4.24303437e-05 5.17142703e-02 ... 9.59681377e-03\n",
      "  2.77186362e-01 1.07873932e-02]\n",
      " ...\n",
      " [6.75299033e-01 2.39917790e-01 9.21116023e-03 ... 3.88473106e-03\n",
      "  4.23899261e-05 2.30645653e-02]\n",
      " [1.55220231e-02 4.49999982e-04 1.58078497e-01 ... 1.43430356e-01\n",
      "  2.42036916e-02 3.41322857e-02]\n",
      " [2.10993513e-01 2.95384038e-01 1.66674883e-02 ... 1.64424813e-02\n",
      "  2.32258261e-03 4.12416516e-01]] & cached\n",
      "Re-used Cached Value, runNum =  372\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  372\n",
      "Provided input from cache for runNum = 372\n",
      "Provided input from cache for runNum = 373\n",
      "activation = [[ 5.92614163  5.80529731  6.39688869 ... 11.54119711  5.02665323\n",
      "   3.58134636]\n",
      " [ 0.          3.39136173  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.07235781  3.68646632  0.87439638 ...  2.40223519  5.27874511\n",
      "   1.94663001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.01325132]\n",
      " [ 3.20135817  3.07245798  3.146168   ...  1.21107668  7.4429131\n",
      "   0.39259967]\n",
      " [ 4.75520977  8.66716064  0.         ...  3.9288355   2.55958522\n",
      "   2.91028127]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33959679 0.         1.51290315 ... 0.         0.90111318 0.        ]\n",
      " [2.05671912 7.4079552  1.9184195  ... 8.48448378 3.21632877 6.45774382]\n",
      " ...\n",
      " [7.37194852 7.7140257  5.79599944 ... 4.51675193 8.41534299 4.93693213]\n",
      " [4.59088612 0.01868042 7.54645359 ... 8.68320862 4.21904711 3.03740828]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.21871592e-08 1.53335105e-07 1.20717193e-04 ... 1.23342414e-01\n",
      "  8.38786944e-05 9.89125469e-03]\n",
      " [7.89732288e-02 1.08498118e-06 6.52353547e-01 ... 5.83308204e-04\n",
      "  1.55690754e-03 2.73899180e-03]\n",
      " [8.98118051e-04 4.23019095e-05 5.17084783e-02 ... 9.58975122e-03\n",
      "  2.76336862e-01 1.07419143e-02]\n",
      " ...\n",
      " [6.76396371e-01 2.39870286e-01 9.17842781e-03 ... 3.88898647e-03\n",
      "  4.16711561e-05 2.29081987e-02]\n",
      " [1.55004482e-02 4.50534521e-04 1.57407352e-01 ... 1.43413649e-01\n",
      "  2.40490136e-02 3.40885332e-02]\n",
      " [2.10558113e-01 2.95985356e-01 1.65896625e-02 ... 1.64943603e-02\n",
      "  2.30487001e-03 4.12780983e-01]] & cached\n",
      "Re-used Cached Value, runNum =  373\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  373\n",
      "Provided input from cache for runNum = 373\n",
      "Provided input from cache for runNum = 374\n",
      "activation = [[ 5.93047327  5.80696476  6.39819915 ... 11.54975477  5.0281349\n",
      "   3.58307096]\n",
      " [ 0.          3.3923907   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.07082584  3.68382111  0.87225946 ...  2.40431617  5.28155595\n",
      "   1.94762226]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.01500329]\n",
      " [ 3.20516962  3.08165594  3.15216783 ...  1.21569458  7.45177007\n",
      "   0.39753972]\n",
      " [ 4.76059308  8.67628035  0.         ...  3.93615584  2.56275673\n",
      "   2.91586545]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33859943 0.         1.51667687 ... 0.         0.90287497 0.        ]\n",
      " [2.06002663 7.4138114  1.91702243 ... 8.50107545 3.22239588 6.46632552]\n",
      " ...\n",
      " [7.37940955 7.73073763 5.80560661 ... 4.53101208 8.42745044 4.95051927]\n",
      " [4.59497073 0.02022357 7.55223642 ... 8.69346658 4.22200863 3.03870032]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[7.07274911e-08 1.50032623e-07 1.17959648e-04 ... 1.22770711e-01\n",
      "  8.31004485e-05 9.81137166e-03]\n",
      " [7.84722469e-02 1.07209565e-06 6.53691297e-01 ... 5.75287981e-04\n",
      "  1.53930860e-03 2.71972735e-03]\n",
      " [8.96554508e-04 4.21754830e-05 5.16887629e-02 ... 9.57993275e-03\n",
      "  2.75479139e-01 1.06956714e-02]\n",
      " ...\n",
      " [6.77432578e-01 2.39809985e-01 9.14305718e-03 ... 3.89182901e-03\n",
      "  4.09548096e-05 2.27499639e-02]\n",
      " [1.54792118e-02 4.51070066e-04 1.56698375e-01 ... 1.43352166e-01\n",
      "  2.38951825e-02 3.40416893e-02]\n",
      " [2.10145118e-01 2.96638400e-01 1.65098178e-02 ... 1.65441824e-02\n",
      "  2.28704168e-03 4.13129162e-01]] & cached\n",
      "Re-used Cached Value, runNum =  374\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  374\n",
      "Provided input from cache for runNum = 374\n",
      "Provided input from cache for runNum = 375\n",
      "activation = [[ 5.93482131  5.80865912  6.39952043 ... 11.55831105  5.02962375\n",
      "   3.58480207]\n",
      " [ 0.          3.39341201  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.06927692  3.68114887  0.87013644 ...  2.40635829  5.28432669\n",
      "   1.94859523]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.01674351]\n",
      " [ 3.20892273  3.09074946  3.15811887 ...  1.22022037  7.46053722\n",
      "   0.40242901]\n",
      " [ 4.76593348  8.68533013  0.         ...  3.94341856  2.56587923\n",
      "   2.92141289]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33769893 0.         1.52049196 ... 0.         0.90474436 0.        ]\n",
      " [2.06329195 7.41960381 1.91560343 ... 8.51756605 3.22841637 6.47485554]\n",
      " ...\n",
      " [7.38679167 7.7473292  5.81513785 ... 4.54517807 8.4394769  4.96403864]\n",
      " [4.59909243 0.0218134  7.558011   ... 8.70374027 4.22500736 3.04001616]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.92976339e-08 1.46827040e-07 1.15274111e-04 ... 1.22204487e-01\n",
      "  8.23353831e-05 9.73292068e-03]\n",
      " [7.79681825e-02 1.05946679e-06 6.54987863e-01 ... 5.67389213e-04\n",
      "  1.52203274e-03 2.70080085e-03]\n",
      " [8.94998496e-04 4.20540529e-05 5.16716585e-02 ... 9.57027588e-03\n",
      "  2.74638655e-01 1.06501933e-02]\n",
      " ...\n",
      " [6.78505100e-01 2.39816480e-01 9.10845408e-03 ... 3.89520935e-03\n",
      "  4.02485369e-05 2.25962028e-02]\n",
      " [1.54619375e-02 4.51810129e-04 1.56032360e-01 ... 1.43341842e-01\n",
      "  2.37438282e-02 3.40035549e-02]\n",
      " [2.09698203e-01 2.97289734e-01 1.64289799e-02 ... 1.65924428e-02\n",
      "  2.26913791e-03 4.13469351e-01]] & cached\n",
      "Re-used Cached Value, runNum =  375\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  375\n",
      "Provided input from cache for runNum = 375\n",
      "Provided input from cache for runNum = 376\n",
      "activation = [[ 5.93912572  5.81029434  6.4008188  ... 11.56677156  5.03103869\n",
      "   3.58650079]\n",
      " [ 0.          3.39447028  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.06776721  3.67852853  0.86805068 ...  2.40846167  5.28713093\n",
      "   1.94959775]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.01850033]\n",
      " [ 3.21264012  3.09975986  3.16403306 ...  1.22467751  7.46925437\n",
      "   0.40728118]\n",
      " [ 4.77128839  8.69440306  0.         ...  3.95073807  2.56901819\n",
      "   2.92697414]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33678805 0.         1.52429559 ... 0.         0.90661323 0.        ]\n",
      " [2.06654224 7.42536967 1.91417336 ... 8.53406214 3.23443147 6.4833823 ]\n",
      " ...\n",
      " [7.39416869 7.76389068 5.82464768 ... 4.55933088 8.45148792 4.97753573]\n",
      " [4.60314302 0.02328152 7.5637332  ... 8.71384366 4.22789287 3.04127589]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.78920179e-08 1.43630931e-07 1.12646703e-04 ... 1.21642332e-01\n",
      "  8.15731628e-05 9.65482642e-03]\n",
      " [7.74711910e-02 1.04687739e-06 6.56319620e-01 ... 5.59678694e-04\n",
      "  1.50485637e-03 2.68209102e-03]\n",
      " [8.93422169e-04 4.19189868e-05 5.16544947e-02 ... 9.56137613e-03\n",
      "  2.73791423e-01 1.06047049e-02]\n",
      " ...\n",
      " [6.79558535e-01 2.39771525e-01 9.07369909e-03 ... 3.89880618e-03\n",
      "  3.95569349e-05 2.24418912e-02]\n",
      " [1.54398998e-02 4.52261766e-04 1.55330142e-01 ... 1.43289015e-01\n",
      "  2.35906147e-02 3.39577050e-02]\n",
      " [2.09263447e-01 2.97905370e-01 1.63490824e-02 ... 1.66440071e-02\n",
      "  2.25148910e-03 4.13807898e-01]] & cached\n",
      "Re-used Cached Value, runNum =  376\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  376\n",
      "Provided input from cache for runNum = 376\n",
      "Provided input from cache for runNum = 377\n",
      "activation = [[ 5.94342204  5.81191209  6.40211268 ... 11.57519144  5.03242219\n",
      "   3.58819089]\n",
      " [ 0.          3.39554813  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.06628683  3.67595813  0.86599419 ...  2.41060959  5.28996527\n",
      "   1.9506179 ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.02026729]\n",
      " [ 3.21637611  3.10881807  3.16993973 ...  1.22919138  7.47801019\n",
      "   0.41215117]\n",
      " [ 4.77663336  8.70344454  0.         ...  3.95804609  2.57213776\n",
      "   2.93252066]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33589583 0.         1.52809527 ... 0.         0.90853884 0.        ]\n",
      " [2.06985556 7.43116017 1.91279978 ... 8.5505135  3.24044761 6.49188784]\n",
      " ...\n",
      " [7.40152054 7.78041265 5.83411916 ... 4.5734717  8.46348057 4.99100673]\n",
      " [4.60714334 0.02467226 7.56940148 ... 8.72384891 4.23071669 3.04250806]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.65416771e-08 1.40556771e-07 1.10109375e-04 ... 1.21099237e-01\n",
      "  8.08183656e-05 9.57868289e-03]\n",
      " [7.69762897e-02 1.03440416e-06 6.57606162e-01 ... 5.52021260e-04\n",
      "  1.48782874e-03 2.66351420e-03]\n",
      " [8.92082941e-04 4.17957296e-05 5.16438168e-02 ... 9.55344152e-03\n",
      "  2.72929051e-01 1.05603585e-02]\n",
      " ...\n",
      " [6.80584042e-01 2.39686446e-01 9.03811058e-03 ... 3.90115981e-03\n",
      "  3.88715747e-05 2.22861765e-02]\n",
      " [1.54221596e-02 4.52805378e-04 1.54654117e-01 ... 1.43244893e-01\n",
      "  2.34381369e-02 3.39150322e-02]\n",
      " [2.08846964e-01 2.98520077e-01 1.62695587e-02 ... 1.66930427e-02\n",
      "  2.23385768e-03 4.14124659e-01]] & cached\n",
      "Re-used Cached Value, runNum =  377\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  377\n",
      "Provided input from cache for runNum = 377\n",
      "Provided input from cache for runNum = 378\n",
      "activation = [[5.94769634e+00 5.81351461e+00 6.40338695e+00 ... 1.15835645e+01\n",
      "  5.03376189e+00 3.58986586e+00]\n",
      " [0.00000000e+00 3.39663712e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.48053841e-02 3.67338821e+00 8.63945394e-01 ... 2.41277040e+00\n",
      "  5.29279819e+00 1.95164110e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.47108610e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.20422432e-02]\n",
      " [3.22008623e+00 3.11782769e+00 3.17581517e+00 ... 1.23367577e+00\n",
      "  7.48673539e+00 4.16993579e-01]\n",
      " [4.78195110e+00 8.71244340e+00 0.00000000e+00 ... 3.96533497e+00\n",
      "  2.57524715e+00 2.93803938e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33508091 0.         1.53195716 ... 0.         0.91053667 0.        ]\n",
      " [2.073109   7.43684567 1.91140319 ... 8.56684134 3.24641066 6.50033237]\n",
      " ...\n",
      " [7.40874366 7.79673142 5.84349968 ... 4.58742731 8.47534163 5.00436148]\n",
      " [4.61111333 0.02602221 7.57504059 ... 8.7337503  4.23347675 3.04371745]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.52159461e-08 1.37556630e-07 1.07631794e-04 ... 1.20573228e-01\n",
      "  8.00792841e-05 9.50413092e-03]\n",
      " [7.64792222e-02 1.02209411e-06 6.58895642e-01 ... 5.44481581e-04\n",
      "  1.47114711e-03 2.64519380e-03]\n",
      " [8.90657584e-04 4.16704716e-05 5.16321622e-02 ... 9.54591724e-03\n",
      "  2.72074440e-01 1.05163725e-02]\n",
      " ...\n",
      " [6.81635046e-01 2.39638316e-01 9.00302369e-03 ... 3.90403110e-03\n",
      "  3.82038863e-05 2.21331864e-02]\n",
      " [1.54036786e-02 4.53360244e-04 1.53976407e-01 ... 1.43209684e-01\n",
      "  2.32879359e-02 3.38742104e-02]\n",
      " [2.08409458e-01 2.99122182e-01 1.61899759e-02 ... 1.67419378e-02\n",
      "  2.21653652e-03 4.14432671e-01]] & cached\n",
      "Re-used Cached Value, runNum =  378\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  378\n",
      "Provided input from cache for runNum = 378\n",
      "Provided input from cache for runNum = 379\n",
      "activation = [[5.95199208e+00 5.81515802e+00 6.40467587e+00 ... 1.15919493e+01\n",
      "  5.03513110e+00 3.59155608e+00]\n",
      " [0.00000000e+00 3.39770414e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.32975867e-02 3.67077761e+00 8.61892230e-01 ... 2.41489754e+00\n",
      "  5.29556693e+00 1.95264334e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 2.11400563e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.38352693e-02]\n",
      " [3.22380145e+00 3.12684626e+00 3.18168611e+00 ... 1.23818821e+00\n",
      "  7.49543671e+00 4.21841723e-01]\n",
      " [4.78724813e+00 8.72141072e+00 0.00000000e+00 ... 3.97260600e+00\n",
      "  2.57832338e+00 2.94354823e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33433023 0.         1.5360573  ... 0.         0.91261109 0.        ]\n",
      " [2.07636119 7.44251111 1.91014489 ... 8.58314126 3.252389   6.50876546]\n",
      " ...\n",
      " [7.41601584 7.81314581 5.85309519 ... 4.60151377 8.48727897 5.01775576]\n",
      " [4.61508554 0.02739334 7.58097084 ... 8.74362224 4.23626045 3.04493161]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.39052074e-08 1.34601802e-07 1.05170504e-04 ... 1.20015008e-01\n",
      "  7.93388286e-05 9.42901353e-03]\n",
      " [7.59923301e-02 1.01023735e-06 6.60267050e-01 ... 5.37136584e-04\n",
      "  1.45461759e-03 2.62724272e-03]\n",
      " [8.89086735e-04 4.15425483e-05 5.16128463e-02 ... 9.53657484e-03\n",
      "  2.71207117e-01 1.04714058e-02]\n",
      " ...\n",
      " [6.82666560e-01 2.39612020e-01 8.96338174e-03 ... 3.90658951e-03\n",
      "  3.75379851e-05 2.19805700e-02]\n",
      " [1.53854155e-02 4.53983180e-04 1.53232475e-01 ... 1.43177970e-01\n",
      "  2.31369784e-02 3.38338268e-02]\n",
      " [2.07983729e-01 2.99783438e-01 1.61074110e-02 ... 1.67918428e-02\n",
      "  2.19910618e-03 4.14748810e-01]] & cached\n",
      "Re-used Cached Value, runNum =  379\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  379\n",
      "Provided input from cache for runNum = 379\n",
      "Provided input from cache for runNum = 380\n",
      "activation = [[5.95626097e+00 5.81676871e+00 6.40595226e+00 ... 1.16002766e+01\n",
      "  5.03645647e+00 3.59322836e+00]\n",
      " [0.00000000e+00 3.39880167e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.18331695e-02 3.66822838e+00 8.59872974e-01 ... 2.41708687e+00\n",
      "  5.29836763e+00 1.95367401e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 4.08667631e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.56287646e-02]\n",
      " [3.22748497e+00 3.13579614e+00 3.18752053e+00 ... 1.24264979e+00\n",
      "  7.50408005e+00 4.26653714e-01]\n",
      " [4.79255490e+00 8.73039871e+00 0.00000000e+00 ... 3.97993077e+00\n",
      "  2.58141993e+00 2.94907021e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33350392 0.         1.54008849 ... 0.         0.91460873 0.        ]\n",
      " [2.07971083 7.44827304 1.90895707 ... 8.59947318 3.25844547 6.51721714]\n",
      " ...\n",
      " [7.42324244 7.82946395 5.86263799 ... 4.61551132 8.49916372 5.03109302]\n",
      " [4.61901156 0.02866606 7.58687463 ... 8.75337092 4.23898293 3.04610874]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.26343859e-08 1.31707753e-07 1.02796513e-04 ... 1.19482065e-01\n",
      "  7.86172252e-05 9.35526925e-03]\n",
      " [7.54875785e-02 9.98075466e-07 6.61596782e-01 ... 5.29826892e-04\n",
      "  1.43810979e-03 2.60921448e-03]\n",
      " [8.87593878e-04 4.14110039e-05 5.16033094e-02 ... 9.52904456e-03\n",
      "  2.70347138e-01 1.04271939e-02]\n",
      " ...\n",
      " [6.83720038e-01 2.39552278e-01 8.92544518e-03 ... 3.90998365e-03\n",
      "  3.68928452e-05 2.18302344e-02]\n",
      " [1.53665513e-02 4.54469220e-04 1.52505667e-01 ... 1.43143105e-01\n",
      "  2.29881080e-02 3.37918086e-02]\n",
      " [2.07549956e-01 3.00380681e-01 1.60268864e-02 ... 1.68435391e-02\n",
      "  2.18208920e-03 4.15057621e-01]] & cached\n",
      "Re-used Cached Value, runNum =  380\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  380\n",
      "Provided input from cache for runNum = 380\n",
      "Provided input from cache for runNum = 381\n",
      "activation = [[5.96055634e+00 5.81841610e+00 6.40724800e+00 ... 1.16086167e+01\n",
      "  5.03779894e+00 3.59491703e+00]\n",
      " [0.00000000e+00 3.39986829e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.03588627e-02 3.66566102e+00 8.57867018e-01 ... 2.41925279e+00\n",
      "  5.30112949e+00 1.95469093e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 6.06248935e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.74257178e-02]\n",
      " [3.23113790e+00 3.14470649e+00 3.19332112e+00 ... 1.24708166e+00\n",
      "  7.51268089e+00 4.31439297e-01]\n",
      " [4.79786758e+00 8.73940259e+00 0.00000000e+00 ... 3.98731053e+00\n",
      "  2.58453120e+00 2.95459844e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33251831 0.         1.54397462 ... 0.         0.91650442 0.        ]\n",
      " [2.08315462 7.45410375 1.90785812 ... 8.6159044  3.26458062 6.52569815]\n",
      " ...\n",
      " [7.43046424 7.84579041 5.87216252 ... 4.62955717 8.51104993 5.04442104]\n",
      " [4.62294405 0.02995416 7.5927696  ... 8.76307055 4.2417026  3.04729028]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.13840043e-08 1.28859541e-07 1.00489023e-04 ... 1.18933913e-01\n",
      "  7.79125463e-05 9.28152368e-03]\n",
      " [7.49664382e-02 9.86072449e-07 6.62891303e-01 ... 5.22595502e-04\n",
      "  1.42167127e-03 2.59130423e-03]\n",
      " [8.85830085e-04 4.12730578e-05 5.15893708e-02 ... 9.52016069e-03\n",
      "  2.69483041e-01 1.03820715e-02]\n",
      " ...\n",
      " [6.84800889e-01 2.39537153e-01 8.88947937e-03 ... 3.91435134e-03\n",
      "  3.62615171e-05 2.16828142e-02]\n",
      " [1.53474745e-02 4.54993881e-04 1.51804155e-01 ... 1.43117867e-01\n",
      "  2.28412905e-02 3.37508979e-02]\n",
      " [2.07108427e-01 3.01017550e-01 1.59488706e-02 ... 1.68983743e-02\n",
      "  2.16534616e-03 4.15382219e-01]] & cached\n",
      "Re-used Cached Value, runNum =  381\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  381\n",
      "Provided input from cache for runNum = 381\n",
      "Provided input from cache for runNum = 382\n",
      "activation = [[5.96482631e+00 5.82003425e+00 6.40853152e+00 ... 1.16169007e+01\n",
      "  5.03910038e+00 3.59658648e+00]\n",
      " [0.00000000e+00 3.40091365e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.89108235e-02 3.66313563e+00 8.55889657e-01 ... 2.42147517e+00\n",
      "  5.30391763e+00 1.95572544e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.04091992e-03 ... 0.00000000e+00\n",
      "  0.00000000e+00 2.92328702e-02]\n",
      " [3.23481467e+00 3.15363657e+00 3.19911906e+00 ... 1.25154958e+00\n",
      "  7.52129848e+00 4.36232796e-01]\n",
      " [4.80317089e+00 8.74840862e+00 0.00000000e+00 ... 3.99475892e+00\n",
      "  2.58763332e+00 2.96013038e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33160062 0.         1.54790054 ... 0.         0.9184794  0.        ]\n",
      " [2.08669306 7.46002306 1.90684286 ... 8.63239044 3.27077986 6.53418554]\n",
      " ...\n",
      " [7.43773597 7.86219984 5.88169427 ... 4.64369526 8.52299825 5.05776701]\n",
      " [4.62688241 0.0312475  7.59865911 ... 8.77269997 4.24442311 3.04846931]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[6.01809792e-08 1.26088428e-07 9.82517250e-05 ... 1.18397173e-01\n",
      "  7.72073299e-05 9.20883176e-03]\n",
      " [7.44614519e-02 9.74203751e-07 6.64184027e-01 ... 5.15482909e-04\n",
      "  1.40531663e-03 2.57363314e-03]\n",
      " [8.84419708e-04 4.11427938e-05 5.15840628e-02 ... 9.51334499e-03\n",
      "  2.68614029e-01 1.03380812e-02]\n",
      " ...\n",
      " [6.85832121e-01 2.39443166e-01 8.85232637e-03 ... 3.91839501e-03\n",
      "  3.56350025e-05 2.15341237e-02]\n",
      " [1.53291285e-02 4.55429486e-04 1.51089105e-01 ... 1.43069981e-01\n",
      "  2.26920070e-02 3.37063235e-02]\n",
      " [2.06693114e-01 3.01628570e-01 1.58708010e-02 ... 1.69542775e-02\n",
      "  2.14853634e-03 4.15689528e-01]] & cached\n",
      "Re-used Cached Value, runNum =  382\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  382\n",
      "Provided input from cache for runNum = 382\n",
      "Provided input from cache for runNum = 383\n",
      "activation = [[5.96911905e+00 5.82169935e+00 6.40983061e+00 ... 1.16252089e+01\n",
      "  5.04042301e+00 3.59827261e+00]\n",
      " [0.00000000e+00 3.40198543e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.74468487e-02 3.66057139e+00 8.53917093e-01 ... 2.42365687e+00\n",
      "  5.30667161e+00 1.95674603e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 1.00164281e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 3.10426127e-02]\n",
      " [3.23851002e+00 3.16258720e+00 3.20491356e+00 ... 1.25607177e+00\n",
      "  7.52992515e+00 4.41037497e-01]\n",
      " [4.80845762e+00 8.75737771e+00 0.00000000e+00 ... 4.00218889e+00\n",
      "  2.59071683e+00 2.96564767e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.33062559 0.         1.55174186 ... 0.         0.92038845 0.        ]\n",
      " [2.09021217 7.46589678 1.90582937 ... 8.64881146 3.27696336 6.54264519]\n",
      " ...\n",
      " [7.44497978 7.87857169 5.89118298 ... 4.65784002 8.53492759 5.07108984]\n",
      " [4.63085104 0.03258916 7.60454718 ... 8.7823524  4.2471803  3.04967162]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.90098891e-08 1.23403716e-07 9.60734947e-05 ... 1.17860039e-01\n",
      "  7.65199756e-05 9.13746216e-03]\n",
      " [7.39665606e-02 9.62790811e-07 6.65471201e-01 ... 5.08524959e-04\n",
      "  1.38922905e-03 2.55639642e-03]\n",
      " [8.82973273e-04 4.10180734e-05 5.15723251e-02 ... 9.50559777e-03\n",
      "  2.67740625e-01 1.02942776e-02]\n",
      " ...\n",
      " [6.86862013e-01 2.39389579e-01 8.81456106e-03 ... 3.92171642e-03\n",
      "  3.50151419e-05 2.13859996e-02]\n",
      " [1.53135296e-02 4.56024506e-04 1.50387300e-01 ... 1.43035181e-01\n",
      "  2.25453945e-02 3.36656329e-02]\n",
      " [2.06268371e-01 3.02275900e-01 1.57914172e-02 ... 1.70076137e-02\n",
      "  2.13166842e-03 4.15981788e-01]] & cached\n",
      "Re-used Cached Value, runNum =  383\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  383\n",
      "Provided input from cache for runNum = 383\n",
      "Provided input from cache for runNum = 384\n",
      "activation = [[ 5.97342345  5.82340909  6.41114084 ... 11.63352757  5.04175319\n",
      "   3.59996966]\n",
      " [ 0.          3.40306594  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05594762  3.65792493  0.85192438 ...  2.42576908  5.3093587\n",
      "   1.95773482]\n",
      " ...\n",
      " [ 0.          0.          0.01198865 ...  0.          0.\n",
      "   0.03285165]\n",
      " [ 3.24216205  3.1714578   3.21067244 ...  1.26053824  7.5384741\n",
      "   0.44580497]\n",
      " [ 4.81375773  8.76635377  0.         ...  4.00966166  2.59381273\n",
      "   2.97117551]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32967352 0.         1.55559398 ... 0.         0.9223601  0.        ]\n",
      " [2.09370613 7.47171509 1.90479942 ... 8.66517745 3.2831431  6.55107668]\n",
      " ...\n",
      " [7.45216958 7.89486862 5.90062126 ... 4.67192602 8.54680672 5.08436224]\n",
      " [4.63482688 0.03395999 7.61044326 ... 8.7919771  4.24994585 3.05087774]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.78476304e-08 1.20749230e-07 9.39375440e-05 ... 1.17317181e-01\n",
      "  7.58423117e-05 9.06646616e-03]\n",
      " [7.34640107e-02 9.51533261e-07 6.66760402e-01 ... 5.01710775e-04\n",
      "  1.37337570e-03 2.53941060e-03]\n",
      " [8.81298579e-04 4.08857325e-05 5.15556905e-02 ... 9.49757650e-03\n",
      "  2.66881936e-01 1.02503050e-02]\n",
      " ...\n",
      " [6.87935585e-01 2.39413940e-01 8.77813963e-03 ... 3.92657867e-03\n",
      "  3.44098162e-05 2.12428429e-02]\n",
      " [1.52967169e-02 4.56670888e-04 1.49693414e-01 ... 1.43026544e-01\n",
      "  2.23996530e-02 3.36279652e-02]\n",
      " [2.05812577e-01 3.02926415e-01 1.57120430e-02 ... 1.70632010e-02\n",
      "  2.11497542e-03 4.16283940e-01]] & cached\n",
      "Re-used Cached Value, runNum =  384\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  384\n",
      "Provided input from cache for runNum = 384\n",
      "Provided input from cache for runNum = 385\n",
      "activation = [[ 5.97772886  5.8251375   6.41246156 ... 11.64184576  5.04307586\n",
      "   3.60167585]\n",
      " [ 0.          3.40414577  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05444533  3.65526814  0.84994056 ...  2.42788964  5.3120264\n",
      "   1.95871863]\n",
      " ...\n",
      " [ 0.          0.          0.01396051 ...  0.          0.\n",
      "   0.03465728]\n",
      " [ 3.24587556  3.18040864  3.21643421 ...  1.26512406  7.54713669\n",
      "   0.45060243]\n",
      " [ 4.81900178  8.77524473  0.         ...  4.01705391  2.59683798\n",
      "   2.97665844]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32879644 0.         1.55945584 ... 0.         0.924473   0.        ]\n",
      " [2.09714487 7.47742646 1.90376845 ... 8.68139861 3.28921114 6.55944292]\n",
      " ...\n",
      " [7.45939802 7.91121692 5.91004695 ... 4.68605585 8.55876126 5.0976312 ]\n",
      " [4.6387903  0.03532664 7.61630672 ... 8.80159652 4.25271985 3.05209191]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.67355098e-08 1.18204359e-07 9.18671334e-05 ... 1.16793742e-01\n",
      "  7.51555150e-05 8.99753236e-03]\n",
      " [7.29913626e-02 9.40737560e-07 6.68040889e-01 ... 4.94999386e-04\n",
      "  1.35779420e-03 2.52280593e-03]\n",
      " [8.80072930e-04 4.07743070e-05 5.15436997e-02 ... 9.49140409e-03\n",
      "  2.66001283e-01 1.02080340e-02]\n",
      " ...\n",
      " [6.88937346e-01 2.39392751e-01 8.73917419e-03 ... 3.92920626e-03\n",
      "  3.37939895e-05 2.10965717e-02]\n",
      " [1.52849101e-02 4.57492800e-04 1.48999598e-01 ... 1.43019449e-01\n",
      "  2.22489354e-02 3.35929598e-02]\n",
      " [2.05389552e-01 3.03599194e-01 1.56311114e-02 ... 1.71133197e-02\n",
      "  2.09768951e-03 4.16550160e-01]] & cached\n",
      "Re-used Cached Value, runNum =  385\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  385\n",
      "Provided input from cache for runNum = 385\n",
      "Provided input from cache for runNum = 386\n",
      "activation = [[ 5.98201958  5.82685815  6.41377636 ... 11.65013574  5.0443726\n",
      "   3.60337467]\n",
      " [ 0.          3.40523743  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05296965  3.652643    0.84798585 ...  2.43003639  5.31471955\n",
      "   1.95971215]\n",
      " ...\n",
      " [ 0.          0.          0.01593162 ...  0.          0.\n",
      "   0.03646504]\n",
      " [ 3.24955652  3.18929532  3.22215665 ...  1.26968489  7.55575584\n",
      "   0.45537539]\n",
      " [ 4.82423957  8.78413043  0.         ...  4.02447369  2.59987208\n",
      "   2.98214387]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32793147 0.         1.56330577 ... 0.         0.92658116 0.        ]\n",
      " [2.10055825 7.48308925 1.90273911 ... 8.69755434 3.2952615  6.56777688]\n",
      " ...\n",
      " [7.4665081  7.92738459 5.91936424 ... 4.70003044 8.57057783 5.11080511]\n",
      " [4.64269066 0.03659751 7.62211747 ... 8.81108958 4.25540697 3.05326202]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.56456136e-08 1.15707359e-07 8.98598971e-05 ... 1.16290273e-01\n",
      "  7.44875173e-05 8.92985451e-03]\n",
      " [7.25070258e-02 9.29810452e-07 6.69302494e-01 ... 4.88341452e-04\n",
      "  1.34246166e-03 2.50623044e-03]\n",
      " [8.78768468e-04 4.06572226e-05 5.15346966e-02 ... 9.48612962e-03\n",
      "  2.65123817e-01 1.01660155e-02]\n",
      " ...\n",
      " [6.89972753e-01 2.39386064e-01 8.70196505e-03 ... 3.93296717e-03\n",
      "  3.31977432e-05 2.09536330e-02]\n",
      " [1.52709434e-02 4.58238592e-04 1.48312682e-01 ... 1.43018365e-01\n",
      "  2.20992352e-02 3.35579425e-02]\n",
      " [2.04945891e-01 3.04226367e-01 1.55515984e-02 ... 1.71648327e-02\n",
      "  2.08083862e-03 4.16812857e-01]] & cached\n",
      "Re-used Cached Value, runNum =  386\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  386\n",
      "Provided input from cache for runNum = 386\n",
      "Provided input from cache for runNum = 387\n",
      "activation = [[ 5.98628268  5.82853424  6.41507634 ... 11.65835765  5.04561634\n",
      "   3.60504922]\n",
      " [ 0.          3.40634027  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05154949  3.65010899  0.84607724 ...  2.43226808  5.31747658\n",
      "   1.96074858]\n",
      " ...\n",
      " [ 0.          0.          0.01790171 ...  0.          0.\n",
      "   0.03827359]\n",
      " [ 3.25323825  3.19820627  3.22787296 ...  1.27427688  7.56436583\n",
      "   0.4601556 ]\n",
      " [ 4.82952303  8.79308154  0.         ...  4.03201113  2.60298693\n",
      "   2.98766865]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3269381  0.         1.56705687 ... 0.         0.9285504  0.        ]\n",
      " [2.10409852 7.48885729 1.90179791 ... 8.7137818  3.30140936 6.57613362]\n",
      " ...\n",
      " [7.47361848 7.94353743 5.92867878 ... 4.71405225 8.58238388 5.123966  ]\n",
      " [4.64648394 0.03770404 7.62786171 ... 8.82036385 4.25793732 3.05434924]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.45901745e-08 1.13259108e-07 8.79188289e-05 ... 1.15795964e-01\n",
      "  7.38319957e-05 8.86278901e-03]\n",
      " [7.20152793e-02 9.18701983e-07 6.70548640e-01 ... 4.81736144e-04\n",
      "  1.32712442e-03 2.48957661e-03]\n",
      " [8.77484379e-04 4.05359141e-05 5.15300350e-02 ... 9.48189812e-03\n",
      "  2.64219323e-01 1.01239103e-02]\n",
      " ...\n",
      " [6.90981468e-01 2.39292607e-01 8.66509478e-03 ... 3.93673224e-03\n",
      "  3.26186513e-05 2.08092316e-02]\n",
      " [1.52545442e-02 4.58766809e-04 1.47617985e-01 ... 1.42981646e-01\n",
      "  2.19496508e-02 3.35160398e-02]\n",
      " [2.04532971e-01 3.04826138e-01 1.54748004e-02 ... 1.72198464e-02\n",
      "  2.06455534e-03 4.17073606e-01]] & cached\n",
      "Re-used Cached Value, runNum =  387\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  387\n",
      "Provided input from cache for runNum = 387\n",
      "Provided input from cache for runNum = 388\n",
      "activation = [[ 5.99055313  5.83023611  6.41637902 ... 11.66657295  5.04685275\n",
      "   3.60672203]\n",
      " [ 0.          3.40745562  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.05014863  3.64760752  0.84419592 ...  2.43454029  5.32026088\n",
      "   1.96180198]\n",
      " ...\n",
      " [ 0.          0.          0.01987732 ...  0.          0.\n",
      "   0.04009435]\n",
      " [ 3.25687718  3.20704249  3.23355287 ...  1.27882435  7.57292494\n",
      "   0.46489911]\n",
      " [ 4.83474053  8.80192498  0.         ...  4.03945853  2.60603262\n",
      "   2.99314294]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32590544 0.         1.57074236 ... 0.         0.93048498 0.        ]\n",
      " [2.10761726 7.49458076 1.900861   ... 8.72995706 3.30753735 6.58446321]\n",
      " ...\n",
      " [7.48060431 7.95949563 5.93788883 ... 4.72791332 8.59404006 5.13701935]\n",
      " [4.65037011 0.03895491 7.63362019 ... 8.82969609 4.26055705 3.05548121]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.35654228e-08 1.10904318e-07 8.60331972e-05 ... 1.15320179e-01\n",
      "  7.32060021e-05 8.79763040e-03]\n",
      " [7.15307622e-02 9.07952877e-07 6.71783931e-01 ... 4.75215845e-04\n",
      "  1.31218406e-03 2.47319738e-03]\n",
      " [8.76147258e-04 4.04201914e-05 5.15210973e-02 ... 9.47700856e-03\n",
      "  2.63328717e-01 1.00820638e-02]\n",
      " ...\n",
      " [6.91996982e-01 2.39236465e-01 8.62856240e-03 ... 3.94012477e-03\n",
      "  3.20529852e-05 2.06660440e-02]\n",
      " [1.52397474e-02 4.59412306e-04 1.46935114e-01 ... 1.42948269e-01\n",
      "  2.18046960e-02 3.34766671e-02]\n",
      " [2.04106175e-01 3.05443211e-01 1.53976655e-02 ... 1.72720981e-02\n",
      "  2.04848095e-03 4.17316639e-01]] & cached\n",
      "Re-used Cached Value, runNum =  388\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  388\n",
      "Provided input from cache for runNum = 388\n",
      "Provided input from cache for runNum = 389\n",
      "activation = [[ 5.99483977  5.83197842  6.41768333 ... 11.67480134  5.04810623\n",
      "   3.60840208]\n",
      " [ 0.          3.40861076  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04873847  3.6450815   0.8423353  ...  2.43678556  5.32301279\n",
      "   1.96284984]\n",
      " ...\n",
      " [ 0.          0.          0.02184611 ...  0.          0.\n",
      "   0.0419069 ]\n",
      " [ 3.2604934   3.21584869  3.23920862 ...  1.28336222  7.5814484\n",
      "   0.46963155]\n",
      " [ 4.83994084  8.81075122  0.         ...  4.04690704  2.60908922\n",
      "   2.99860681]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32482612 0.         1.57436161 ... 0.         0.93237249 0.        ]\n",
      " [2.11113642 7.50030152 1.8999318  ... 8.74607424 3.31366158 6.59276054]\n",
      " ...\n",
      " [7.48753193 7.97536987 5.94703458 ... 4.74170866 8.60563664 5.1500157 ]\n",
      " [4.65425495 0.04020176 7.63934183 ... 8.83898904 4.26316614 3.05660824]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.25642994e-08 1.08614218e-07 8.41925214e-05 ... 1.14836959e-01\n",
      "  7.25952885e-05 8.73311097e-03]\n",
      " [7.10538132e-02 8.97499140e-07 6.73018409e-01 ... 4.68828148e-04\n",
      "  1.29749198e-03 2.45712899e-03]\n",
      " [8.74702210e-04 4.03045066e-05 5.15041373e-02 ... 9.47041467e-03\n",
      "  2.62429321e-01 1.00397127e-02]\n",
      " ...\n",
      " [6.93000395e-01 2.39209890e-01 8.59177716e-03 ... 3.94338783e-03\n",
      "  3.14968030e-05 2.05238011e-02]\n",
      " [1.52261932e-02 4.60155765e-04 1.46258047e-01 ... 1.42921357e-01\n",
      "  2.16616615e-02 3.34391597e-02]\n",
      " [2.03684599e-01 3.06107025e-01 1.53206391e-02 ... 1.73241557e-02\n",
      "  2.03258261e-03 4.17559288e-01]] & cached\n",
      "Re-used Cached Value, runNum =  389\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  389\n",
      "Provided input from cache for runNum = 389\n",
      "Provided input from cache for runNum = 390\n",
      "activation = [[ 5.99912046  5.83372955  6.41898542 ... 11.68301553  5.04935276\n",
      "   3.61008374]\n",
      " [ 0.          3.40976056  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04729373  3.64249883  0.84045759 ...  2.43896664  5.32570924\n",
      "   1.96386603]\n",
      " ...\n",
      " [ 0.          0.          0.0238229  ...  0.          0.\n",
      "   0.04373437]\n",
      " [ 3.26412372  3.22466596  3.24486964 ...  1.28792559  7.58997394\n",
      "   0.47437439]\n",
      " [ 4.84511922  8.81955405  0.         ...  4.05436967  2.61211967\n",
      "   3.00406737]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32393278 0.         1.57813092 ... 0.         0.93446004 0.        ]\n",
      " [2.11457655 7.50589763 1.89894781 ... 8.76205421 3.31969809 6.60098877]\n",
      " ...\n",
      " [7.49444548 7.99120666 5.95615969 ... 4.75548687 8.61720652 5.16299088]\n",
      " [4.65814697 0.04146478 7.6450565  ... 8.84824906 4.2657944  3.0577425 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.15761771e-08 1.06364162e-07 8.23791708e-05 ... 1.14351225e-01\n",
      "  7.19792262e-05 8.66900510e-03]\n",
      " [7.05857392e-02 8.87208567e-07 6.74264003e-01 ... 4.62574012e-04\n",
      "  1.28310943e-03 2.44133482e-03]\n",
      " [8.73348415e-04 4.01925717e-05 5.14882672e-02 ... 9.46531876e-03\n",
      "  2.61543653e-01 9.99827071e-03]\n",
      " ...\n",
      " [6.94005158e-01 2.39205270e-01 8.55488429e-03 ... 3.94742858e-03\n",
      "  3.09476127e-05 2.03843968e-02]\n",
      " [1.52130648e-02 4.60963641e-04 1.45578840e-01 ... 1.42924476e-01\n",
      "  2.15171911e-02 3.34051869e-02]\n",
      " [2.03252689e-01 3.06754233e-01 1.52427053e-02 ... 1.73769650e-02\n",
      "  2.01662336e-03 4.17799056e-01]] & cached\n",
      "Re-used Cached Value, runNum =  390\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  390\n",
      "Provided input from cache for runNum = 390\n",
      "Provided input from cache for runNum = 391\n",
      "activation = [[ 6.00336406  5.83543796  6.42026434 ... 11.6911558   5.05054191\n",
      "   3.61173626]\n",
      " [ 0.          3.41090951  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04588992  3.63997235  0.83861889 ...  2.4412147   5.32844153\n",
      "   1.96491368]\n",
      " ...\n",
      " [ 0.          0.          0.02580998 ...  0.          0.\n",
      "   0.04557425]\n",
      " [ 3.26778574  3.23351285  3.25051962 ...  1.29254122  7.59853278\n",
      "   0.47912693]\n",
      " [ 4.8502803   8.82832747  0.         ...  4.06184517  2.61513281\n",
      "   3.0095201 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32311612 0.         1.58193171 ... 0.         0.93665105 0.        ]\n",
      " [2.11800934 7.51147561 1.89799    ... 8.77800223 3.32571586 6.60920331]\n",
      " ...\n",
      " [7.5013528  8.00703341 5.96525313 ... 4.76927464 8.628782   5.17595126]\n",
      " [4.66198119 0.04265723 7.65071516 ... 8.85738843 4.26835399 3.05884062]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[5.06092642e-08 1.04147904e-07 8.06106924e-05 ... 1.13870953e-01\n",
      "  7.13582163e-05 8.60526040e-03]\n",
      " [7.01230119e-02 8.76893014e-07 6.75520472e-01 ... 4.56428741e-04\n",
      "  1.26886161e-03 2.42561164e-03]\n",
      " [8.72082183e-04 4.00780655e-05 5.14745240e-02 ... 9.46162453e-03\n",
      "  2.60650003e-01 9.95691567e-03]\n",
      " ...\n",
      " [6.94985055e-01 2.39142537e-01 8.51840280e-03 ... 3.95185289e-03\n",
      "  3.04092462e-05 2.02446611e-02]\n",
      " [1.51972956e-02 4.61606010e-04 1.44877799e-01 ... 1.42899786e-01\n",
      "  2.13705492e-02 3.33653866e-02]\n",
      " [2.02838533e-01 3.07368414e-01 1.51666050e-02 ... 1.74327569e-02\n",
      "  2.00087580e-03 4.18034333e-01]] & cached\n",
      "Re-used Cached Value, runNum =  391\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  391\n",
      "Provided input from cache for runNum = 391\n",
      "Provided input from cache for runNum = 392\n",
      "activation = [[ 6.00759702  5.83713716  6.42154037 ... 11.69925931  5.05170792\n",
      "   3.61338333]\n",
      " [ 0.          3.41206588  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04449065  3.63745837  0.83679376 ...  2.44347949  5.33117857\n",
      "   1.96596798]\n",
      " ...\n",
      " [ 0.          0.          0.02780193 ...  0.          0.\n",
      "   0.04742426]\n",
      " [ 3.27148153  3.2423954   3.25616931 ...  1.29722495  7.60715979\n",
      "   0.48389026]\n",
      " [ 4.85541384  8.83705767  0.         ...  4.06928944  2.61811264\n",
      "   3.01494918]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32237394 0.         1.58577166 ... 0.         0.93892213 0.        ]\n",
      " [2.12147871 7.51708535 1.8970766  ... 8.79395459 3.33174686 6.61743082]\n",
      " ...\n",
      " [7.50829608 8.02290414 5.97434571 ... 4.78313411 8.64041631 5.18892071]\n",
      " [4.66580007 0.04383138 7.65633954 ... 8.86647202 4.27090107 3.05993409]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.96627461e-08 1.01982645e-07 7.88820173e-05 ... 1.13386151e-01\n",
      "  7.07361672e-05 8.54206510e-03]\n",
      " [6.96672271e-02 8.66771982e-07 6.76766763e-01 ... 4.50356578e-04\n",
      "  1.25474978e-03 2.41000562e-03]\n",
      " [8.70800736e-04 3.99635964e-05 5.14563715e-02 ... 9.45723250e-03\n",
      "  2.59736017e-01 9.91509399e-03]\n",
      " ...\n",
      " [6.95938247e-01 2.39071716e-01 8.48178481e-03 ... 3.95575045e-03\n",
      "  2.98749885e-05 2.01044127e-02]\n",
      " [1.51828928e-02 4.62293964e-04 1.44183738e-01 ... 1.42878376e-01\n",
      "  2.12239785e-02 3.33261408e-02]\n",
      " [2.02442662e-01 3.08017832e-01 1.50914310e-02 ... 1.74885698e-02\n",
      "  1.98513920e-03 4.18265522e-01]] & cached\n",
      "Re-used Cached Value, runNum =  392\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  392\n",
      "Provided input from cache for runNum = 392\n",
      "Provided input from cache for runNum = 393\n",
      "activation = [[ 6.01183537  5.8388597   6.42282096 ... 11.70735675  5.05287532\n",
      "   3.61503734]\n",
      " [ 0.          3.41319691  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04306116  3.6349129   0.83496978 ...  2.44571557  5.33387737\n",
      "   1.96700521]\n",
      " ...\n",
      " [ 0.          0.          0.02979375 ...  0.          0.\n",
      "   0.04927319]\n",
      " [ 3.27519848  3.25129359  3.26180229 ...  1.30195578  7.61581125\n",
      "   0.48866887]\n",
      " [ 4.86052065  8.84573863  0.         ...  4.07670145  2.62105723\n",
      "   3.02034852]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3216301  0.         1.58957923 ... 0.         0.94121121 0.        ]\n",
      " [2.1249143  7.52262785 1.8961621  ... 8.80981798 3.33772258 6.62561047]\n",
      " ...\n",
      " [7.51517635 8.03868849 5.98335703 ... 4.79692605 8.65199672 5.20183594]\n",
      " [4.66964204 0.04505728 7.66195721 ... 8.87555743 4.27348077 3.06105177]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.87362451e-08 9.98738275e-08 7.71947806e-05 ... 1.12899567e-01\n",
      "  7.01270424e-05 8.47973808e-03]\n",
      " [6.92182500e-02 8.56986566e-07 6.78012561e-01 ... 4.44413353e-04\n",
      "  1.24091775e-03 2.39473577e-03]\n",
      " [8.69425835e-04 3.98495725e-05 5.14306555e-02 ... 9.45169674e-03\n",
      "  2.58828807e-01 9.87317325e-03]\n",
      " ...\n",
      " [6.96895579e-01 2.39034116e-01 8.44548085e-03 ... 3.95970432e-03\n",
      "  2.93493035e-05 1.99660293e-02]\n",
      " [1.51693475e-02 4.63064538e-04 1.43495586e-01 ... 1.42865524e-01\n",
      "  2.10797730e-02 3.32897052e-02]\n",
      " [2.02036503e-01 3.08687384e-01 1.50159983e-02 ... 1.75434281e-02\n",
      "  1.96943642e-03 4.18487151e-01]] & cached\n",
      "Re-used Cached Value, runNum =  393\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  393\n",
      "Provided input from cache for runNum = 393\n",
      "Provided input from cache for runNum = 394\n",
      "activation = [[ 6.01607782  5.84058532  6.42409976 ... 11.71542543  5.05402266\n",
      "   3.61669127]\n",
      " [ 0.          3.41428818  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04164342  3.63239736  0.83316912 ...  2.44798815  5.33659189\n",
      "   1.96805606]\n",
      " ...\n",
      " [ 0.          0.          0.03178916 ...  0.          0.\n",
      "   0.05113006]\n",
      " [ 3.2789327   3.26021155  3.26742608 ...  1.30671343  7.62448201\n",
      "   0.49344705]\n",
      " [ 4.8656127   8.85440423  0.         ...  4.08412923  2.62399354\n",
      "   3.0257349 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.32084976 0.         1.59333036 ... 0.         0.94346724 0.        ]\n",
      " [2.12835942 7.52816337 1.89526663 ... 8.82565453 3.3436827  6.63377469]\n",
      " ...\n",
      " [7.52203323 8.05445167 5.99231853 ... 4.81069525 8.6635549  5.21471706]\n",
      " [4.67357829 0.04641269 7.66760681 ... 8.88467619 4.27612616 3.06222179]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.78364673e-08 9.78162312e-08 7.55567656e-05 ... 1.12422776e-01\n",
      "  6.95295988e-05 8.41838135e-03]\n",
      " [6.87746723e-02 8.47371410e-07 6.79249548e-01 ... 4.38576194e-04\n",
      "  1.22724654e-03 2.37968807e-03]\n",
      " [8.68140127e-04 3.97383662e-05 5.14071897e-02 ... 9.44698896e-03\n",
      "  2.57937788e-01 9.83169810e-03]\n",
      " ...\n",
      " [6.97847563e-01 2.38968772e-01 8.40952865e-03 ... 3.96374895e-03\n",
      "  2.88350565e-05 1.98284218e-02]\n",
      " [1.51560749e-02 4.63789110e-04 1.42808539e-01 ... 1.42840990e-01\n",
      "  2.09375880e-02 3.32519451e-02]\n",
      " [2.01627251e-01 3.09328530e-01 1.49408867e-02 ... 1.75984328e-02\n",
      "  1.95384579e-03 4.18693326e-01]] & cached\n",
      "Re-used Cached Value, runNum =  394\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  394\n",
      "Provided input from cache for runNum = 394\n",
      "Provided input from cache for runNum = 395\n",
      "activation = [[ 6.02031545  5.84230567  6.42537916 ... 11.72346755  5.05515732\n",
      "   3.61833939]\n",
      " [ 0.          3.41538794  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.04024768  3.62991472  0.83139696 ...  2.45029329  5.339317\n",
      "   1.96912411]\n",
      " ...\n",
      " [ 0.          0.          0.03378266 ...  0.          0.\n",
      "   0.05298768]\n",
      " [ 3.28268948  3.2691147   3.27304586 ...  1.31149288  7.63315261\n",
      "   0.49821512]\n",
      " [ 4.87068888  8.86305388  0.         ...  4.09156386  2.62690401\n",
      "   3.03110878]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31997098 0.         1.59697592 ... 0.         0.94565245 0.        ]\n",
      " [2.13189142 7.53380901 1.89445979 ... 8.84155824 3.34974396 6.6419692 ]\n",
      " ...\n",
      " [7.52886749 8.07016228 6.00123734 ... 4.82444308 8.67509438 5.227553  ]\n",
      " [4.67743566 0.04764528 7.67318469 ... 8.89365599 4.27869784 3.06333735]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.69636443e-08 9.58142710e-08 7.39735703e-05 ... 1.11953239e-01\n",
      "  6.89482029e-05 8.35801149e-03]\n",
      " [6.83266812e-02 8.37816593e-07 6.80449172e-01 ... 4.32768562e-04\n",
      "  1.21362720e-03 2.36464897e-03]\n",
      " [8.66785093e-04 3.96231272e-05 5.13818066e-02 ... 9.44085195e-03\n",
      "  2.57018148e-01 9.78952437e-03]\n",
      " ...\n",
      " [6.98786281e-01 2.38898909e-01 8.37415425e-03 ... 3.96731483e-03\n",
      "  2.83291375e-05 1.96907631e-02]\n",
      " [1.51441702e-02 4.64519140e-04 1.42143365e-01 ... 1.42809643e-01\n",
      "  2.07970175e-02 3.32137368e-02]\n",
      " [2.01233896e-01 3.10007110e-01 1.48677869e-02 ... 1.76538119e-02\n",
      "  1.93850854e-03 4.18899449e-01]] & cached\n",
      "Re-used Cached Value, runNum =  395\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  395\n",
      "Provided input from cache for runNum = 395\n",
      "Provided input from cache for runNum = 396\n",
      "activation = [[ 6.02456447  5.84406185  6.4266612  ... 11.73152388  5.05630277\n",
      "   3.61999677]\n",
      " [ 0.          3.41641755  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0388045   3.62735756  0.82960309 ...  2.45253227  5.34197739\n",
      "   1.97016386]\n",
      " ...\n",
      " [ 0.          0.          0.03577913 ...  0.          0.\n",
      "   0.05485048]\n",
      " [ 3.28641925  3.2779401   3.27864033 ...  1.31623009  7.64178293\n",
      "   0.50294607]\n",
      " [ 4.87573696  8.87166475  0.         ...  4.09897865  2.62977994\n",
      "   3.03646333]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31906703 0.         1.60058405 ... 0.         0.94783921 0.        ]\n",
      " [2.13536046 7.53935994 1.89361075 ... 8.85733873 3.3557197  6.6501057 ]\n",
      " ...\n",
      " [7.53560393 8.08572722 6.01006582 ... 4.83805981 8.68652458 5.24029349]\n",
      " [4.68128222 0.04889023 7.6787313  ... 8.90259465 4.28126055 3.06444726]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.61109942e-08 9.38747077e-08 7.24336983e-05 ... 1.11491591e-01\n",
      "  6.83817242e-05 8.29887722e-03]\n",
      " [6.78768562e-02 8.28518229e-07 6.81619632e-01 ... 4.27036176e-04\n",
      "  1.20032602e-03 2.34985157e-03]\n",
      " [8.65420546e-04 3.95139191e-05 5.13580095e-02 ... 9.43497571e-03\n",
      "  2.56109725e-01 9.74789460e-03]\n",
      " ...\n",
      " [6.99752994e-01 2.38895709e-01 8.33940923e-03 ... 3.97149741e-03\n",
      "  2.78320876e-05 1.95567390e-02]\n",
      " [1.51348862e-02 4.65440359e-04 1.41506624e-01 ... 1.42817855e-01\n",
      "  2.06585430e-02 3.31816083e-02]\n",
      " [2.00814405e-01 3.10703178e-01 1.47939805e-02 ... 1.77079053e-02\n",
      "  1.92321583e-03 4.19100109e-01]] & cached\n",
      "Re-used Cached Value, runNum =  396\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  396\n",
      "Provided input from cache for runNum = 396\n",
      "Provided input from cache for runNum = 397\n",
      "activation = [[ 6.02879776  5.84580775  6.42793718 ... 11.7395516   5.05742988\n",
      "   3.62164394]\n",
      " [ 0.          3.41745132  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03736873  3.62480123  0.82782432 ...  2.45477415  5.34462207\n",
      "   1.97120616]\n",
      " ...\n",
      " [ 0.          0.          0.03777727 ...  0.          0.\n",
      "   0.05671468]\n",
      " [ 3.29015977  3.286713    3.28421373 ...  1.32092172  7.65037481\n",
      "   0.50764448]\n",
      " [ 4.88079412  8.88031079  0.         ...  4.10646923  2.63267359\n",
      "   3.04182923]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31818437 0.         1.6041872  ... 0.         0.95004925 0.        ]\n",
      " [2.13883507 7.54495021 1.89277924 ... 8.87312736 3.36171161 6.65823559]\n",
      " ...\n",
      " [7.54233508 8.10125462 6.0188524  ... 4.85162816 8.69792988 5.25298566]\n",
      " [4.68503564 0.04997998 7.6842151  ... 8.91136458 4.28373087 3.06549314]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.52809915e-08 9.19747594e-08 7.09304608e-05 ... 1.11030018e-01\n",
      "  6.78203327e-05 8.24016097e-03]\n",
      " [6.74412338e-02 8.19317295e-07 6.82813293e-01 ... 4.21403435e-04\n",
      "  1.18717910e-03 2.33522379e-03]\n",
      " [8.64140627e-04 3.94013333e-05 5.13310006e-02 ... 9.42818599e-03\n",
      "  2.55182508e-01 9.70584177e-03]\n",
      " ...\n",
      " [7.00682127e-01 2.38863587e-01 8.30359514e-03 ... 3.97557573e-03\n",
      "  2.73408993e-05 1.94229563e-02]\n",
      " [1.51243378e-02 4.66268038e-04 1.40845583e-01 ... 1.42795858e-01\n",
      "  2.05177180e-02 3.31450261e-02]\n",
      " [2.00416875e-01 3.11408802e-01 1.47200517e-02 ... 1.77634291e-02\n",
      "  1.90802402e-03 4.19298678e-01]] & cached\n",
      "Re-used Cached Value, runNum =  397\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  397\n",
      "Provided input from cache for runNum = 397\n",
      "Provided input from cache for runNum = 398\n",
      "activation = [[ 6.03304905  5.84759403  6.42923022 ... 11.7476018   5.05857431\n",
      "   3.62330758]\n",
      " [ 0.          3.41848309  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03589782  3.62217997  0.82602556 ...  2.45692989  5.34720026\n",
      "   1.97221052]\n",
      " ...\n",
      " [ 0.          0.          0.03977527 ...  0.          0.\n",
      "   0.05858167]\n",
      " [ 3.2939002   3.29547764  3.28976524 ...  1.32561982  7.65896265\n",
      "   0.51233704]\n",
      " [ 4.88585699  8.88894811  0.         ...  4.11397427  2.63555643\n",
      "   3.04719833]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31733437 0.         1.6077955  ... 0.         0.95231083 0.        ]\n",
      " [2.14236558 7.55055989 1.89200879 ... 8.88890973 3.36772462 6.6663602 ]\n",
      " ...\n",
      " [7.54905887 8.11677602 6.02760932 ... 4.86520798 8.70933377 5.26566474]\n",
      " [4.68880991 0.05112044 7.68971345 ... 8.92015721 4.28624372 3.06656388]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.44670708e-08 9.01141751e-08 6.94604872e-05 ... 1.10559843e-01\n",
      "  6.72657411e-05 8.18175107e-03]\n",
      " [6.70105996e-02 8.10344654e-07 6.84007486e-01 ... 4.15886400e-04\n",
      "  1.17419924e-03 2.32088383e-03]\n",
      " [8.62809781e-04 3.92881006e-05 5.12986437e-02 ... 9.42037040e-03\n",
      "  2.54255304e-01 9.66361518e-03]\n",
      " ...\n",
      " [7.01609674e-01 2.38851550e-01 8.26772090e-03 ... 3.97972208e-03\n",
      "  2.68545507e-05 1.92911914e-02]\n",
      " [1.51140400e-02 4.67145635e-04 1.40186956e-01 ... 1.42783617e-01\n",
      "  2.03768544e-02 3.31105614e-02]\n",
      " [2.00016242e-01 3.12125776e-01 1.46459366e-02 ... 1.78185820e-02\n",
      "  1.89276233e-03 4.19494746e-01]] & cached\n",
      "Re-used Cached Value, runNum =  398\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  398\n",
      "Provided input from cache for runNum = 398\n",
      "Provided input from cache for runNum = 399\n",
      "activation = [[ 6.03726905  5.84933643  6.43050682 ... 11.7555904   5.05967329\n",
      "   3.62494837]\n",
      " [ 0.          3.41951871  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03446296  3.61962671  0.82425873 ...  2.45915457  5.34980872\n",
      "   1.97324347]\n",
      " ...\n",
      " [ 0.          0.          0.0417699  ...  0.          0.\n",
      "   0.06045301]\n",
      " [ 3.29764965  3.30424249  3.29531192 ...  1.33035557  7.66756966\n",
      "   0.51703546]\n",
      " [ 4.89092344  8.89758758  0.         ...  4.12149372  2.63844891\n",
      "   3.05256813]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31648215 0.         1.61138939 ... 0.         0.95457179 0.        ]\n",
      " [2.14586363 7.55611684 1.89121799 ... 8.90458396 3.37367558 6.67443089]\n",
      " ...\n",
      " [7.5557521  8.13223015 6.03632214 ... 4.87873038 8.72071443 5.27830131]\n",
      " [4.69250726 0.05213995 7.69516289 ... 8.92883848 4.28867741 3.06758879]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.36813755e-08 8.83053176e-08 6.80321230e-05 ... 1.10110512e-01\n",
      "  6.67137058e-05 8.12470762e-03]\n",
      " [6.65904263e-02 8.01424539e-07 6.85209266e-01 ... 4.10494320e-04\n",
      "  1.16137383e-03 2.30678430e-03]\n",
      " [8.61730498e-04 3.91808687e-05 5.12716360e-02 ... 9.41446210e-03\n",
      "  2.53324800e-01 9.62264646e-03]\n",
      " ...\n",
      " [7.02511361e-01 2.38772169e-01 8.23088720e-03 ... 3.98328394e-03\n",
      "  2.63752802e-05 1.91586253e-02]\n",
      " [1.51040317e-02 4.67931924e-04 1.39517025e-01 ... 1.42752140e-01\n",
      "  2.02352451e-02 3.30741609e-02]\n",
      " [1.99625462e-01 3.12793025e-01 1.45711642e-02 ... 1.78729884e-02\n",
      "  1.87751934e-03 4.19667090e-01]] & cached\n",
      "Re-used Cached Value, runNum =  399\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  399\n",
      "Provided input from cache for runNum = 399\n",
      "Provided input from cache for runNum = 400\n",
      "activation = [[ 6.04148863  5.85107816  6.43178392 ... 11.76355911  5.06075468\n",
      "   3.62658389]\n",
      " [ 0.          3.42059302  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03300087  3.61705245  0.82247519 ...  2.46134792  5.35238678\n",
      "   1.97426002]\n",
      " ...\n",
      " [ 0.          0.          0.04376369 ...  0.          0.\n",
      "   0.06233162]\n",
      " [ 3.30135042  3.31293476  3.3008166  ...  1.33503899  7.67612771\n",
      "   0.52169506]\n",
      " [ 4.89600461  8.90623799  0.         ...  4.12903862  2.64136428\n",
      "   3.05794015]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3157348  0.         1.6150841  ... 0.         0.95698557 0.        ]\n",
      " [2.14954103 7.56185514 1.8905697  ... 8.92036398 3.37974113 6.68253765]\n",
      " ...\n",
      " [7.56236038 8.14754863 6.04496483 ... 4.89214894 8.73201558 5.29086446]\n",
      " [4.69615479 0.05307661 7.70057336 ... 8.93742621 4.29104936 3.06858695]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.29107445e-08 8.65422622e-08 6.66472846e-05 ... 1.09673693e-01\n",
      "  6.61702927e-05 8.06856028e-03]\n",
      " [6.61473670e-02 7.92226263e-07 6.86361680e-01 ... 4.05079767e-04\n",
      "  1.14864898e-03 2.29252357e-03]\n",
      " [8.60516809e-04 3.90685197e-05 5.12486987e-02 ... 9.40811779e-03\n",
      "  2.52395163e-01 9.58151435e-03]\n",
      " ...\n",
      " [7.03445694e-01 2.38714073e-01 8.19637328e-03 ... 3.98816830e-03\n",
      "  2.59112861e-05 1.90295097e-02]\n",
      " [1.50931965e-02 4.68707716e-04 1.38874432e-01 ... 1.42739926e-01\n",
      "  2.00947217e-02 3.30397028e-02]\n",
      " [1.99226954e-01 3.13447639e-01 1.44994517e-02 ... 1.79300642e-02\n",
      "  1.86275337e-03 4.19849347e-01]] & cached\n",
      "Re-used Cached Value, runNum =  400\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  400\n",
      "Provided input from cache for runNum = 400\n",
      "Provided input from cache for runNum = 401\n",
      "activation = [[ 6.04571749  5.85285413  6.43306849 ... 11.77153313  5.06184859\n",
      "   3.62822702]\n",
      " [ 0.          3.4216375   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0315525   3.61448278  0.82071528 ...  2.46355619  5.35495293\n",
      "   1.97528294]\n",
      " ...\n",
      " [ 0.          0.          0.04574871 ...  0.          0.\n",
      "   0.064204  ]\n",
      " [ 3.30502137  3.3215679   3.30629064 ...  1.33968626  7.68465087\n",
      "   0.52632348]\n",
      " [ 4.90101861  8.91479407  0.         ...  4.13651204  2.6442148\n",
      "   3.06326671]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31494232 0.         1.61871648 ... 0.         0.95937843 0.        ]\n",
      " [2.15323022 7.56761538 1.88994383 ... 8.93612889 3.38579863 6.69063325]\n",
      " ...\n",
      " [7.56892512 8.16281091 6.05355464 ... 4.90550966 8.74326916 5.30337537]\n",
      " [4.69988454 0.05417241 7.70599932 ... 8.94609814 4.29353097 3.06963783]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.21659593e-08 8.48487628e-08 6.52944285e-05 ... 1.09238251e-01\n",
      "  6.56424754e-05 8.01348179e-03]\n",
      " [6.57254875e-02 7.83493152e-07 6.87536898e-01 ... 3.99803406e-04\n",
      "  1.13619016e-03 2.27869597e-03]\n",
      " [8.59361213e-04 3.89642093e-05 5.12172929e-02 ... 9.39992445e-03\n",
      "  2.51466116e-01 9.54013898e-03]\n",
      " ...\n",
      " [7.04342777e-01 2.38649368e-01 8.16057904e-03 ... 3.99174859e-03\n",
      "  2.54506815e-05 1.88991886e-02]\n",
      " [1.50835481e-02 4.69572933e-04 1.38217121e-01 ... 1.42700077e-01\n",
      "  1.99551035e-02 3.30042515e-02]\n",
      " [1.98842924e-01 3.14148395e-01 1.44267701e-02 ... 1.79842444e-02\n",
      "  1.84790021e-03 4.20013901e-01]] & cached\n",
      "Re-used Cached Value, runNum =  401\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  401\n",
      "Provided input from cache for runNum = 401\n",
      "Provided input from cache for runNum = 402\n",
      "activation = [[ 6.04994883  5.85464184  6.43436096 ... 11.77951197  5.06295833\n",
      "   3.62987764]\n",
      " [ 0.          3.42270399  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.03010429  3.61190628  0.81896138 ...  2.46574218  5.35748356\n",
      "   1.97629916]\n",
      " ...\n",
      " [ 0.          0.          0.04772781 ...  0.          0.\n",
      "   0.06606658]\n",
      " [ 3.3087928   3.33030092  3.31179935 ...  1.34444686  7.69324267\n",
      "   0.53099325]\n",
      " [ 4.90602194  8.92333892  0.         ...  4.14399864  2.64706265\n",
      "   3.06859355]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31416768 0.         1.62232353 ... 0.         0.96179108 0.        ]\n",
      " [2.15677759 7.5732037  1.88923446 ... 8.9517038  3.39173567 6.69864138]\n",
      " ...\n",
      " [7.57554876 8.17812462 6.062158   ... 4.91894017 8.75457215 5.31589611]\n",
      " [4.70358269 0.05520778 7.7113936  ... 8.95470353 4.29598575 3.07066473]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.14324830e-08 8.31842016e-08 6.39685004e-05 ... 1.08804873e-01\n",
      "  6.51080708e-05 7.95901954e-03]\n",
      " [6.53083367e-02 7.74871178e-07 6.88694032e-01 ... 3.94633678e-04\n",
      "  1.12391849e-03 2.26509656e-03]\n",
      " [8.58369707e-04 3.88696172e-05 5.11898503e-02 ... 9.39454050e-03\n",
      "  2.50536625e-01 9.50033111e-03]\n",
      " ...\n",
      " [7.05248873e-01 2.38594645e-01 8.12495584e-03 ... 3.99547002e-03\n",
      "  2.49946865e-05 1.87711165e-02]\n",
      " [1.50770454e-02 4.70565275e-04 1.37584296e-01 ... 1.42711544e-01\n",
      "  1.98155148e-02 3.29755635e-02]\n",
      " [1.98441615e-01 3.14822529e-01 1.43531960e-02 ... 1.80376228e-02\n",
      "  1.83291477e-03 4.20171096e-01]] & cached\n",
      "Re-used Cached Value, runNum =  402\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  402\n",
      "Provided input from cache for runNum = 402\n",
      "Provided input from cache for runNum = 403\n",
      "activation = [[ 6.05415428  5.85640206  6.43563582 ... 11.78743992  5.06402978\n",
      "   3.63151392]\n",
      " [ 0.          3.42377811  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02868671  3.60938677  0.81723949 ...  2.46797748  5.36004595\n",
      "   1.97733545]\n",
      " ...\n",
      " [ 0.          0.          0.04970847 ...  0.          0.\n",
      "   0.06793198]\n",
      " [ 3.31258858  3.33908577  3.31729957 ...  1.34926657  7.70186704\n",
      "   0.53568353]\n",
      " [ 4.91107365  8.93195299  0.         ...  4.15158309  2.64998639\n",
      "   3.07396571]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31347802 0.         1.62601043 ... 0.         0.96430199 0.        ]\n",
      " [2.1603359  7.57878159 1.88853489 ... 8.9672359  3.39766183 6.70663217]\n",
      " ...\n",
      " [7.58211912 8.19336358 6.07068335 ... 4.93230247 8.76582265 5.3283711 ]\n",
      " [4.7071824  0.05610813 7.71672826 ... 8.96314139 4.29832438 3.07162775]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[4.07098337e-08 8.15409841e-08 6.26724235e-05 ... 1.08376181e-01\n",
      "  6.45738774e-05 7.90471567e-03]\n",
      " [6.48888281e-02 7.66152928e-07 6.89871630e-01 ... 3.89550673e-04\n",
      "  1.11180093e-03 2.25150153e-03]\n",
      " [8.57314201e-04 3.87689138e-05 5.11615039e-02 ... 9.38957791e-03\n",
      "  2.49602513e-01 9.46025690e-03]\n",
      " ...\n",
      " [7.06154424e-01 2.38513666e-01 8.08995446e-03 ... 4.00015335e-03\n",
      "  2.45516218e-05 1.86447372e-02]\n",
      " [1.50659268e-02 4.71370490e-04 1.36925843e-01 ... 1.42696356e-01\n",
      "  1.96742271e-02 3.29411877e-02]\n",
      " [1.98046228e-01 3.15466852e-01 1.42809360e-02 ... 1.80947641e-02\n",
      "  1.81831166e-03 4.20336048e-01]] & cached\n",
      "Re-used Cached Value, runNum =  403\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  403\n",
      "Provided input from cache for runNum = 403\n",
      "iterations = 400\n",
      "Accuracy = 0.6911219512195121\n",
      "Provided input from cache for runNum = 404\n",
      "activation = [[ 6.05838574  5.85822184  6.43693539 ... 11.79540867  5.06514035\n",
      "   3.63317249]\n",
      " [ 0.          3.42477106  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02720339  3.60676809  0.81546988 ...  2.47009968  5.36250205\n",
      "   1.97832087]\n",
      " ...\n",
      " [ 0.          0.          0.05168336 ...  0.          0.\n",
      "   0.06978861]\n",
      " [ 3.31636535  3.34785208  3.32276812 ...  1.35408415  7.71046133\n",
      "   0.54036789]\n",
      " [ 4.91610735  8.9405343   0.         ...  4.15915002  2.65289281\n",
      "   3.07932336]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31292348 0.         1.6297913  ... 0.         0.9669733  0.        ]\n",
      " [2.16392957 7.58436919 1.88787577 ... 8.98277954 3.40363556 6.71462491]\n",
      " ...\n",
      " [7.58869167 8.20864107 6.07919527 ... 4.9457197  8.77711368 5.34084779]\n",
      " [4.71079904 0.05709334 7.72206069 ... 8.97160299 4.30070815 3.07261287]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.99901661e-08 7.99218223e-08 6.13902942e-05 ... 1.07918156e-01\n",
      "  6.40391999e-05 7.84986485e-03]\n",
      " [6.44751643e-02 7.57793478e-07 6.91061719e-01 ... 3.84571457e-04\n",
      "  1.09982844e-03 2.23815948e-03]\n",
      " [8.55998785e-04 3.86621159e-05 5.11183150e-02 ... 9.38147417e-03\n",
      "  2.48656922e-01 9.41862475e-03]\n",
      " ...\n",
      " [7.07053601e-01 2.38492928e-01 8.05532220e-03 ... 4.00534513e-03\n",
      "  2.41121333e-05 1.85206768e-02]\n",
      " [1.50541340e-02 4.72288713e-04 1.36268043e-01 ... 1.42699923e-01\n",
      "  1.95322307e-02 3.29093122e-02]\n",
      " [1.97656519e-01 3.16193509e-01 1.42093365e-02 ... 1.81536049e-02\n",
      "  1.80371560e-03 4.20521547e-01]] & cached\n",
      "Re-used Cached Value, runNum =  404\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  404\n",
      "Provided input from cache for runNum = 404\n",
      "Provided input from cache for runNum = 405\n",
      "activation = [[ 6.06259283  5.86000298  6.43822047 ... 11.80332251  5.06620664\n",
      "   3.63480522]\n",
      " [ 0.          3.42576786  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02577561  3.60422508  0.81375202 ...  2.47228371  5.36501383\n",
      "   1.97933796]\n",
      " ...\n",
      " [ 0.          0.          0.05366338 ...  0.          0.\n",
      "   0.07165328]\n",
      " [ 3.32013548  3.35659786  3.32823198 ...  1.3589234   7.71904506\n",
      "   0.54504239]\n",
      " [ 4.92111153  8.94906795  0.         ...  4.16669635  2.65576147\n",
      "   3.08465994]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31222416 0.         1.63344383 ... 0.         0.96955624 0.        ]\n",
      " [2.16749999 7.58990723 1.88720529 ... 8.99821057 3.40954913 6.72255863]\n",
      " ...\n",
      " [7.59520406 8.2238256  6.08764682 ... 4.95905941 8.78833471 5.3532563 ]\n",
      " [4.71439921 0.05803576 7.72737032 ... 8.98002394 4.30306593 3.07358235]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.93066107e-08 7.83625132e-08 6.01563183e-05 ... 1.07489254e-01\n",
      "  6.35182513e-05 7.79689235e-03]\n",
      " [6.40738147e-02 7.49567601e-07 6.92243435e-01 ... 3.79694837e-04\n",
      "  1.08801859e-03 2.22509231e-03]\n",
      " [8.55008466e-04 3.85651045e-05 5.10833641e-02 ... 9.37541156e-03\n",
      "  2.47710011e-01 9.37840658e-03]\n",
      " ...\n",
      " [7.07922653e-01 2.38394557e-01 8.01957548e-03 ... 4.00945105e-03\n",
      "  2.36785399e-05 1.83950945e-02]\n",
      " [1.50442107e-02 4.73137345e-04 1.35606796e-01 ... 1.42676924e-01\n",
      "  1.93907027e-02 3.28747808e-02]\n",
      " [1.97276224e-01 3.16866963e-01 1.41368296e-02 ... 1.82095843e-02\n",
      "  1.78907928e-03 4.20669187e-01]] & cached\n",
      "Re-used Cached Value, runNum =  405\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  405\n",
      "Provided input from cache for runNum = 405\n",
      "Provided input from cache for runNum = 406\n",
      "activation = [[ 6.06678303  5.86176666  6.43949748 ... 11.81119655  5.06723712\n",
      "   3.63642571]\n",
      " [ 0.          3.42675847  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02436261  3.60171909  0.81205105 ...  2.47449451  5.36755572\n",
      "   1.98036646]\n",
      " ...\n",
      " [ 0.          0.          0.05564924 ...  0.          0.\n",
      "   0.07353047]\n",
      " [ 3.32386129  3.365259    3.33365989 ...  1.3636996   7.72757119\n",
      "   0.54967252]\n",
      " [ 4.92611105  8.95761024  0.         ...  4.17428229  2.6586354\n",
      "   3.09000212]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31142031 0.         1.6369973  ... 0.         0.97200198 0.        ]\n",
      " [2.17108951 7.59546555 1.88655682 ... 9.01364097 3.41547244 6.73049004]\n",
      " ...\n",
      " [7.60158205 8.23880896 6.09598526 ... 4.97220482 8.79939061 5.3655501 ]\n",
      " [4.7179231  0.05886937 7.73261632 ... 8.9882928  4.30532398 3.07449682]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.86412449e-08 7.68419552e-08 5.89633650e-05 ... 1.07080399e-01\n",
      "  6.30220697e-05 7.74508882e-03]\n",
      " [6.36636729e-02 7.41339169e-07 6.93401385e-01 ... 3.74855402e-04\n",
      "  1.07636768e-03 2.21205081e-03]\n",
      " [8.53879302e-04 3.84613357e-05 5.10475121e-02 ... 9.36829840e-03\n",
      "  2.46758406e-01 9.33770779e-03]\n",
      " ...\n",
      " [7.08808874e-01 2.38299892e-01 7.98499333e-03 ... 4.01395769e-03\n",
      "  2.32596659e-05 1.82710734e-02]\n",
      " [1.50333992e-02 4.73903459e-04 1.34959533e-01 ... 1.42642567e-01\n",
      "  1.92530291e-02 3.28386188e-02]\n",
      " [1.96888294e-01 3.17532988e-01 1.40657335e-02 ... 1.82666222e-02\n",
      "  1.77490511e-03 4.20816659e-01]] & cached\n",
      "Re-used Cached Value, runNum =  406\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  406\n",
      "Provided input from cache for runNum = 406\n",
      "Provided input from cache for runNum = 407\n",
      "activation = [[ 6.07097469  5.86356633  6.44077704 ... 11.81908907  5.0682686\n",
      "   3.63805686]\n",
      " [ 0.          3.42773021  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02298598  3.59925099  0.81038854 ...  2.47674325  5.37012551\n",
      "   1.98141728]\n",
      " ...\n",
      " [ 0.          0.          0.05764503 ...  0.          0.\n",
      "   0.07542576]\n",
      " [ 3.32755159  3.37385038  3.33905222 ...  1.3684319   7.73605689\n",
      "   0.55426877]\n",
      " [ 4.93108618  8.96609722  0.         ...  4.18182856  2.6614852\n",
      "   3.09531793]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.31051611 0.         1.64044814 ... 0.         0.97435481 0.        ]\n",
      " [2.17472384 7.6010804  1.88594413 ... 9.02910458 3.42144571 6.73843956]\n",
      " ...\n",
      " [7.60793252 8.2537617  6.10428602 ... 4.98533835 8.81043644 5.37781366]\n",
      " [4.72151816 0.05987358 7.73789317 ... 8.99665559 4.30767944 3.0754731 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.79990091e-08 7.53748545e-08 5.78059342e-05 ... 1.06680841e-01\n",
      "  6.25415765e-05 7.69446157e-03]\n",
      " [6.32611880e-02 7.33403168e-07 6.94544788e-01 ... 3.70079659e-04\n",
      "  1.06484703e-03 2.19924240e-03]\n",
      " [8.52826655e-04 3.83641544e-05 5.10115135e-02 ... 9.36064848e-03\n",
      "  2.45807891e-01 9.29729285e-03]\n",
      " ...\n",
      " [7.09677898e-01 2.38198714e-01 7.94982224e-03 ... 4.01720618e-03\n",
      "  2.28459865e-05 1.81462878e-02]\n",
      " [1.50247919e-02 4.74744696e-04 1.34322080e-01 ... 1.42600247e-01\n",
      "  1.91177597e-02 3.28031620e-02]\n",
      " [1.96506653e-01 3.18222414e-01 1.39942628e-02 ... 1.83203770e-02\n",
      "  1.76074300e-03 4.20944897e-01]] & cached\n",
      "Re-used Cached Value, runNum =  407\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  407\n",
      "Provided input from cache for runNum = 407\n",
      "Provided input from cache for runNum = 408\n",
      "activation = [[ 6.07513002  5.8653173   6.44203919 ... 11.82691529  5.06925062\n",
      "   3.63966034]\n",
      " [ 0.          3.42870894  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02164817  3.59684806  0.80875074 ...  2.47903837  5.37273044\n",
      "   1.98249437]\n",
      " ...\n",
      " [ 0.          0.          0.05964999 ...  0.          0.\n",
      "   0.07733781]\n",
      " [ 3.33126406  3.38246748  3.34443956 ...  1.37320464  7.74457897\n",
      "   0.55888098]\n",
      " [ 4.93605699  8.97457731  0.         ...  4.18939549  2.66432739\n",
      "   3.10063123]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30968615 0.         1.64395013 ... 0.         0.97678587 0.        ]\n",
      " [2.17830762 7.60663026 1.8853036  ... 9.0444649  3.42735077 6.74633378]\n",
      " ...\n",
      " [7.61423642 8.26864749 6.11253946 ... 4.99843599 8.82146157 5.39003865]\n",
      " [4.72506607 0.06081947 7.74313415 ... 9.00491977 4.31000377 3.07642477]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.73726568e-08 7.39429486e-08 5.66725326e-05 ... 1.06290588e-01\n",
      "  6.20605007e-05 7.64460767e-03]\n",
      " [6.28696362e-02 7.25589478e-07 6.95701179e-01 ... 3.65420295e-04\n",
      "  1.05353099e-03 2.18670096e-03]\n",
      " [8.51899908e-04 3.82699404e-05 5.09758819e-02 ... 9.35428347e-03\n",
      "  2.44860226e-01 9.25761073e-03]\n",
      " ...\n",
      " [7.10531765e-01 2.38054814e-01 7.91409563e-03 ... 4.02034600e-03\n",
      "  2.24389590e-05 1.80216240e-02]\n",
      " [1.50159751e-02 4.75503974e-04 1.33677008e-01 ... 1.42552414e-01\n",
      "  1.89830716e-02 3.27663068e-02]\n",
      " [1.96127070e-01 3.18877965e-01 1.39221164e-02 ... 1.83745273e-02\n",
      "  1.74662675e-03 4.21059010e-01]] & cached\n",
      "Re-used Cached Value, runNum =  408\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  408\n",
      "Provided input from cache for runNum = 408\n",
      "Provided input from cache for runNum = 409\n",
      "activation = [[ 6.07926702  5.86704822  6.44329471 ... 11.83469324  5.07020415\n",
      "   3.6412552 ]\n",
      " [ 0.          3.42970273  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.02032969  3.59447781  0.80713141 ...  2.48136946  5.37535025\n",
      "   1.98357514]\n",
      " ...\n",
      " [ 0.          0.          0.06164561 ...  0.          0.\n",
      "   0.07923735]\n",
      " [ 3.33494173  3.39101522  3.34979224 ...  1.37792018  7.75305809\n",
      "   0.56345484]\n",
      " [ 4.94104881  8.98309679  0.         ...  4.19704505  2.66721171\n",
      "   3.10596551]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30891107 0.         1.64749216 ... 0.         0.97926636 0.        ]\n",
      " [2.18194352 7.61223779 1.88470336 ... 9.05984625 3.43329111 6.75423206]\n",
      " ...\n",
      " [7.6204192  8.28333754 6.1206889  ... 5.01136966 8.83235392 5.40216426]\n",
      " [4.72851382 0.06161307 7.74830527 ... 9.01298208 4.31220999 3.07730731]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.67569492e-08 7.25478264e-08 5.55686107e-05 ... 1.05910527e-01\n",
      "  6.15926766e-05 7.59545713e-03]\n",
      " [6.24665915e-02 7.17710296e-07 6.96836224e-01 ... 3.60777043e-04\n",
      "  1.04238661e-03 2.17414257e-03]\n",
      " [8.50807662e-04 3.81702323e-05 5.09375994e-02 ... 9.34669971e-03\n",
      "  2.43904621e-01 9.21725345e-03]\n",
      " ...\n",
      " [7.11407234e-01 2.37943789e-01 7.87942197e-03 ... 4.02431283e-03\n",
      "  2.20439964e-05 1.78993648e-02]\n",
      " [1.50065483e-02 4.76267535e-04 1.33047427e-01 ... 1.42512756e-01\n",
      "  1.88508971e-02 3.27307447e-02]\n",
      " [1.95740240e-01 3.19548926e-01 1.38513287e-02 ... 1.84310343e-02\n",
      "  1.73294314e-03 4.21184802e-01]] & cached\n",
      "Re-used Cached Value, runNum =  409\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  409\n",
      "Provided input from cache for runNum = 409\n",
      "Provided input from cache for runNum = 410\n",
      "activation = [[ 6.08342558  5.86882489  6.4445662  ... 11.84248998  5.07117518\n",
      "   3.64287012]\n",
      " [ 0.          3.43068598  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01899146  3.59206454  0.80550941 ...  2.4836602   5.37794154\n",
      "   1.98463319]\n",
      " ...\n",
      " [ 0.          0.          0.06363753 ...  0.          0.\n",
      "   0.08113367]\n",
      " [ 3.33862459  3.39955463  3.35514125 ...  1.38267756  7.7615428\n",
      "   0.56802696]\n",
      " [ 4.94602414  8.99158548  0.         ...  4.20467326  2.67006854\n",
      "   3.1112842 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30802346 0.         1.65091132 ... 0.         0.98164607 0.        ]\n",
      " [2.18555388 7.61779406 1.88408909 ... 9.07516263 3.43920922 6.76210188]\n",
      " ...\n",
      " [7.62660263 8.29803551 6.1288147  ... 5.0243458  8.84324874 5.41427794]\n",
      " [4.73196267 0.06243595 7.75346    ... 9.02104094 4.3144318  3.07819941]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.61522901e-08 7.11810978e-08 5.44866939e-05 ... 1.05516633e-01\n",
      "  6.11346428e-05 7.54654131e-03]\n",
      " [6.20729266e-02 7.10206400e-07 6.97975967e-01 ... 3.56254411e-04\n",
      "  1.03141305e-03 2.16194044e-03]\n",
      " [8.49620591e-04 3.80718011e-05 5.08909426e-02 ... 9.33788978e-03\n",
      "  2.42940702e-01 9.17673049e-03]\n",
      " ...\n",
      " [7.12273592e-01 2.37867784e-01 7.84397878e-03 ... 4.02728707e-03\n",
      "  2.16516302e-05 1.77773498e-02]\n",
      " [1.49986825e-02 4.77145712e-04 1.32422752e-01 ... 1.42476311e-01\n",
      "  1.87203670e-02 3.26973676e-02]\n",
      " [1.95353641e-01 3.20277066e-01 1.37796744e-02 ... 1.84855171e-02\n",
      "  1.71919055e-03 4.21304798e-01]] & cached\n",
      "Re-used Cached Value, runNum =  410\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  410\n",
      "Provided input from cache for runNum = 410\n",
      "Provided input from cache for runNum = 411\n",
      "activation = [[ 6.08754572  5.87053748  6.44581422 ... 11.85020506  5.07207805\n",
      "   3.64445303]\n",
      " [ 0.          3.43167386  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01769145  3.58973184  0.80392328 ...  2.48601487  5.38058188\n",
      "   1.98572015]\n",
      " ...\n",
      " [ 0.          0.          0.06562481 ...  0.          0.\n",
      "   0.08302922]\n",
      " [ 3.34234799  3.4081588   3.36050179 ...  1.38752978  7.77009349\n",
      "   0.57262038]\n",
      " [ 4.95099506  9.00006951  0.         ...  4.2123225   2.67293093\n",
      "   3.1166012 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30708023 0.         1.6542674  ... 0.         0.9840011  0.        ]\n",
      " [2.18902904 7.62316035 1.88337796 ... 9.09023433 3.44491846 6.76985743]\n",
      " ...\n",
      " [7.63273545 8.31265018 6.13688541 ... 5.03726919 8.85409558 5.42633225]\n",
      " [4.73535891 0.06315981 7.75857239 ... 9.02898577 4.31657478 3.07904927]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.55687612e-08 6.98527281e-08 5.34397966e-05 ... 1.05150708e-01\n",
      "  6.06739100e-05 7.49910414e-03]\n",
      " [6.16766065e-02 7.02573145e-07 6.99086709e-01 ... 3.51810743e-04\n",
      "  1.02067118e-03 2.14983870e-03]\n",
      " [8.48821369e-04 3.79896488e-05 5.08616166e-02 ... 9.33466506e-03\n",
      "  2.42003307e-01 9.13895888e-03]\n",
      " ...\n",
      " [7.13156041e-01 2.37728129e-01 7.80902247e-03 ... 4.03060265e-03\n",
      "  2.12694139e-05 1.76566379e-02]\n",
      " [1.49927605e-02 4.77987222e-04 1.31813801e-01 ... 1.42459976e-01\n",
      "  1.85910741e-02 3.26657740e-02]\n",
      " [1.94946264e-01 3.20886676e-01 1.37075757e-02 ... 1.85393368e-02\n",
      "  1.70552499e-03 4.21395579e-01]] & cached\n",
      "Re-used Cached Value, runNum =  411\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  411\n",
      "Provided input from cache for runNum = 411\n",
      "Provided input from cache for runNum = 412\n",
      "activation = [[ 6.0916643   5.87225309  6.44706073 ... 11.8579069   5.07297852\n",
      "   3.64603602]\n",
      " [ 0.          3.4326344   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01636108  3.58736855  0.80232258 ...  2.48832664  5.38317569\n",
      "   1.98678548]\n",
      " ...\n",
      " [ 0.          0.          0.06761496 ...  0.          0.\n",
      "   0.08492943]\n",
      " [ 3.34600956  3.41667056  3.36581209 ...  1.39230217  7.77856772\n",
      "   0.57716095]\n",
      " [ 4.95595012  9.00852831  0.         ...  4.21995584  2.67579295\n",
      "   3.1219048 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30608681 0.         1.65757238 ... 0.         0.98630572 0.        ]\n",
      " [2.19261302 7.62864272 1.88275469 ... 9.10543567 3.45074345 6.77767041]\n",
      " ...\n",
      " [7.6387895  8.32715603 6.14488666 ... 5.05014234 8.8648726  5.43832203]\n",
      " [4.73866955 0.06379006 7.76359862 ... 9.0368064  4.31862816 3.07984864]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.49889991e-08 6.85455967e-08 5.24124565e-05 ... 1.04764526e-01\n",
      "  6.02294862e-05 7.45127402e-03]\n",
      " [6.12751047e-02 6.95099896e-07 7.00207785e-01 ... 3.47423233e-04\n",
      "  1.01004413e-03 2.13776796e-03]\n",
      " [8.47538656e-04 3.78914541e-05 5.08140871e-02 ... 9.32625425e-03\n",
      "  2.41036472e-01 9.09843010e-03]\n",
      " ...\n",
      " [7.14032916e-01 2.37629549e-01 7.77507543e-03 ... 4.03397786e-03\n",
      "  2.08966220e-05 1.75367848e-02]\n",
      " [1.49828862e-02 4.78767568e-04 1.31195923e-01 ... 1.42409096e-01\n",
      "  1.84633021e-02 3.26292767e-02]\n",
      " [1.94558066e-01 3.21596728e-01 1.36383040e-02 ... 1.85965176e-02\n",
      "  1.69234677e-03 4.21516412e-01]] & cached\n",
      "Re-used Cached Value, runNum =  412\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  412\n",
      "Provided input from cache for runNum = 412\n",
      "Provided input from cache for runNum = 413\n",
      "activation = [[ 6.09576964  5.87396153  6.44830248 ... 11.86558042  5.07385716\n",
      "   3.64761233]\n",
      " [ 0.          3.4335816   0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.01503094  3.5850012   0.80073145 ...  2.49062106  5.38575104\n",
      "   1.98784109]\n",
      " ...\n",
      " [ 0.          0.          0.06960622 ...  0.          0.\n",
      "   0.08683027]\n",
      " [ 3.34964595  3.42511728  3.37109899 ...  1.39703879  7.78700934\n",
      "   0.58167247]\n",
      " [ 4.96086604  9.01693476  0.         ...  4.22755222  2.67861022\n",
      "   3.12718209]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3052049  0.         1.66095263 ... 0.         0.98874723 0.        ]\n",
      " [2.19613553 7.63403558 1.88209666 ... 9.12051129 3.45650999 6.7854252 ]\n",
      " ...\n",
      " [7.64483477 8.34162067 6.15289102 ... 5.06302672 8.87566739 5.45028675]\n",
      " [4.74199457 0.06443999 7.76862332 ... 9.04461523 4.32070945 3.08066064]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.44155855e-08 6.72565819e-08 5.13997122e-05 ... 1.04377497e-01\n",
      "  5.97804417e-05 7.40389185e-03]\n",
      " [6.08798522e-02 6.87757700e-07 7.01335248e-01 ... 3.43148609e-04\n",
      "  9.99596905e-04 2.12598541e-03]\n",
      " [8.46319771e-04 3.77943389e-05 5.07682714e-02 ... 9.31958910e-03\n",
      "  2.40089717e-01 9.05882507e-03]\n",
      " ...\n",
      " [7.14921182e-01 2.37553192e-01 7.74116253e-03 ... 4.03786229e-03\n",
      "  2.05294578e-05 1.74191441e-02]\n",
      " [1.49734748e-02 4.79575297e-04 1.30580872e-01 ... 1.42384380e-01\n",
      "  1.83358768e-02 3.25960020e-02]\n",
      " [1.94151979e-01 3.22281356e-01 1.35681653e-02 ... 1.86538723e-02\n",
      "  1.67912312e-03 4.21629322e-01]] & cached\n",
      "Re-used Cached Value, runNum =  413\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  413\n",
      "Provided input from cache for runNum = 413\n",
      "Provided input from cache for runNum = 414\n",
      "activation = [[6.09987927e+00 5.87567636e+00 6.44954996e+00 ... 1.18732504e+01\n",
      "  5.07473956e+00 3.64918932e+00]\n",
      " [0.00000000e+00 3.43455987e+00 0.00000000e+00 ... 5.29905289e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.36682739e-02 3.58260882e+00 7.99119092e-01 ... 2.49287412e+00\n",
      "  5.38827964e+00 1.98887442e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.16042979e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 8.87499565e-02]\n",
      " [3.35328039e+00 3.43356762e+00 3.37636407e+00 ... 1.40178108e+00\n",
      "  7.79543664e+00 5.86176550e-01]\n",
      " [4.96574177e+00 9.02528278e+00 0.00000000e+00 ... 4.23510678e+00\n",
      "  2.68138510e+00 3.13243066e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30427097 0.         1.66427869 ... 0.         0.99118071 0.        ]\n",
      " [2.1996566  7.63941429 1.88145801 ... 9.13568175 3.46224612 6.7931444 ]\n",
      " ...\n",
      " [7.65081326 8.35597705 6.16082783 ... 5.0757457  8.88638874 5.46218781]\n",
      " [4.74533192 0.06509578 7.77363288 ... 9.05222665 4.32281999 3.08148752]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.38605909e-08 6.60143854e-08 5.04159233e-05 ... 1.04013279e-01\n",
      "  5.93437572e-05 7.35750337e-03]\n",
      " [6.04888842e-02 6.80582621e-07 7.02444410e-01 ... 3.38830841e-04\n",
      "  9.89339445e-04 2.11442651e-03]\n",
      " [8.45151057e-04 3.77034238e-05 5.07211603e-02 ... 9.31202125e-03\n",
      "  2.39141917e-01 9.01932525e-03]\n",
      " ...\n",
      " [7.15799748e-01 2.37477559e-01 7.70729181e-03 ... 4.04186541e-03\n",
      "  2.01669290e-05 1.73017829e-02]\n",
      " [1.49658971e-02 4.80472276e-04 1.29977193e-01 ... 1.42356688e-01\n",
      "  1.82094630e-02 3.25641876e-02]\n",
      " [1.93749245e-01 3.22984701e-01 1.34983362e-02 ... 1.87117506e-02\n",
      "  1.66591962e-03 4.21729415e-01]] & cached\n",
      "Re-used Cached Value, runNum =  414\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  414\n",
      "Provided input from cache for runNum = 414\n",
      "Provided input from cache for runNum = 415\n",
      "activation = [[6.10397200e+00 5.87737102e+00 6.45078507e+00 ... 1.18808715e+01\n",
      "  5.07559092e+00 3.65075097e+00]\n",
      " [0.00000000e+00 3.43553422e+00 0.00000000e+00 ... 1.52915900e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.23217423e-02 3.58024669e+00 7.97527778e-01 ... 2.49516058e+00\n",
      "  5.39082164e+00 1.98992423e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.36121180e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.06865503e-02]\n",
      " [3.35690664e+00 3.44200506e+00 3.38161706e+00 ... 1.40653929e+00\n",
      "  7.80385889e+00 5.90678501e-01]\n",
      " [4.97062901e+00 9.03364963e+00 0.00000000e+00 ... 4.24271236e+00\n",
      "  2.68418828e+00 3.13769011e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.3033218  0.         1.66759038 ... 0.         0.99362673 0.        ]\n",
      " [2.20314747 7.64472699 1.8807997  ... 9.15093908 3.46795147 6.80083632]\n",
      " ...\n",
      " [7.65676827 8.37029067 6.16873601 ... 5.08837851 8.89709358 5.47406213]\n",
      " [4.74864543 0.06571755 7.77862464 ... 9.05957746 4.32488671 3.08229609]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.33121885e-08 6.47813428e-08 4.94545288e-05 ... 1.03672599e-01\n",
      "  5.89077123e-05 7.31157636e-03]\n",
      " [6.00919340e-02 6.73343549e-07 7.03550817e-01 ... 3.34486122e-04\n",
      "  9.79186316e-04 2.10290282e-03]\n",
      " [8.43955639e-04 3.76092844e-05 5.06783995e-02 ... 9.30640267e-03\n",
      "  2.38202651e-01 8.98052376e-03]\n",
      " ...\n",
      " [7.16698251e-01 2.37394249e-01 7.67442857e-03 ... 4.04738949e-03\n",
      "  1.98141614e-05 1.71860602e-02]\n",
      " [1.49560330e-02 4.81254028e-04 1.29372023e-01 ... 1.42325958e-01\n",
      "  1.80832980e-02 3.25307775e-02]\n",
      " [1.93333475e-01 3.23636452e-01 1.34294036e-02 ... 1.87745928e-02\n",
      "  1.65293771e-03 4.21825608e-01]] & cached\n",
      "Re-used Cached Value, runNum =  415\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  415\n",
      "Provided input from cache for runNum = 415\n",
      "Provided input from cache for runNum = 416\n",
      "activation = [[6.10802872e+00 5.87901706e+00 6.45200489e+00 ... 1.18884226e+01\n",
      "  5.07638822e+00 3.65228416e+00]\n",
      " [0.00000000e+00 3.43651657e+00 0.00000000e+00 ... 2.53731147e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.09686668e-02 3.57789404e+00 7.95922040e-01 ... 2.49746858e+00\n",
      "  5.39336605e+00 1.99097376e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.56168948e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.26252516e-02]\n",
      " [3.36050817e+00 3.45037554e+00 3.38684139e+00 ... 1.41126643e+00\n",
      "  7.81224194e+00 5.95147672e-01]\n",
      " [4.97551748e+00 9.04203233e+00 0.00000000e+00 ... 4.25036301e+00\n",
      "  2.68700223e+00 3.14295717e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30239087 0.         1.67090151 ... 0.         0.99608708 0.        ]\n",
      " [2.20668703 7.65011642 1.88019348 ... 9.16621337 3.47368654 6.80853543]\n",
      " ...\n",
      " [7.66263728 8.38446302 6.17657073 ... 5.10087651 8.90770094 5.48585528]\n",
      " [4.75182115 0.06612869 7.78352074 ... 9.06669557 4.32679599 3.08300824]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.27834811e-08 6.35903593e-08 4.85220182e-05 ... 1.03345143e-01\n",
      "  5.84818889e-05 7.26642070e-03]\n",
      " [5.97025607e-02 6.66167091e-07 7.04648694e-01 ... 3.30168509e-04\n",
      "  9.69161573e-04 2.09140926e-03]\n",
      " [8.42779778e-04 3.75124054e-05 5.06319051e-02 ... 9.29875102e-03\n",
      "  2.37229440e-01 8.94069587e-03]\n",
      " ...\n",
      " [7.17552588e-01 2.37276338e-01 7.64062002e-03 ... 4.05205448e-03\n",
      "  1.94669544e-05 1.70691784e-02]\n",
      " [1.49466775e-02 4.81993706e-04 1.28765730e-01 ... 1.42267858e-01\n",
      "  1.79571968e-02 3.24939609e-02]\n",
      " [1.92952305e-01 3.24332635e-01 1.33613197e-02 ... 1.88377118e-02\n",
      "  1.64022126e-03 4.21922583e-01]] & cached\n",
      "Re-used Cached Value, runNum =  416\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  416\n",
      "Provided input from cache for runNum = 416\n",
      "Provided input from cache for runNum = 417\n",
      "activation = [[6.11206525e+00 5.88065083e+00 6.45321704e+00 ... 1.18959413e+01\n",
      "  5.07716409e+00 3.65380791e+00]\n",
      " [0.00000000e+00 3.43751157e+00 0.00000000e+00 ... 3.55738440e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.63650166e-03 3.57557294e+00 7.94327107e-01 ... 2.49980667e+00\n",
      "  5.39592957e+00 1.99203270e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.76150575e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.45620642e-02]\n",
      " [3.36408264e+00 3.45868628e+00 3.39203628e+00 ... 1.41595421e+00\n",
      "  7.82059166e+00 5.99584243e-01]\n",
      " [4.98043067e+00 9.05045314e+00 0.00000000e+00 ... 4.25808092e+00\n",
      "  2.68987542e+00 3.14824179e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30135368 0.         1.67413527 ... 0.         0.99842991 0.        ]\n",
      " [2.21019342 7.65546157 1.87955623 ... 9.1814302  3.47938551 6.81619742]\n",
      " ...\n",
      " [7.66839083 8.39845168 6.18431057 ... 5.11321701 8.91817152 5.49754817]\n",
      " [4.75492005 0.06643281 7.78837681 ... 9.07368309 4.32861017 3.08367409]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.22706016e-08 6.24359393e-08 4.76148427e-05 ... 1.03034604e-01\n",
      "  5.80735210e-05 7.22243247e-03]\n",
      " [5.93139365e-02 6.59085040e-07 7.05737227e-01 ... 3.25905962e-04\n",
      "  9.59334086e-04 2.08007397e-03]\n",
      " [8.41607130e-04 3.74176525e-05 5.05842248e-02 ... 9.29104432e-03\n",
      "  2.36255539e-01 8.90133384e-03]\n",
      " ...\n",
      " [7.18417235e-01 2.37163547e-01 7.60658613e-03 ... 4.05650494e-03\n",
      "  1.91286280e-05 1.69536015e-02]\n",
      " [1.49387563e-02 4.82756519e-04 1.28171515e-01 ... 1.42213544e-01\n",
      "  1.78345633e-02 3.24590648e-02]\n",
      " [1.92558303e-01 3.25013536e-01 1.32922634e-02 ... 1.88993322e-02\n",
      "  1.62773987e-03 4.22005417e-01]] & cached\n",
      "Re-used Cached Value, runNum =  417\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  417\n",
      "Provided input from cache for runNum = 417\n",
      "Provided input from cache for runNum = 418\n",
      "activation = [[6.11608250e+00 5.88226155e+00 6.45441607e+00 ... 1.19034174e+01\n",
      "  5.07790182e+00 3.65531767e+00]\n",
      " [0.00000000e+00 3.43849883e+00 0.00000000e+00 ... 4.57252658e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.32964784e-03 3.57329627e+00 7.92758001e-01 ... 2.50218834e+00\n",
      "  5.39852515e+00 1.99310904e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 7.96115682e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.64986503e-02]\n",
      " [3.36766746e+00 3.46701444e+00 3.39721672e+00 ... 1.42069202e+00\n",
      "  7.82895193e+00 6.04026210e-01]\n",
      " [4.98531848e+00 9.05883008e+00 0.00000000e+00 ... 4.26576872e+00\n",
      "  2.69271990e+00 3.15350424e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.30037419 0.         1.67740484 ... 0.         1.00085291 0.        ]\n",
      " [2.21369012 7.66076533 1.87893261 ... 9.19656631 3.48505894 6.823824  ]\n",
      " ...\n",
      " [7.67409792 8.41236435 6.19200529 ... 5.12552404 8.92859456 5.50919983]\n",
      " [4.75799859 0.06671373 7.79319886 ... 9.08062116 4.33039587 3.08432946]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.17681594e-08 6.13094462e-08 4.67249728e-05 ... 1.02727215e-01\n",
      "  5.76679661e-05 7.17899751e-03]\n",
      " [5.89344208e-02 6.52158788e-07 7.06840277e-01 ... 3.21740749e-04\n",
      "  9.49693907e-04 2.06898153e-03]\n",
      " [8.40476888e-04 3.73256488e-05 5.05343049e-02 ... 9.28380904e-03\n",
      "  2.35287899e-01 8.86235031e-03]\n",
      " ...\n",
      " [7.19269740e-01 2.37039061e-01 7.57210853e-03 ... 4.06039745e-03\n",
      "  1.87963508e-05 1.68380778e-02]\n",
      " [1.49302497e-02 4.83500788e-04 1.27565800e-01 ... 1.42148919e-01\n",
      "  1.77126280e-02 3.24233143e-02]\n",
      " [1.92167239e-01 3.25691203e-01 1.32230002e-02 ... 1.89599359e-02\n",
      "  1.61537562e-03 4.22075782e-01]] & cached\n",
      "Re-used Cached Value, runNum =  418\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  418\n",
      "Provided input from cache for runNum = 418\n",
      "Provided input from cache for runNum = 419\n",
      "activation = [[6.12010383e+00 5.88388455e+00 6.45561777e+00 ... 1.19108865e+01\n",
      "  5.07863869e+00 3.65683084e+00]\n",
      " [0.00000000e+00 3.43948511e+00 0.00000000e+00 ... 5.58857746e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.02167221e-03 3.57102352e+00 7.91198134e-01 ... 2.50457697e+00\n",
      "  5.40111427e+00 1.99418597e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.16128088e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.84458402e-02]\n",
      " [3.37125257e+00 3.47532374e+00 3.40238560e+00 ... 1.42542810e+00\n",
      "  7.83730046e+00 6.08457044e-01]\n",
      " [4.99019628e+00 9.06718701e+00 0.00000000e+00 ... 4.27346485e+00\n",
      "  2.69554986e+00 3.15876274e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29933779 0.         1.68060612 ... 0.         1.00321705 0.        ]\n",
      " [2.21714744 7.66601291 1.87828644 ... 9.21164811 3.49070217 6.83142345]\n",
      " ...\n",
      " [7.67976737 8.42620299 6.1996622  ... 5.13778361 8.93897011 5.52080696]\n",
      " [4.76105423 0.06696582 7.79799122 ... 9.08748339 4.33215541 3.08496336]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.12688354e-08 6.01983062e-08 4.58523620e-05 ... 1.02416206e-01\n",
      "  5.72707289e-05 7.13575554e-03]\n",
      " [5.85483794e-02 6.45314210e-07 7.07927338e-01 ... 3.17636459e-04\n",
      "  9.40203581e-04 2.05798466e-03]\n",
      " [8.39194576e-04 3.72315066e-05 5.04823883e-02 ... 9.27672069e-03\n",
      "  2.34326025e-01 8.82342958e-03]\n",
      " ...\n",
      " [7.20149322e-01 2.36953931e-01 7.53876763e-03 ... 4.06514072e-03\n",
      "  1.84716586e-05 1.67249169e-02]\n",
      " [1.49218762e-02 4.84297232e-04 1.26978100e-01 ... 1.42109554e-01\n",
      "  1.75936041e-02 3.23905958e-02]\n",
      " [1.91757583e-01 3.26377245e-01 1.31544697e-02 ... 1.90223025e-02\n",
      "  1.60319476e-03 4.22152739e-01]] & cached\n",
      "Re-used Cached Value, runNum =  419\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  419\n",
      "Provided input from cache for runNum = 419\n",
      "Provided input from cache for runNum = 420\n",
      "activation = [[6.12410573e+00 5.88548834e+00 6.45681024e+00 ... 1.19183211e+01\n",
      "  5.07935105e+00 3.65833437e+00]\n",
      " [0.00000000e+00 3.44047730e+00 0.00000000e+00 ... 6.61041391e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [5.71814320e-03 3.56876272e+00 7.89645472e-01 ... 2.50697574e+00\n",
      "  5.40371169e+00 1.99526245e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.36140856e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.00399394e-01]\n",
      " [3.37482570e+00 3.48359275e+00 3.40752917e+00 ... 1.43014067e+00\n",
      "  7.84562918e+00 6.12865429e-01]\n",
      " [4.99505833e+00 9.07551551e+00 0.00000000e+00 ... 4.28114391e+00\n",
      "  2.69836551e+00 3.16400806e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.2983497  0.         1.6838243  ... 0.         1.00558096 0.        ]\n",
      " [2.22059936 7.67125892 1.87765028 ... 9.22670133 3.49633741 6.83900535]\n",
      " ...\n",
      " [7.68539217 8.43996234 6.20726859 ... 5.14997941 8.94929749 5.53236401]\n",
      " [4.76408779 0.06719664 7.80275497 ... 9.09430073 4.33390488 3.08559138]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.07800663e-08 5.91128216e-08 4.49976227e-05 ... 1.02106184e-01\n",
      "  5.68805040e-05 7.09294050e-03]\n",
      " [5.81702372e-02 6.38578605e-07 7.09022863e-01 ... 3.13606795e-04\n",
      "  9.30857640e-04 2.04714764e-03]\n",
      " [8.37947866e-04 3.71374809e-05 5.04281128e-02 ... 9.26897299e-03\n",
      "  2.33368533e-01 8.78445377e-03]\n",
      " ...\n",
      " [7.21010344e-01 2.36851212e-01 7.50511703e-03 ... 4.06950178e-03\n",
      "  1.81529664e-05 1.66119315e-02]\n",
      " [1.49128016e-02 4.85049940e-04 1.26381222e-01 ... 1.42051745e-01\n",
      "  1.74754688e-02 3.23560369e-02]\n",
      " [1.91358211e-01 3.27067174e-01 1.30861564e-02 ... 1.90844724e-02\n",
      "  1.59115580e-03 4.22223225e-01]] & cached\n",
      "Re-used Cached Value, runNum =  420\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  420\n",
      "Provided input from cache for runNum = 420\n",
      "Provided input from cache for runNum = 421\n",
      "activation = [[6.12810115e+00 5.88709077e+00 6.45799428e+00 ... 1.19257357e+01\n",
      "  5.08004769e+00 3.65983534e+00]\n",
      " [0.00000000e+00 3.44145297e+00 0.00000000e+00 ... 7.62153040e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.39573146e-03 3.56648283e+00 7.88091486e-01 ... 2.50935466e+00\n",
      "  5.40629434e+00 1.99632493e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.56169076e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.02353714e-01]\n",
      " [3.37838387e+00 3.49184289e+00 3.41265698e+00 ... 1.43483894e+00\n",
      "  7.85393871e+00 6.17255811e-01]\n",
      " [4.99990190e+00 9.08381484e+00 0.00000000e+00 ... 4.28882510e+00\n",
      "  2.70116917e+00 3.16923951e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29745585 0.         1.68710137 ... 0.         1.00801186 0.        ]\n",
      " [2.22403423 7.67646589 1.87700317 ... 9.24170031 3.50194253 6.84656349]\n",
      " ...\n",
      " [7.69099667 8.45370003 6.21484329 ... 5.16215296 8.95959255 5.54389348]\n",
      " [4.76711114 0.06742993 7.80749073 ... 9.10107224 4.33563772 3.08621709]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[3.02953398e-08 5.80421631e-08 4.41577326e-05 ... 1.01792537e-01\n",
      "  5.64920100e-05 7.05030792e-03]\n",
      " [5.77915534e-02 6.31917550e-07 7.10111086e-01 ... 3.09641422e-04\n",
      "  9.21652645e-04 2.03641756e-03]\n",
      " [8.36658899e-04 3.70428561e-05 5.03739437e-02 ... 9.26155493e-03\n",
      "  2.32418482e-01 8.74563336e-03]\n",
      " ...\n",
      " [7.21880024e-01 2.36766077e-01 7.47204058e-03 ... 4.07459171e-03\n",
      "  1.78406467e-05 1.65008056e-02]\n",
      " [1.49036971e-02 4.85836207e-04 1.25792549e-01 ... 1.42014815e-01\n",
      "  1.73585336e-02 3.23236846e-02]\n",
      " [1.90951695e-01 3.27757879e-01 1.30183404e-02 ... 1.91483116e-02\n",
      "  1.57924289e-03 4.22297168e-01]] & cached\n",
      "Re-used Cached Value, runNum =  421\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  421\n",
      "Provided input from cache for runNum = 421\n",
      "Provided input from cache for runNum = 422\n",
      "activation = [[6.13207281e+00 5.88866214e+00 6.45916141e+00 ... 1.19330937e+01\n",
      "  5.08070477e+00 3.66131761e+00]\n",
      " [0.00000000e+00 3.44242496e+00 0.00000000e+00 ... 8.62687146e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.03756574e-03 3.56416803e+00 7.86510551e-01 ... 2.51171462e+00\n",
      "  5.40885470e+00 1.99736929e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.76141260e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.04303359e-01]\n",
      " [3.38196508e+00 3.50012464e+00 3.41779143e+00 ... 1.43959323e+00\n",
      "  7.86226436e+00 6.21662923e-01]\n",
      " [5.00470534e+00 9.09205074e+00 0.00000000e+00 ... 4.29646389e+00\n",
      "  2.70394385e+00 3.17443919e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29661295 0.         1.69040112 ... 0.         1.01048314 0.        ]\n",
      " [2.2274017  7.68155381 1.87632085 ... 9.25658535 3.50747704 6.85406254]\n",
      " ...\n",
      " [7.69657896 8.46739792 6.22238597 ... 5.17428781 8.96984666 5.55538841]\n",
      " [4.77012323 0.06765447 7.81219053 ... 9.10779295 4.33733318 3.08683304]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.98255575e-08 5.70006599e-08 4.33367766e-05 ... 1.01491052e-01\n",
      "  5.61059023e-05 7.00860621e-03]\n",
      " [5.74290282e-02 6.25465391e-07 7.11207170e-01 ... 3.05750985e-04\n",
      "  9.12614215e-04 2.02590859e-03]\n",
      " [8.35560342e-04 3.69549433e-05 5.03202571e-02 ... 9.25467873e-03\n",
      "  2.31463608e-01 8.70750452e-03]\n",
      " ...\n",
      " [7.22713382e-01 2.36646802e-01 7.43765047e-03 ... 4.07819992e-03\n",
      "  1.75318697e-05 1.63883002e-02]\n",
      " [1.48960079e-02 4.86633991e-04 1.25195563e-01 ... 1.41960046e-01\n",
      "  1.72416769e-02 3.22905877e-02]\n",
      " [1.90561474e-01 3.28446359e-01 1.29497379e-02 ... 1.92090150e-02\n",
      "  1.56734425e-03 4.22346619e-01]] & cached\n",
      "Re-used Cached Value, runNum =  422\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  422\n",
      "Provided input from cache for runNum = 422\n",
      "Provided input from cache for runNum = 423\n",
      "activation = [[6.13603031e+00 5.89023547e+00 6.46031719e+00 ... 1.19404327e+01\n",
      "  5.08135580e+00 3.66279711e+00]\n",
      " [0.00000000e+00 3.44341093e+00 0.00000000e+00 ... 9.64841659e-03\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.68331859e-03 3.56185635e+00 7.84945411e-01 ... 2.51407573e+00\n",
      "  5.41140871e+00 1.99840858e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 8.96030853e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.06246548e-01]\n",
      " [3.38555847e+00 3.50840822e+00 3.42291788e+00 ... 1.44435263e+00\n",
      "  7.87057385e+00 6.26067185e-01]\n",
      " [5.00949319e+00 9.10026146e+00 0.00000000e+00 ... 4.30409887e+00\n",
      "  2.70670461e+00 3.17962805e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29584661 0.         1.69374754 ... 0.         1.01302937 0.        ]\n",
      " [2.23076069 7.68661291 1.87564558 ... 9.27142237 3.51299951 6.8615313 ]\n",
      " ...\n",
      " [7.70210001 8.48098817 6.22985876 ... 5.18631754 8.98001031 5.56681719]\n",
      " [4.77312267 0.06788632 7.816852   ... 9.11448371 4.33902272 3.0874556 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.93625062e-08 5.59821525e-08 4.25342949e-05 ... 1.01196413e-01\n",
      "  5.57280107e-05 6.96756908e-03]\n",
      " [5.70643710e-02 6.19053208e-07 7.12288635e-01 ... 3.01897203e-04\n",
      "  9.03743375e-04 2.01547052e-03]\n",
      " [8.34393834e-04 3.68664130e-05 5.02653860e-02 ... 9.24755324e-03\n",
      "  2.30514792e-01 8.66948475e-03]\n",
      " ...\n",
      " [7.23556128e-01 2.36547669e-01 7.40403456e-03 ... 4.08229394e-03\n",
      "  1.72305255e-05 1.62774159e-02]\n",
      " [1.48883433e-02 4.87468503e-04 1.24609459e-01 ... 1.41919591e-01\n",
      "  1.71269389e-02 3.22594264e-02]\n",
      " [1.90164791e-01 3.29132029e-01 1.28820393e-02 ... 1.92700244e-02\n",
      "  1.55565685e-03 4.22392062e-01]] & cached\n",
      "Re-used Cached Value, runNum =  423\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  423\n",
      "Provided input from cache for runNum = 423\n",
      "Provided input from cache for runNum = 424\n",
      "activation = [[6.13997182e+00 5.89179702e+00 6.46146640e+00 ... 1.19477422e+01\n",
      "  5.08197844e+00 3.66426889e+00]\n",
      " [0.00000000e+00 3.44440001e+00 0.00000000e+00 ... 1.06723561e-02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.23703455e-04 3.55953712e+00 7.83388889e-01 ... 2.51642371e+00\n",
      "  5.41395163e+00 1.99944036e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 9.15889222e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.08192354e-01]\n",
      " [3.38910000e+00 3.51658717e+00 3.42800458e+00 ... 1.44903588e+00\n",
      "  7.87881638e+00 6.30421970e-01]\n",
      " [5.01425765e+00 9.10843533e+00 0.00000000e+00 ... 4.31171528e+00\n",
      "  2.70943704e+00 3.18479813e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29499361 0.         1.69699486 ... 0.         1.01553085 0.        ]\n",
      " [2.23416051 7.69172134 1.87501426 ... 9.28629092 3.51855545 6.86901457]\n",
      " ...\n",
      " [7.70756068 8.4944744  6.23727609 ... 5.19827556 8.99010915 5.5781889 ]\n",
      " [4.77602464 0.06799451 7.8214325  ... 9.12105143 4.34061911 3.08802829]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.89070081e-08 5.49804923e-08 4.17539884e-05 ... 1.00899364e-01\n",
      "  5.53596792e-05 6.92668906e-03]\n",
      " [5.66938348e-02 6.12681579e-07 7.13348427e-01 ... 2.98076902e-04\n",
      "  8.94961045e-04 2.00505356e-03]\n",
      " [8.33041017e-04 3.67699103e-05 5.02075368e-02 ... 9.23861637e-03\n",
      "  2.29551487e-01 8.63055728e-03]\n",
      " ...\n",
      " [7.24399796e-01 2.36467957e-01 7.37114883e-03 ... 4.08644138e-03\n",
      "  1.69353784e-05 1.61674212e-02]\n",
      " [1.48799912e-02 4.88283839e-04 1.24035139e-01 ... 1.41873418e-01\n",
      "  1.70134756e-02 3.22273668e-02]\n",
      " [1.89774737e-01 3.29856752e-01 1.28161131e-02 ... 1.93323935e-02\n",
      "  1.54421787e-03 4.22446768e-01]] & cached\n",
      "Re-used Cached Value, runNum =  424\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  424\n",
      "Provided input from cache for runNum = 424\n",
      "Provided input from cache for runNum = 425\n",
      "activation = [[6.14389945e+00 5.89335749e+00 6.46260952e+00 ... 1.19550266e+01\n",
      "  5.08258353e+00 3.66573280e+00]\n",
      " [0.00000000e+00 3.44540446e+00 0.00000000e+00 ... 1.17112724e-02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 3.55723084e+00 7.81832011e-01 ... 2.51878350e+00\n",
      "  5.41649548e+00 2.00047379e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 9.35743215e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.10138428e-01]\n",
      " [3.39262723e+00 3.52475390e+00 3.43307313e+00 ... 1.45371787e+00\n",
      "  7.88705223e+00 6.34766948e-01]\n",
      " [5.01899892e+00 9.11658919e+00 0.00000000e+00 ... 4.31934169e+00\n",
      "  2.71215180e+00 3.18995964e+00]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29382351 0.         1.70018057 ... 0.         1.01794905 0.        ]\n",
      " [2.23776271 7.69672451 1.87434366 ... 9.30104098 3.52402566 6.87643054]\n",
      " ...\n",
      " [7.71269192 8.50784368 6.24464001 ... 5.21013749 9.00013475 5.58949361]\n",
      " [4.77870259 0.06806441 7.82599123 ... 9.12754791 4.34219535 3.08858336]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.84845759e-08 5.40053716e-08 4.09927020e-05 ... 1.00613273e-01\n",
      "  5.50000877e-05 6.88663101e-03]\n",
      " [5.62953022e-02 6.06395366e-07 7.14400776e-01 ... 2.94318969e-04\n",
      "  8.86349872e-04 1.99481360e-03]\n",
      " [8.31805878e-04 3.66796130e-05 5.01521308e-02 ... 9.23096610e-03\n",
      "  2.28598676e-01 8.59260390e-03]\n",
      " ...\n",
      " [7.25265734e-01 2.36378367e-01 7.33814507e-03 ... 4.09055821e-03\n",
      "  1.66458028e-05 1.60584215e-02]\n",
      " [1.48720230e-02 4.89128925e-04 1.23467352e-01 ... 1.41836388e-01\n",
      "  1.69019135e-02 3.21969621e-02]\n",
      " [1.89384245e-01 3.30549848e-01 1.27496738e-02 ... 1.93936565e-02\n",
      "  1.53283747e-03 4.22484994e-01]] & cached\n",
      "Re-used Cached Value, runNum =  425\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  425\n",
      "Provided input from cache for runNum = 425\n",
      "Provided input from cache for runNum = 426\n",
      "activation = [[ 6.14779435  5.8948784   6.46373412 ... 11.96224739  5.08314171\n",
      "   3.66717474]\n",
      " [ 0.          3.44639692  0.         ...  0.01273786  0.\n",
      "   0.        ]\n",
      " [ 0.          3.55497273  0.78031325 ...  2.52120149  5.41907956\n",
      "   2.00153072]\n",
      " ...\n",
      " [ 0.          0.          0.09556587 ...  0.          0.\n",
      "   0.11209477]\n",
      " [ 3.39612673  3.53286433  3.43811317 ...  1.45835527  7.89525077\n",
      "   0.6390807 ]\n",
      " [ 5.02375582  9.12476639  0.         ...  4.32702656  2.71489963\n",
      "   3.19513783]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29259846 0.         1.70337586 ... 0.         1.02039575 0.        ]\n",
      " [2.24146596 7.70175151 1.87369837 ... 9.31580504 3.52951425 6.88384953]\n",
      " ...\n",
      " [7.71769984 8.52113036 6.2519494  ... 5.22194048 9.01010566 5.6007461 ]\n",
      " [4.7812295  0.06800969 7.83048408 ... 9.13388096 4.34365798 3.08907499]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.80725869e-08 5.30396579e-08 4.02483588e-05 ... 1.00327740e-01\n",
      "  5.46431575e-05 6.84653301e-03]\n",
      " [5.58840746e-02 6.00057688e-07 7.15452818e-01 ... 2.90609409e-04\n",
      "  8.77820906e-04 1.98454415e-03]\n",
      " [8.30479558e-04 3.65814165e-05 5.00970978e-02 ... 9.22336414e-03\n",
      "  2.27642915e-01 8.55419741e-03]\n",
      " ...\n",
      " [7.26137746e-01 2.36269146e-01 7.30596815e-03 ... 4.09571010e-03\n",
      "  1.63643006e-05 1.59507304e-02]\n",
      " [1.48607328e-02 4.89812582e-04 1.22893262e-01 ... 1.41783200e-01\n",
      "  1.67907505e-02 3.21622265e-02]\n",
      " [1.89001472e-01 3.31228741e-01 1.26848440e-02 ... 1.94594379e-02\n",
      "  1.52177960e-03 4.22533741e-01]] & cached\n",
      "Re-used Cached Value, runNum =  426\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  426\n",
      "Provided input from cache for runNum = 426\n",
      "Provided input from cache for runNum = 427\n",
      "activation = [[ 6.15169487  5.89641407  6.4648654  ... 11.96946408  5.08370359\n",
      "   3.66862119]\n",
      " [ 0.          3.44739826  0.         ...  0.0137712   0.\n",
      "   0.        ]\n",
      " [ 0.          3.55273907  0.77881894 ...  2.5236278   5.421676\n",
      "   2.00259044]\n",
      " ...\n",
      " [ 0.          0.          0.09755749 ...  0.          0.\n",
      "   0.11405699]\n",
      " [ 3.39958882  3.54091067  3.44311495 ...  1.4629474   7.90340081\n",
      "   0.64336276]\n",
      " [ 5.02850636  9.13294145  0.         ...  4.33472376  2.71764849\n",
      "   3.2003145 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29144699 0.         1.70662427 ... 0.         1.02291878 0.        ]\n",
      " [2.24537311 7.70700204 1.87323345 ... 9.33073494 3.53518792 6.89133896]\n",
      " ...\n",
      " [7.7226673  8.53433302 6.25921216 ... 5.23369581 9.02002562 5.61195538]\n",
      " [4.78373097 0.06790916 7.83494479 ... 9.14014636 4.34509177 3.08954942]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.76673876e-08 5.20967256e-08 3.95257877e-05 ... 1.00044980e-01\n",
      "  5.42960349e-05 6.80672812e-03]\n",
      " [5.54623212e-02 5.93619669e-07 7.16463262e-01 ... 2.86882508e-04\n",
      "  8.69317767e-04 1.97413950e-03]\n",
      " [8.29014375e-04 3.64775160e-05 5.00432427e-02 ... 9.21451090e-03\n",
      "  2.26681060e-01 8.51522420e-03]\n",
      " ...\n",
      " [7.27014638e-01 2.36175352e-01 7.27529624e-03 ... 4.10153154e-03\n",
      "  1.60900161e-05 1.58448157e-02]\n",
      " [1.48490039e-02 4.90498214e-04 1.22339805e-01 ... 1.41739009e-01\n",
      "  1.66809160e-02 3.21284391e-02]\n",
      " [1.88625938e-01 3.31932988e-01 1.26227501e-02 ... 1.95274939e-02\n",
      "  1.51105465e-03 4.22594511e-01]] & cached\n",
      "Re-used Cached Value, runNum =  427\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  427\n",
      "Provided input from cache for runNum = 427\n",
      "Provided input from cache for runNum = 428\n",
      "activation = [[ 6.15559264  5.89795637  6.46599275 ... 11.97666515  5.08425074\n",
      "   3.67006761]\n",
      " [ 0.          3.44839046  0.         ...  0.01480147  0.\n",
      "   0.        ]\n",
      " [ 0.          3.55049604  0.77732912 ...  2.52604251  5.42426601\n",
      "   2.00364069]\n",
      " ...\n",
      " [ 0.          0.          0.09954573 ...  0.          0.\n",
      "   0.11601834]\n",
      " [ 3.40309169  3.54901257  3.448124   ...  1.46760782  7.91157996\n",
      "   0.64766619]\n",
      " [ 5.03322027  9.14107299  0.         ...  4.34238466  2.72037043\n",
      "   3.20546717]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.29035562 0.         1.70990654 ... 0.         1.02553418 0.        ]\n",
      " [2.24920934 7.71214667 1.87272418 ... 9.34553293 3.54077887 6.89876519]\n",
      " ...\n",
      " [7.72766179 8.54756728 6.26647423 ... 5.24549505 9.02995998 5.62316919]\n",
      " [4.78628628 0.06788405 7.83941865 ... 9.14645263 4.34656589 3.09006002]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.72727270e-08 5.11811428e-08 3.88124663e-05 ... 9.97586734e-02\n",
      "  5.39454585e-05 6.76741775e-03]\n",
      " [5.50630194e-02 5.87481378e-07 7.17496372e-01 ... 2.83263410e-04\n",
      "  8.61008256e-04 1.96413111e-03]\n",
      " [8.27770441e-04 3.63856214e-05 4.99864509e-02 ... 9.20647220e-03\n",
      "  2.25726210e-01 8.47718054e-03]\n",
      " ...\n",
      " [7.27858398e-01 2.36068896e-01 7.24299378e-03 ... 4.10614525e-03\n",
      "  1.58168932e-05 1.57383357e-02]\n",
      " [1.48401243e-02 4.91313538e-04 1.21777006e-01 ... 1.41700364e-01\n",
      "  1.65708274e-02 3.20973765e-02]\n",
      " [1.88257072e-01 3.32648006e-01 1.25585096e-02 ... 1.95919554e-02\n",
      "  1.50016487e-03 4.22631442e-01]] & cached\n",
      "Re-used Cached Value, runNum =  428\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  428\n",
      "Provided input from cache for runNum = 428\n",
      "Provided input from cache for runNum = 429\n",
      "activation = [[ 6.15946072  5.89945202  6.46710348 ... 11.9838038   5.08474678\n",
      "   3.67149735]\n",
      " [ 0.          3.44943171  0.         ...  0.01588621  0.\n",
      "   0.        ]\n",
      " [ 0.          3.54830482  0.77587049 ...  2.52851389  5.42689334\n",
      "   2.00470812]\n",
      " ...\n",
      " [ 0.          0.          0.10153865 ...  0.          0.\n",
      "   0.11798652]\n",
      " [ 3.40665009  3.55716865  3.45314677 ...  1.47234157  7.91981131\n",
      "   0.65199343]\n",
      " [ 5.03793189  9.14919816  0.         ...  4.35004496  2.72310504\n",
      "   3.21061547]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28943522 0.         1.71331523 ... 0.         1.02835869 0.        ]\n",
      " [2.2529953  7.71723879 1.87219738 ... 9.36024208 3.54630664 6.90614316]\n",
      " ...\n",
      " [7.73266741 8.56076775 6.27370872 ... 5.25727043 9.03988764 5.63436064]\n",
      " [4.78879373 0.06774749 7.84385113 ... 9.1526387  4.34795819 3.09053926]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.68824497e-08 5.02801098e-08 3.81114518e-05 ... 9.94798194e-02\n",
      "  5.35854635e-05 6.72852480e-03]\n",
      " [5.46698097e-02 5.81301245e-07 7.18531557e-01 ... 2.79693024e-04\n",
      "  8.52842688e-04 1.95421086e-03]\n",
      " [8.26640540e-04 3.62955158e-05 4.99330835e-02 ... 9.20025263e-03\n",
      "  2.24776289e-01 8.43990241e-03]\n",
      " ...\n",
      " [7.28697825e-01 2.35942623e-01 7.21082913e-03 ... 4.11136202e-03\n",
      "  1.55492577e-05 1.56330674e-02]\n",
      " [1.48310096e-02 4.92083767e-04 1.21212060e-01 ... 1.41675322e-01\n",
      "  1.64599828e-02 3.20673335e-02]\n",
      " [1.87885457e-01 3.33321170e-01 1.24942318e-02 ... 1.96577508e-02\n",
      "  1.48938026e-03 4.22659619e-01]] & cached\n",
      "Re-used Cached Value, runNum =  429\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  429\n",
      "Provided input from cache for runNum = 429\n",
      "Provided input from cache for runNum = 430\n",
      "activation = [[ 6.16332697  5.90095124  6.46821181 ... 11.99092994  5.08523564\n",
      "   3.67292572]\n",
      " [ 0.          3.45047736  0.         ...  0.01697896  0.\n",
      "   0.        ]\n",
      " [ 0.          3.54613115  0.77442738 ...  2.53100208  5.42952923\n",
      "   2.00578105]\n",
      " ...\n",
      " [ 0.          0.          0.10353318 ...  0.          0.\n",
      "   0.11995765]\n",
      " [ 3.41019392  3.5653074   3.4581472  ...  1.47706314  7.92803078\n",
      "   0.65629785]\n",
      " [ 5.04261418  9.15727366  0.         ...  4.35767257  2.72580211\n",
      "   3.21574162]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28845933 0.         1.71665288 ... 0.         1.03112665 0.        ]\n",
      " [2.25686742 7.72239948 1.87175828 ... 9.37499338 3.55187381 6.9135336 ]\n",
      " ...\n",
      " [7.73755633 8.5738101  6.28085035 ... 5.26890075 9.04969457 5.64545233]\n",
      " [4.79128374 0.06760431 7.84824779 ... 9.15878004 4.34935077 3.09101064]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.65064297e-08 4.94151311e-08 3.74323547e-05 ... 9.92115351e-02\n",
      "  5.32436748e-05 6.69060335e-03]\n",
      " [5.42817668e-02 5.75295602e-07 7.19557820e-01 ... 2.76149191e-04\n",
      "  8.44836976e-04 1.94440821e-03]\n",
      " [8.25417628e-04 3.62055710e-05 4.98722052e-02 ... 9.19126475e-03\n",
      "  2.23813692e-01 8.40179955e-03]\n",
      " ...\n",
      " [7.29515893e-01 2.35814375e-01 7.17834636e-03 ... 4.11538239e-03\n",
      "  1.52861400e-05 1.55272672e-02]\n",
      " [1.48224194e-02 4.92894072e-04 1.20648374e-01 ... 1.41623295e-01\n",
      "  1.63513734e-02 3.20356805e-02]\n",
      " [1.87529803e-01 3.34043150e-01 1.24306896e-02 ... 1.97208659e-02\n",
      "  1.47876423e-03 4.22676753e-01]] & cached\n",
      "Re-used Cached Value, runNum =  430\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  430\n",
      "Provided input from cache for runNum = 430\n",
      "Provided input from cache for runNum = 431\n",
      "activation = [[ 6.16717263  5.9024303   6.4693134  ... 11.99802656  5.08570144\n",
      "   3.67434571]\n",
      " [ 0.          3.45152951  0.         ...  0.01807873  0.\n",
      "   0.        ]\n",
      " [ 0.          3.5439808   0.77299326 ...  2.53349321  5.43216465\n",
      "   2.00685591]\n",
      " ...\n",
      " [ 0.          0.          0.10553591 ...  0.          0.\n",
      "   0.12194073]\n",
      " [ 3.41372355  3.57340679  3.46312644 ...  1.48175633  7.93621698\n",
      "   0.66057682]\n",
      " [ 5.04732678  9.16538554  0.         ...  4.36533909  2.72853658\n",
      "   3.22087936]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28754259 0.         1.72003902 ... 0.         1.03394511 0.        ]\n",
      " [2.2606745  7.72747216 1.87126816 ... 9.38965979 3.55739564 6.92088557]\n",
      " ...\n",
      " [7.74237579 8.58673028 6.28793376 ... 5.28043223 9.05942053 5.65647994]\n",
      " [4.79372333 0.06738949 7.8526281  ... 9.16483438 4.35068495 3.0914562 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.61337681e-08 4.85635115e-08 3.67623848e-05 ... 9.89495491e-02\n",
      "  5.29061715e-05 6.65322468e-03]\n",
      " [5.38956366e-02 5.69343407e-07 7.20595623e-01 ... 2.72668956e-04\n",
      "  8.36997299e-04 1.93476438e-03]\n",
      " [8.24164709e-04 3.61146085e-05 4.98081848e-02 ... 9.18272443e-03\n",
      "  2.22861073e-01 8.36410477e-03]\n",
      " ...\n",
      " [7.30349641e-01 2.35699399e-01 7.14588869e-03 ... 4.11966591e-03\n",
      "  1.50293464e-05 1.54229413e-02]\n",
      " [1.48135660e-02 4.93690515e-04 1.20085123e-01 ... 1.41584358e-01\n",
      "  1.62446419e-02 3.20057809e-02]\n",
      " [1.87157360e-01 3.34737529e-01 1.23662895e-02 ... 1.97838199e-02\n",
      "  1.46830947e-03 4.22686625e-01]] & cached\n",
      "Re-used Cached Value, runNum =  431\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  431\n",
      "Provided input from cache for runNum = 431\n",
      "Provided input from cache for runNum = 432\n",
      "activation = [[ 6.17101361  5.90391317  6.47041537 ... 12.00511928  5.08616255\n",
      "   3.67576572]\n",
      " [ 0.          3.45259089  0.         ...  0.01918726  0.\n",
      "   0.        ]\n",
      " [ 0.          3.54179055  0.77153972 ...  2.53592688  5.43474621\n",
      "   2.00790484]\n",
      " ...\n",
      " [ 0.          0.          0.10753001 ...  0.          0.\n",
      "   0.12391723]\n",
      " [ 3.41724797  3.58146358  3.4680869  ...  1.48641745  7.94438361\n",
      "   0.66483166]\n",
      " [ 5.05203941  9.17350072  0.         ...  4.37303955  2.73128137\n",
      "   3.22601975]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28663885 0.         1.72343118 ... 0.         1.03682566 0.        ]\n",
      " [2.26448642 7.73255121 1.87078029 ... 9.40431394 3.56290925 6.92822563]\n",
      " ...\n",
      " [7.74717163 8.59960861 6.29498679 ... 5.29194519 9.06913994 5.6674739 ]\n",
      " [4.79611119 0.06710751 7.85697202 ... 9.17080347 4.35197745 3.09187165]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.57639001e-08 4.77207284e-08 3.61004205e-05 ... 9.86688845e-02\n",
      "  5.25685466e-05 6.61547652e-03]\n",
      " [5.35155123e-02 5.63531939e-07 7.21651917e-01 ... 2.69273845e-04\n",
      "  8.29259699e-04 1.92529780e-03]\n",
      " [8.22770082e-04 3.60167205e-05 4.97347261e-02 ... 9.17205913e-03\n",
      "  2.21904605e-01 8.32556162e-03]\n",
      " ...\n",
      " [7.31171010e-01 2.35598723e-01 7.11315576e-03 ... 4.12423764e-03\n",
      "  1.47758087e-05 1.53197787e-02]\n",
      " [1.48034933e-02 4.94452796e-04 1.19513205e-01 ... 1.41538173e-01\n",
      "  1.61380981e-02 3.19748046e-02]\n",
      " [1.86793628e-01 3.35472272e-01 1.23020065e-02 ... 1.98491779e-02\n",
      "  1.45791937e-03 4.22709687e-01]] & cached\n",
      "Re-used Cached Value, runNum =  432\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  432\n",
      "Provided input from cache for runNum = 432\n",
      "Provided input from cache for runNum = 433\n",
      "activation = [[ 6.17484107  5.90538791  6.47151542 ... 12.01219128  5.08660023\n",
      "   3.67717866]\n",
      " [ 0.          3.45365694  0.         ...  0.02029922  0.\n",
      "   0.        ]\n",
      " [ 0.          3.53960617  0.77007476 ...  2.53835428  5.43733128\n",
      "   2.00894665]\n",
      " ...\n",
      " [ 0.          0.          0.10952553 ...  0.          0.\n",
      "   0.12589917]\n",
      " [ 3.42079652  3.58954689  3.47305429 ...  1.49113602  7.95257681\n",
      "   0.66909598]\n",
      " [ 5.05674836  9.18160706  0.         ...  4.38075982  2.73403145\n",
      "   3.23115885]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28578519 0.         1.7268675  ... 0.         1.03977867 0.        ]\n",
      " [2.26821824 7.73750763 1.87024026 ... 9.4187928  3.56830415 6.93548029]\n",
      " ...\n",
      " [7.75192054 8.61240866 6.30200219 ... 5.30338978 9.07880007 5.67841276]\n",
      " [4.79848679 0.06681062 7.86131384 ... 9.17673348 4.35324522 3.0922806 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.54057395e-08 4.69049360e-08 3.54577572e-05 ... 9.84096555e-02\n",
      "  5.22308384e-05 6.57891062e-03]\n",
      " [5.31346778e-02 5.57716261e-07 7.22668488e-01 ... 2.65905638e-04\n",
      "  8.21675762e-04 1.91589527e-03]\n",
      " [8.21682730e-04 3.59331467e-05 4.96760782e-02 ... 9.16517887e-03\n",
      "  2.20966778e-01 8.28905052e-03]\n",
      " ...\n",
      " [7.32002872e-01 2.35490400e-01 7.08053225e-03 ... 4.12902406e-03\n",
      "  1.45267120e-05 1.52181577e-02]\n",
      " [1.47970922e-02 4.95338643e-04 1.18967642e-01 ... 1.41532894e-01\n",
      "  1.60324722e-02 3.19493077e-02]\n",
      " [1.86414665e-01 3.36152660e-01 1.22371275e-02 ... 1.99124896e-02\n",
      "  1.44752081e-03 4.22711068e-01]] & cached\n",
      "Re-used Cached Value, runNum =  433\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  433\n",
      "Provided input from cache for runNum = 433\n",
      "Provided input from cache for runNum = 434\n",
      "activation = [[ 6.17863885  5.90682148  6.4725985  ... 12.01920351  5.0869919\n",
      "   3.67857045]\n",
      " [ 0.          3.45469451  0.         ...  0.02138563  0.\n",
      "   0.        ]\n",
      " [ 0.          3.5374626   0.76863234 ...  2.54081125  5.43994224\n",
      "   2.01000042]\n",
      " ...\n",
      " [ 0.          0.          0.11153203 ...  0.          0.\n",
      "   0.12789642]\n",
      " [ 3.42433707  3.59762003  3.4780003  ...  1.49584824  7.96075356\n",
      "   0.67335266]\n",
      " [ 5.06146378  9.18972773  0.         ...  4.38852132  2.73679983\n",
      "   3.23630836]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28492614 0.         1.73029637 ... 0.         1.04274061 0.        ]\n",
      " [2.2720068  7.74252281 1.86975565 ... 9.43332095 3.57375865 6.94275652]\n",
      " ...\n",
      " [7.75661994 8.62513835 6.30895934 ... 5.31478555 9.08841155 5.68930921]\n",
      " [4.80076647 0.06638699 7.86558157 ... 9.18250841 4.35439854 3.09262641]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.50508241e-08 4.60959406e-08 3.48283432e-05 ... 9.81440016e-02\n",
      "  5.18973247e-05 6.54204840e-03]\n",
      " [5.27516876e-02 5.51883635e-07 7.23698022e-01 ... 2.62584638e-04\n",
      "  8.14141690e-04 1.90645732e-03]\n",
      " [8.20384516e-04 3.58378238e-05 4.96111380e-02 ... 9.15655406e-03\n",
      "  2.20016035e-01 8.25127817e-03]\n",
      " ...\n",
      " [7.32825189e-01 2.35354131e-01 7.04884242e-03 ... 4.13445320e-03\n",
      "  1.42849477e-05 1.51169405e-02]\n",
      " [1.47858233e-02 4.95998595e-04 1.18403198e-01 ... 1.41484100e-01\n",
      "  1.59267538e-02 3.19162198e-02]\n",
      " [1.86051663e-01 3.36842313e-01 1.21745039e-02 ... 1.99804501e-02\n",
      "  1.43749782e-03 4.22725571e-01]] & cached\n",
      "Re-used Cached Value, runNum =  434\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  434\n",
      "Provided input from cache for runNum = 434\n",
      "Provided input from cache for runNum = 435\n",
      "activation = [[ 6.1824434   5.90827863  6.47368928 ... 12.02621971  5.08739055\n",
      "   3.67996939]\n",
      " [ 0.          3.45569153  0.         ...  0.02243703  0.\n",
      "   0.        ]\n",
      " [ 0.          3.53529162  0.76719101 ...  2.54324262  5.44252694\n",
      "   2.01103981]\n",
      " ...\n",
      " [ 0.          0.          0.1135489  ...  0.          0.\n",
      "   0.12990664]\n",
      " [ 3.42787525  3.60568399  3.48294397 ...  1.5005825   7.96892552\n",
      "   0.67760932]\n",
      " [ 5.0661618   9.19783462  0.         ...  4.39627117  2.73957602\n",
      "   3.24145399]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28409589 0.         1.73375852 ... 0.         1.04575051 0.        ]\n",
      " [2.27578151 7.74749139 1.86926637 ... 9.44780868 3.57921431 6.95001918]\n",
      " ...\n",
      " [7.7612823  8.63783343 6.31588387 ... 5.32617081 9.0979961  5.70017979]\n",
      " [4.80303025 0.0659715  7.86982977 ... 9.18826647 4.35552985 3.09296311]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.47004859e-08 4.53021747e-08 3.42073205e-05 ... 9.78702757e-02\n",
      "  5.15685375e-05 6.50546078e-03]\n",
      " [5.23796277e-02 5.46345549e-07 7.24739857e-01 ... 2.59339518e-04\n",
      "  8.06728155e-04 1.89727125e-03]\n",
      " [8.18952363e-04 3.57397326e-05 4.95353771e-02 ... 9.14559120e-03\n",
      "  2.19053327e-01 8.21293248e-03]\n",
      " ...\n",
      " [7.33631373e-01 2.35255587e-01 7.01606038e-03 ... 4.13867176e-03\n",
      "  1.40448818e-05 1.50156047e-02]\n",
      " [1.47761215e-02 4.96777114e-04 1.17839645e-01 ... 1.41438661e-01\n",
      "  1.58222157e-02 3.18853715e-02]\n",
      " [1.85694677e-01 3.37603388e-01 1.21108747e-02 ... 2.00458525e-02\n",
      "  1.42746306e-03 4.22736932e-01]] & cached\n",
      "Re-used Cached Value, runNum =  435\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  435\n",
      "Provided input from cache for runNum = 435\n",
      "Provided input from cache for runNum = 436\n",
      "activation = [[ 6.1862425   5.90974013  6.47477744 ... 12.03322849  5.08777377\n",
      "   3.68136819]\n",
      " [ 0.          3.4567033   0.         ...  0.02351092  0.\n",
      "   0.        ]\n",
      " [ 0.          3.53313192  0.76576747 ...  2.54568411  5.44509907\n",
      "   2.01208547]\n",
      " ...\n",
      " [ 0.          0.          0.11556737 ...  0.          0.\n",
      "   0.13192347]\n",
      " [ 3.43138748  3.61370779  3.48786582 ...  1.5052772   7.97705957\n",
      "   0.68184405]\n",
      " [ 5.07089262  9.20597499  0.         ...  4.40407961  2.74238598\n",
      "   3.2466212 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.2832433  0.         1.73721488 ... 0.         1.04877302 0.        ]\n",
      " [2.27948247 7.75235588 1.86871281 ... 9.4622216  3.58458696 6.95723773]\n",
      " ...\n",
      " [7.76591503 8.6504754  6.32277188 ... 5.33750531 9.10755085 5.71101675]\n",
      " [4.80531021 0.06558468 7.87410014 ... 9.19401703 4.35666567 3.09331657]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.43498580e-08 4.45081182e-08 3.35959269e-05 ... 9.75947989e-02\n",
      "  5.12402842e-05 6.46891717e-03]\n",
      " [5.20030512e-02 5.40822361e-07 7.25780448e-01 ... 2.56160424e-04\n",
      "  7.99415045e-04 1.88822103e-03]\n",
      " [8.17446824e-04 3.56373005e-05 4.94624812e-02 ... 9.13594173e-03\n",
      "  2.18122332e-01 8.17525652e-03]\n",
      " ...\n",
      " [7.34475776e-01 2.35177138e-01 6.98429967e-03 ... 4.14438663e-03\n",
      "  1.38117253e-05 1.49171739e-02]\n",
      " [1.47649821e-02 4.97477628e-04 1.17282646e-01 ... 1.41411405e-01\n",
      "  1.57195814e-02 3.18558492e-02]\n",
      " [1.85305535e-01 3.38302608e-01 1.20469558e-02 ... 2.01134831e-02\n",
      "  1.41752791e-03 4.22745401e-01]] & cached\n",
      "Re-used Cached Value, runNum =  436\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  436\n",
      "Provided input from cache for runNum = 436\n",
      "Provided input from cache for runNum = 437\n",
      "activation = [[ 6.19004088  5.91121665  6.47586925 ... 12.04024261  5.08815204\n",
      "   3.68277283]\n",
      " [ 0.          3.4576924   0.         ...  0.02456305  0.\n",
      "   0.        ]\n",
      " [ 0.          3.53095649  0.76433455 ...  2.54808446  5.44764895\n",
      "   2.01311143]\n",
      " ...\n",
      " [ 0.          0.          0.1175881  ...  0.          0.\n",
      "   0.1339462 ]\n",
      " [ 3.43486397  3.62166338  3.49275248 ...  1.50990758  7.98514705\n",
      "   0.68604059]\n",
      " [ 5.07561449  9.21411137  0.         ...  4.41190223  2.74521523\n",
      "   3.25178336]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28235018 0.         1.74065142 ... 0.         1.05177713 0.        ]\n",
      " [2.28324079 7.7572957  1.86820275 ... 9.47666963 3.58999967 6.96446913]\n",
      " ...\n",
      " [7.77053544 8.66310264 6.32963717 ... 5.34883348 9.11710137 5.72182878]\n",
      " [4.80756114 0.06517725 7.87834255 ... 9.19973234 4.35775936 3.09366051]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.40088474e-08 4.37343238e-08 3.29996180e-05 ... 9.73141096e-02\n",
      "  5.09158455e-05 6.43254380e-03]\n",
      " [5.16282582e-02 5.35373518e-07 7.26803107e-01 ... 2.53010557e-04\n",
      "  7.92112554e-04 1.87918869e-03]\n",
      " [8.15960253e-04 3.55353600e-05 4.93888247e-02 ... 9.12487205e-03\n",
      "  2.17178756e-01 8.13714975e-03]\n",
      " ...\n",
      " [7.35294856e-01 2.35086829e-01 6.95251610e-03 ... 4.14988698e-03\n",
      "  1.35814410e-05 1.48190128e-02]\n",
      " [1.47550715e-02 4.98215560e-04 1.16733076e-01 ... 1.41380692e-01\n",
      "  1.56168708e-02 3.18259923e-02]\n",
      " [1.84937975e-01 3.39045262e-01 1.19841449e-02 ... 2.01819603e-02\n",
      "  1.40767306e-03 4.22759421e-01]] & cached\n",
      "Re-used Cached Value, runNum =  437\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  437\n",
      "Provided input from cache for runNum = 437\n",
      "Provided input from cache for runNum = 438\n",
      "activation = [[ 6.1938284   5.91268409  6.4769488  ... 12.04722563  5.08851618\n",
      "   3.68416788]\n",
      " [ 0.          3.45866835  0.         ...  0.02560853  0.\n",
      "   0.        ]\n",
      " [ 0.          3.5287569   0.76290923 ...  2.55047405  5.45016717\n",
      "   2.01412812]\n",
      " ...\n",
      " [ 0.          0.          0.11961851 ...  0.          0.\n",
      "   0.135977  ]\n",
      " [ 3.43835956  3.62963537  3.49764079 ...  1.51456702  7.99324617\n",
      "   0.69024844]\n",
      " [ 5.08032031  9.22221905  0.         ...  4.41972647  2.74803484\n",
      "   3.25693573]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28173509 0.         1.74429142 ... 0.         1.05509563 0.        ]\n",
      " [2.28695205 7.76215606 1.86766703 ... 9.49102945 3.59535382 6.9716512 ]\n",
      " ...\n",
      " [7.77515683 8.67574131 6.33648101 ... 5.36015401 9.12666288 5.73263003]\n",
      " [4.80983101 0.06480744 7.88258664 ... 9.20544735 4.35887044 3.09401922]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.36721810e-08 4.29738580e-08 3.24110059e-05 ... 9.70362863e-02\n",
      "  5.05807734e-05 6.39661063e-03]\n",
      " [5.12672861e-02 5.30062649e-07 7.27842395e-01 ... 2.49925908e-04\n",
      "  7.84936096e-04 1.87036035e-03]\n",
      " [8.14642033e-04 3.54373508e-05 4.93169702e-02 ... 9.11461015e-03\n",
      "  2.16244485e-01 8.09978589e-03]\n",
      " ...\n",
      " [7.36097441e-01 2.34997752e-01 6.91983095e-03 ... 4.15520355e-03\n",
      "  1.33524722e-05 1.47213953e-02]\n",
      " [1.47456203e-02 4.98998737e-04 1.16172178e-01 ... 1.41353323e-01\n",
      "  1.55120625e-02 3.17971669e-02]\n",
      " [1.84572022e-01 3.39777920e-01 1.19201425e-02 ... 2.02492978e-02\n",
      "  1.39766948e-03 4.22757486e-01]] & cached\n",
      "Re-used Cached Value, runNum =  438\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  438\n",
      "Provided input from cache for runNum = 438\n",
      "Provided input from cache for runNum = 439\n",
      "activation = [[ 6.19760761  5.9141473   6.47802447 ... 12.05418461  5.08886162\n",
      "   3.68556037]\n",
      " [ 0.          3.45964775  0.         ...  0.02666063  0.\n",
      "   0.        ]\n",
      " [ 0.          3.52656007  0.76148968 ...  2.55286527  5.45268558\n",
      "   2.01513876]\n",
      " ...\n",
      " [ 0.          0.          0.12165246 ...  0.          0.\n",
      "   0.13801095]\n",
      " [ 3.44182452  3.63754177  3.50249527 ...  1.51917227  8.00130125\n",
      "   0.69441589]\n",
      " [ 5.08503218  9.23033612  0.         ...  4.42759632  2.75085425\n",
      "   3.26209785]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28101098 0.         1.74782382 ... 0.         1.05832707 0.        ]\n",
      " [2.29074095 7.76710328 1.8671998  ... 9.50545145 3.60074577 6.97886097]\n",
      " ...\n",
      " [7.77974409 8.6883127  6.34328022 ... 5.37141481 9.13616781 5.74339032]\n",
      " [4.81205205 0.06436632 7.8867958  ... 9.21106657 4.35992856 3.09434923]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.33456719e-08 4.22309945e-08 3.18429580e-05 ... 9.67664311e-02\n",
      "  5.02554920e-05 6.36104891e-03]\n",
      " [5.09004223e-02 5.24714180e-07 7.28846597e-01 ... 2.46849215e-04\n",
      "  7.77778014e-04 1.86145852e-03]\n",
      " [8.13298512e-04 3.53364070e-05 4.92477628e-02 ... 9.10349961e-03\n",
      "  2.15301013e-01 8.06204349e-03]\n",
      " ...\n",
      " [7.36897440e-01 2.34894731e-01 6.88788755e-03 ... 4.16072362e-03\n",
      "  1.31286435e-05 1.46247377e-02]\n",
      " [1.47370146e-02 4.99754276e-04 1.15628649e-01 ... 1.41318249e-01\n",
      "  1.54087357e-02 3.17671538e-02]\n",
      " [1.84212191e-01 3.40506735e-01 1.18577076e-02 ... 2.03178667e-02\n",
      "  1.38783226e-03 4.22755583e-01]] & cached\n",
      "Re-used Cached Value, runNum =  439\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  439\n",
      "Provided input from cache for runNum = 439\n",
      "Provided input from cache for runNum = 440\n",
      "activation = [[ 6.20139213  5.91562257  6.47910336 ... 12.06114042  5.08920252\n",
      "   3.68695784]\n",
      " [ 0.          3.46062364  0.         ...  0.02771104  0.\n",
      "   0.        ]\n",
      " [ 0.          3.52437048  0.76008157 ...  2.55525319  5.45520746\n",
      "   2.01614271]\n",
      " ...\n",
      " [ 0.          0.          0.12368368 ...  0.          0.\n",
      "   0.14003981]\n",
      " [ 3.44527787  3.64542392  3.50733013 ...  1.52376612  8.00934386\n",
      "   0.69856392]\n",
      " [ 5.0896994   9.23838519  0.         ...  4.4354047   2.75360187\n",
      "   3.26723093]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.28022117 0.         1.75128253 ... 0.         1.06151359 0.        ]\n",
      " [2.29450177 7.77200524 1.86672934 ... 9.51979763 3.60608062 6.98603357]\n",
      " ...\n",
      " [7.78428048 8.70080501 6.35002585 ... 5.38260251 9.14560662 5.75409613]\n",
      " [4.81429053 0.06395451 7.89098706 ... 9.21669832 4.36101328 3.09468937]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.30321515e-08 4.15185814e-08 3.12915612e-05 ... 9.65090904e-02\n",
      "  4.99397528e-05 6.32651016e-03]\n",
      " [5.05437325e-02 5.19562775e-07 7.29834116e-01 ... 2.43814608e-04\n",
      "  7.70763646e-04 1.85274983e-03]\n",
      " [8.12107115e-04 3.52456051e-05 4.91794718e-02 ... 9.09235175e-03\n",
      "  2.14352564e-01 8.02488765e-03]\n",
      " ...\n",
      " [7.37677063e-01 2.34782731e-01 6.85526350e-03 ... 4.16501848e-03\n",
      "  1.29058225e-05 1.45278575e-02]\n",
      " [1.47318406e-02 5.00651001e-04 1.15096786e-01 ... 1.41288514e-01\n",
      "  1.53064171e-02 3.17397666e-02]\n",
      " [1.83858136e-01 3.41255156e-01 1.17946847e-02 ... 2.03820791e-02\n",
      "  1.37787933e-03 4.22732955e-01]] & cached\n",
      "Re-used Cached Value, runNum =  440\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  440\n",
      "Provided input from cache for runNum = 440\n",
      "Provided input from cache for runNum = 441\n",
      "activation = [[ 6.20517565  5.91710634  6.48018105 ... 12.06809121  5.08954139\n",
      "   3.68835946]\n",
      " [ 0.          3.46157382  0.         ...  0.02873196  0.\n",
      "   0.        ]\n",
      " [ 0.          3.52215794  0.75867021 ...  2.55760568  5.45769557\n",
      "   2.01712632]\n",
      " ...\n",
      " [ 0.          0.          0.12571487 ...  0.          0.\n",
      "   0.14207141]\n",
      " [ 3.44869577  3.6532486   3.51213645 ...  1.52831962  8.01733654\n",
      "   0.70267734]\n",
      " [ 5.09435332  9.24641853  0.         ...  4.44320722  2.75633851\n",
      "   3.27235653]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27945016 0.         1.75475183 ... 0.         1.06475405 0.        ]\n",
      " [2.29825202 7.77687585 1.86625288 ... 9.53409636 3.61139992 6.99318555]\n",
      " ...\n",
      " [7.78878349 8.71325887 6.35673819 ... 5.3937729  9.15501977 5.76476741]\n",
      " [4.8165257  0.06355378 7.8951672  ... 9.22233279 4.36209908 3.09503391]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.27211731e-08 4.08133294e-08 3.07478643e-05 ... 9.62415357e-02\n",
      "  4.96260230e-05 6.29188427e-03]\n",
      " [5.01913718e-02 5.14536551e-07 7.30838515e-01 ... 2.40860434e-04\n",
      "  7.63853707e-04 1.84425669e-03]\n",
      " [8.10822098e-04 3.51505952e-05 4.91055656e-02 ... 9.08015063e-03\n",
      "  2.13410226e-01 7.98738917e-03]\n",
      " ...\n",
      " [7.38461510e-01 2.34691524e-01 6.82259864e-03 ... 4.16942926e-03\n",
      "  1.26867638e-05 1.44325738e-02]\n",
      " [1.47252773e-02 5.01523849e-04 1.14556829e-01 ... 1.41254886e-01\n",
      "  1.52043349e-02 3.17121515e-02]\n",
      " [1.83497001e-01 3.42009086e-01 1.17313908e-02 ... 2.04465022e-02\n",
      "  1.36797034e-03 4.22713691e-01]] & cached\n",
      "Re-used Cached Value, runNum =  441\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  441\n",
      "Provided input from cache for runNum = 441\n",
      "Provided input from cache for runNum = 442\n",
      "activation = [[ 6.20893418  5.91856677  6.48124567 ... 12.07498965  5.08984443\n",
      "   3.68974629]\n",
      " [ 0.          3.46254778  0.         ...  0.02977862  0.\n",
      "   0.        ]\n",
      " [ 0.          3.52001377  0.75729752 ...  2.56002729  5.46023319\n",
      "   2.01813804]\n",
      " ...\n",
      " [ 0.          0.          0.12774657 ...  0.          0.\n",
      "   0.14410712]\n",
      " [ 3.45212923  3.66109465  3.51693838 ...  1.53291164  8.02534788\n",
      "   0.7067945 ]\n",
      " [ 5.09899355  9.25441983  0.         ...  4.45100945  2.7590477\n",
      "   3.27747739]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27870655 0.         1.75821709 ... 0.         1.0680197  0.        ]\n",
      " [2.30194852 7.78165891 1.86575941 ... 9.54829371 3.61664596 7.00028721]\n",
      " ...\n",
      " [7.79324934 8.72563631 6.36340393 ... 5.40486682 9.16437027 5.77539042]\n",
      " [4.8187484  0.06313106 7.89932318 ... 9.22789384 4.36315538 3.0953612 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.24220262e-08 4.01311314e-08 3.02215987e-05 ... 9.60044971e-02\n",
      "  4.93151703e-05 6.25862303e-03]\n",
      " [4.98422471e-02 5.09482119e-07 7.31819729e-01 ... 2.37920997e-04\n",
      "  7.57052785e-04 1.83576117e-03]\n",
      " [8.09839022e-04 3.50669151e-05 4.90426489e-02 ... 9.07105371e-03\n",
      "  2.12474243e-01 7.95160061e-03]\n",
      " ...\n",
      " [7.39237603e-01 2.34556421e-01 6.78993718e-03 ... 4.17328900e-03\n",
      "  1.24714071e-05 1.43371563e-02]\n",
      " [1.47209613e-02 5.02411660e-04 1.14027153e-01 ... 1.41228049e-01\n",
      "  1.51027462e-02 3.16861277e-02]\n",
      " [1.83135339e-01 3.42696319e-01 1.16682366e-02 ... 2.05086229e-02\n",
      "  1.35808642e-03 4.22665750e-01]] & cached\n",
      "Re-used Cached Value, runNum =  442\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  442\n",
      "Provided input from cache for runNum = 442\n",
      "Provided input from cache for runNum = 443\n",
      "activation = [[ 6.21267616  5.92000479  6.48230456 ... 12.08185067  5.09011961\n",
      "   3.69112358]\n",
      " [ 0.          3.46353207  0.         ...  0.0308372   0.\n",
      "   0.        ]\n",
      " [ 0.          3.51790138  0.75594113 ...  2.56247105  5.46278958\n",
      "   2.01915729]\n",
      " ...\n",
      " [ 0.          0.          0.12978221 ...  0.          0.\n",
      "   0.14614623]\n",
      " [ 3.4555651   3.66897119  3.52173883 ...  1.53755092  8.03336215\n",
      "   0.71092835]\n",
      " [ 5.10362863  9.26241289  0.         ...  4.45883008  2.761752\n",
      "   3.2826004 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27796902 0.         1.76166536 ... 0.         1.07128968 0.        ]\n",
      " [2.30568625 7.78648007 1.86530206 ... 9.56249947 3.62191815 7.00738825]\n",
      " ...\n",
      " [7.79767818 8.73796789 6.37002609 ... 5.41594456 9.17367718 5.78598964]\n",
      " [4.82086561 0.06256142 7.90339542 ... 9.23328743 4.36410045 3.09561993]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.21255119e-08 3.94584433e-08 2.97057175e-05 ... 9.57606737e-02\n",
      "  4.90079409e-05 6.22507875e-03]\n",
      " [4.94911396e-02 5.04431092e-07 7.32799907e-01 ... 2.35018412e-04\n",
      "  7.50318913e-04 1.82725706e-03]\n",
      " [8.08668117e-04 3.49765167e-05 4.89733866e-02 ... 9.06054110e-03\n",
      "  2.11521286e-01 7.91490131e-03]\n",
      " ...\n",
      " [7.40003619e-01 2.34408818e-01 6.75791269e-03 ... 4.17748055e-03\n",
      "  1.22608909e-05 1.42423715e-02]\n",
      " [1.47143744e-02 5.03211445e-04 1.13495394e-01 ... 1.41189994e-01\n",
      "  1.50017122e-02 3.16572684e-02]\n",
      " [1.82788673e-01 3.43414321e-01 1.16068732e-02 ... 2.05743887e-02\n",
      "  1.34846711e-03 4.22635513e-01]] & cached\n",
      "Re-used Cached Value, runNum =  443\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  443\n",
      "Provided input from cache for runNum = 443\n",
      "Provided input from cache for runNum = 444\n",
      "activation = [[ 6.21642234  5.92148145  6.48337048 ... 12.0887257   5.09041052\n",
      "   3.69251526]\n",
      " [ 0.          3.46448809  0.         ...  0.03187466  0.\n",
      "   0.        ]\n",
      " [ 0.          3.51572331  0.75457479 ...  2.56485362  5.46529231\n",
      "   2.02014444]\n",
      " ...\n",
      " [ 0.          0.          0.13182107 ...  0.          0.\n",
      "   0.14818845]\n",
      " [ 3.45897696  3.67681034  3.52651507 ...  1.54217469  8.04133412\n",
      "   0.7150404 ]\n",
      " [ 5.10823145  9.27035639  0.         ...  4.4666135   2.76442058\n",
      "   3.28769608]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27727904 0.         1.7651401  ... 0.         1.07463653 0.        ]\n",
      " [2.30940815 7.79122794 1.86484022 ... 9.57662758 3.62716017 7.01445663]\n",
      " ...\n",
      " [7.80204464 8.75023314 6.37659679 ... 5.42697004 9.18292182 5.79654107]\n",
      " [4.82298319 0.06204214 7.90745536 ... 9.23870072 4.36506663 3.09589979]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.18283955e-08 3.87915452e-08 2.91967118e-05 ... 9.55057799e-02\n",
      "  4.87051794e-05 6.19147686e-03]\n",
      " [4.91371079e-02 4.99509258e-07 7.33775810e-01 ... 2.32175740e-04\n",
      "  7.43726879e-04 1.81891649e-03]\n",
      " [8.07311657e-04 3.48832371e-05 4.88993143e-02 ... 9.04934715e-03\n",
      "  2.10581397e-01 7.87806826e-03]\n",
      " ...\n",
      " [7.40795839e-01 2.34338529e-01 6.72681883e-03 ... 4.18255024e-03\n",
      "  1.20546004e-05 1.41504000e-02]\n",
      " [1.47072818e-02 5.04126999e-04 1.12973357e-01 ... 1.41179372e-01\n",
      "  1.49021000e-02 3.16323737e-02]\n",
      " [1.82422578e-01 3.44165408e-01 1.15458959e-02 ... 2.06409904e-02\n",
      "  1.33893451e-03 4.22617483e-01]] & cached\n",
      "Re-used Cached Value, runNum =  444\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  444\n",
      "Provided input from cache for runNum = 444\n",
      "Provided input from cache for runNum = 445\n",
      "activation = [[ 6.22016106  5.92295762  6.48443502 ... 12.09558798  5.09069536\n",
      "   3.69390424]\n",
      " [ 0.          3.46543298  0.         ...  0.03290536  0.\n",
      "   0.        ]\n",
      " [ 0.          3.5135611   0.7532328  ...  2.56723925  5.46779404\n",
      "   2.02113466]\n",
      " ...\n",
      " [ 0.          0.          0.13387767 ...  0.          0.\n",
      "   0.15025193]\n",
      " [ 3.46239038  3.68465679  3.53128191 ...  1.54681766  8.04931575\n",
      "   0.7191504 ]\n",
      " [ 5.11280987  9.27826704  0.         ...  4.47437406  2.76705844\n",
      "   3.29277332]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27664434 0.         1.76864732 ... 0.         1.07806838 0.        ]\n",
      " [2.31309703 7.79591494 1.86437415 ... 9.59067434 3.63235008 7.02148622]\n",
      " ...\n",
      " [7.80640621 8.76249368 6.38314367 ... 5.43798459 9.19216654 5.80707325]\n",
      " [4.82508319 0.06150101 7.91149293 ... 9.24409198 4.36602523 3.09617596]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.15391063e-08 3.81390029e-08 2.86987678e-05 ... 9.52546269e-02\n",
      "  4.83998823e-05 6.15828294e-03]\n",
      " [4.87927823e-02 4.94665948e-07 7.34752827e-01 ... 2.29381064e-04\n",
      "  7.37228878e-04 1.81069752e-03]\n",
      " [8.06128045e-04 3.47951404e-05 4.88278968e-02 ... 9.03890273e-03\n",
      "  2.09640906e-01 7.84185216e-03]\n",
      " ...\n",
      " [7.41564168e-01 2.34239894e-01 6.69508740e-03 ... 4.18700003e-03\n",
      "  1.18497850e-05 1.40584765e-02]\n",
      " [1.47010414e-02 5.05048792e-04 1.12446448e-01 ... 1.41164501e-01\n",
      "  1.48013526e-02 3.16073264e-02]\n",
      " [1.82067644e-01 3.44901347e-01 1.14846665e-02 ... 2.07059537e-02\n",
      "  1.32932494e-03 4.22585297e-01]] & cached\n",
      "Re-used Cached Value, runNum =  445\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  445\n",
      "Provided input from cache for runNum = 445\n",
      "Provided input from cache for runNum = 446\n",
      "activation = [[ 6.22389118  5.92442068  6.48549538 ... 12.10241673  5.09096375\n",
      "   3.69528406]\n",
      " [ 0.          3.46638638  0.         ...  0.03393877  0.\n",
      "   0.        ]\n",
      " [ 0.          3.51141119  0.75190934 ...  2.56964632  5.47029812\n",
      "   2.02213108]\n",
      " ...\n",
      " [ 0.          0.          0.13593933 ...  0.          0.\n",
      "   0.15232386]\n",
      " [ 3.46578     3.69244926  3.536024   ...  1.55142827  8.05725457\n",
      "   0.72323555]\n",
      " [ 5.11738066  9.28617339  0.         ...  4.48215534  2.76971182\n",
      "   3.2978486 ]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.2760573  0.         1.77217806 ... 0.         1.08153167 0.        ]\n",
      " [2.31682181 7.80064154 1.86394625 ... 9.60475128 3.63758831 7.02852961]\n",
      " ...\n",
      " [7.81075342 8.77471985 6.38966651 ... 5.44897787 9.2014024  5.81758455]\n",
      " [4.8271637  0.06092553 7.91550969 ... 9.24942064 4.36694926 3.09644136]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.12521170e-08 3.74902715e-08 2.82095552e-05 ... 9.49994641e-02\n",
      "  4.80964731e-05 6.12500904e-03]\n",
      " [4.84491067e-02 4.89828760e-07 7.35735855e-01 ... 2.26629856e-04\n",
      "  7.30757502e-04 1.80252926e-03]\n",
      " [8.04841627e-04 3.46981145e-05 4.87530626e-02 ... 9.02744739e-03\n",
      "  2.08699046e-01 7.80509128e-03]\n",
      " ...\n",
      " [7.42330645e-01 2.34136377e-01 6.66390056e-03 ... 4.19193499e-03\n",
      "  1.16497599e-05 1.39674168e-02]\n",
      " [1.46926265e-02 5.05843084e-04 1.11913337e-01 ... 1.41133906e-01\n",
      "  1.47008868e-02 3.15794002e-02]\n",
      " [1.81715662e-01 3.45629633e-01 1.14244396e-02 ... 2.07734747e-02\n",
      "  1.31988912e-03 4.22557177e-01]] & cached\n",
      "Re-used Cached Value, runNum =  446\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  446\n",
      "Provided input from cache for runNum = 446\n",
      "Provided input from cache for runNum = 447\n",
      "activation = [[ 6.22763979  5.92592643  6.48656412 ... 12.10927075  5.09125649\n",
      "   3.69667731]\n",
      " [ 0.          3.46730885  0.         ...  0.03494212  0.\n",
      "   0.        ]\n",
      " [ 0.          3.50923932  0.75057998 ...  2.57203239  5.47278567\n",
      "   2.02311808]\n",
      " ...\n",
      " [ 0.          0.          0.13799915 ...  0.          0.\n",
      "   0.15440019]\n",
      " [ 3.46914623  3.70018849  3.54074006 ...  1.55599813  8.06516226\n",
      "   0.72729277]\n",
      " [ 5.12194845  9.29407088  0.         ...  4.48995014  2.77237303\n",
      "   3.30293018]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.2754333  0.         1.77567153 ... 0.         1.08496411 0.        ]\n",
      " [2.32056667 7.80537966 1.86353678 ... 9.61884225 3.64285347 7.03558219]\n",
      " ...\n",
      " [7.81508887 8.78694201 6.39616829 ... 5.45997263 9.21063896 5.82807785]\n",
      " [4.82936545 0.06057808 7.91959404 ... 9.25489729 4.36801806 3.09679169]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.09705948e-08 3.68545938e-08 2.77327293e-05 ... 9.47502097e-02\n",
      "  4.78000460e-05 6.09226722e-03]\n",
      " [4.81031812e-02 4.85063602e-07 7.36696677e-01 ... 2.23907272e-04\n",
      "  7.24335621e-04 1.79440372e-03]\n",
      " [8.03582240e-04 3.46043266e-05 4.86827260e-02 ... 9.01686097e-03\n",
      "  2.07777481e-01 7.76889795e-03]\n",
      " ...\n",
      " [7.43109543e-01 2.34043536e-01 6.63349411e-03 ... 4.19722286e-03\n",
      "  1.14538474e-05 1.38776227e-02]\n",
      " [1.46850484e-02 5.06684147e-04 1.11394809e-01 ... 1.41114965e-01\n",
      "  1.46022976e-02 3.15530873e-02]\n",
      " [1.81352260e-01 3.46340235e-01 1.13647610e-02 ... 2.08406230e-02\n",
      "  1.31049160e-03 4.22520994e-01]] & cached\n",
      "Re-used Cached Value, runNum =  447\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  447\n",
      "Provided input from cache for runNum = 447\n",
      "Provided input from cache for runNum = 448\n",
      "activation = [[ 6.23137802  5.92741715  6.48762646 ... 12.11609351  5.09153271\n",
      "   3.69806181]\n",
      " [ 0.          3.46821829  0.         ...  0.03594374  0.\n",
      "   0.        ]\n",
      " [ 0.          3.50708113  0.74925688 ...  2.57442835  5.47527106\n",
      "   2.02410817]\n",
      " ...\n",
      " [ 0.          0.          0.14007111 ...  0.          0.\n",
      "   0.15649461]\n",
      " [ 3.47256641  3.70796013  3.54546402 ...  1.5606238   8.07309107\n",
      "   0.73136387]\n",
      " [ 5.12651295  9.30193471  0.         ...  4.49774071  2.77500963\n",
      "   3.30800286]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27483753 0.         1.77916868 ... 0.         1.08841749 0.        ]\n",
      " [2.32432269 7.81011621 1.86315099 ... 9.63291842 3.64812386 7.0426262 ]\n",
      " ...\n",
      " [7.81946222 8.79916504 6.40265927 ... 5.47097809 9.21988687 5.83856217]\n",
      " [4.83151985 0.0601796  7.92363274 ... 9.26029589 4.36904881 3.09711492]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.06949189e-08 3.62328448e-08 2.72631260e-05 ... 9.44968697e-02\n",
      "  4.75038245e-05 6.05963724e-03]\n",
      " [4.77714064e-02 4.80463132e-07 7.37682797e-01 ... 2.21248562e-04\n",
      "  7.17960978e-04 1.78644016e-03]\n",
      " [8.02326212e-04 3.45086434e-05 4.86029194e-02 ... 9.00458993e-03\n",
      "  2.06833616e-01 7.73209784e-03]\n",
      " ...\n",
      " [7.43840672e-01 2.33911747e-01 6.60208172e-03 ... 4.20153933e-03\n",
      "  1.12589205e-05 1.37866571e-02]\n",
      " [1.46771284e-02 5.07477060e-04 1.10858410e-01 ... 1.41068158e-01\n",
      "  1.45029613e-02 3.15234041e-02]\n",
      " [1.81021239e-01 3.47090927e-01 1.13052439e-02 ... 2.09081391e-02\n",
      "  1.30107320e-03 4.22481758e-01]] & cached\n",
      "Re-used Cached Value, runNum =  448\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  448\n",
      "Provided input from cache for runNum = 448\n",
      "Provided input from cache for runNum = 449\n",
      "activation = [[ 6.23510885  5.92889946  6.48868874 ... 12.12289609  5.09179634\n",
      "   3.69944363]\n",
      " [ 0.          3.46913393  0.         ...  0.0369525   0.\n",
      "   0.        ]\n",
      " [ 0.          3.5049886   0.74796987 ...  2.57686904  5.47780119\n",
      "   2.02512107]\n",
      " ...\n",
      " [ 0.          0.          0.14213777 ...  0.          0.\n",
      "   0.15858594]\n",
      " [ 3.47595671  3.71568848  3.55015744 ...  1.56521916  8.08098428\n",
      "   0.7354075 ]\n",
      " [ 5.1310813   9.30980285  0.         ...  4.50555117  2.77765312\n",
      "   3.31307443]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27409794 0.         1.78253937 ... 0.         1.09171108 0.        ]\n",
      " [2.32805463 7.81481569 1.86275721 ... 9.64694561 3.65337052 7.04964383]\n",
      " ...\n",
      " [7.82377373 8.81127413 6.40907677 ... 5.48188728 9.22904339 5.84897761]\n",
      " [4.83364697 0.05972617 7.92764716 ... 9.2656275  4.3700374  3.09741664]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.04270752e-08 3.56275208e-08 2.68106790e-05 ... 9.42648140e-02\n",
      "  4.72197526e-05 6.02803053e-03]\n",
      " [4.74325524e-02 4.75841421e-07 7.38620933e-01 ... 2.18595856e-04\n",
      "  7.11645555e-04 1.77847088e-03]\n",
      " [8.01124802e-04 3.44171791e-05 4.85320701e-02 ... 8.99405562e-03\n",
      "  2.05897315e-01 7.69629446e-03]\n",
      " ...\n",
      " [7.44595273e-01 2.33785378e-01 6.57168536e-03 ... 4.20607975e-03\n",
      "  1.10692623e-05 1.36971914e-02]\n",
      " [1.46718359e-02 5.08324740e-04 1.10355886e-01 ... 1.41052022e-01\n",
      "  1.44068321e-02 3.14979681e-02]\n",
      " [1.80670728e-01 3.47803880e-01 1.12462828e-02 ... 2.09744133e-02\n",
      "  1.29180135e-03 4.22430193e-01]] & cached\n",
      "Re-used Cached Value, runNum =  449\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  449\n",
      "Provided input from cache for runNum = 449\n",
      "Provided input from cache for runNum = 450\n",
      "activation = [[ 6.23882397  5.93037818  6.48974527 ... 12.12968055  5.09203584\n",
      "   3.70082207]\n",
      " [ 0.          3.4700695   0.         ...  0.03798105  0.\n",
      "   0.        ]\n",
      " [ 0.          3.50289345  0.74668824 ...  2.57930297  5.48032946\n",
      "   2.0261322 ]\n",
      " ...\n",
      " [ 0.          0.          0.1441975  ...  0.          0.\n",
      "   0.1606714 ]\n",
      " [ 3.47938844  3.72344315  3.55486074 ...  1.5698648   8.08889991\n",
      "   0.73945274]\n",
      " [ 5.13563564  9.31765193  0.         ...  4.51336602  2.78029986\n",
      "   3.31814194]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27355679 0.         1.78606483 ... 0.         1.09522486 0.        ]\n",
      " [2.33175484 7.81948556 1.86235532 ... 9.66094195 3.6585961  7.05665047]\n",
      " ...\n",
      " [7.82811042 8.82339046 6.41549448 ... 5.49283915 9.23822368 5.8593884 ]\n",
      " [4.83576357 0.05926358 7.93164201 ... 9.27091868 4.37099522 3.09771059]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[2.01582224e-08 3.50265799e-08 2.63604327e-05 ... 9.40203871e-02\n",
      "  4.69269312e-05 5.99607329e-03]\n",
      " [4.71015106e-02 4.71287927e-07 7.39582844e-01 ... 2.15995221e-04\n",
      "  7.05419911e-04 1.77058111e-03]\n",
      " [7.99909160e-04 3.43231031e-05 4.84578499e-02 ... 8.98364071e-03\n",
      "  2.04963737e-01 7.66027877e-03]\n",
      " ...\n",
      " [7.45339066e-01 2.33668495e-01 6.54126809e-03 ... 4.21129936e-03\n",
      "  1.08821627e-05 1.36090165e-02]\n",
      " [1.46641920e-02 5.09120268e-04 1.09836119e-01 ... 1.41032946e-01\n",
      "  1.43090409e-02 3.14708902e-02]\n",
      " [1.80326224e-01 3.48532025e-01 1.11875148e-02 ... 2.10436834e-02\n",
      "  1.28257381e-03 4.22390327e-01]] & cached\n",
      "Re-used Cached Value, runNum =  450\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  450\n",
      "Provided input from cache for runNum = 450\n",
      "Provided input from cache for runNum = 451\n",
      "activation = [[ 6.24251722  5.93182543  6.49079196 ... 12.13641274  5.09223697\n",
      "   3.70218588]\n",
      " [ 0.          3.47102887  0.         ...  0.03903748  0.\n",
      "   0.        ]\n",
      " [ 0.          3.50084332  0.74542452 ...  2.58178024  5.4828925\n",
      "   2.02716124]\n",
      " ...\n",
      " [ 0.          0.          0.14626475 ...  0.          0.\n",
      "   0.16277208]\n",
      " [ 3.48289868  3.73134578  3.55960206 ...  1.57466369  8.09691512\n",
      "   0.74356768]\n",
      " [ 5.14017848  9.32547202  0.         ...  4.52117982  2.78293795\n",
      "   3.32320156]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27308564 0.         1.78963661 ... 0.         1.09883028 0.        ]\n",
      " [2.33540089 7.82405197 1.86193744 ... 9.67484521 3.66376181 7.06360926]\n",
      " ...\n",
      " [7.83246183 8.83553138 6.421906   ... 5.50382362 9.24742481 5.86981235]\n",
      " [4.8378641  0.05877322 7.93562059 ... 9.27614748 4.37191907 3.09799874]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.98956876e-08 3.44385542e-08 2.59197273e-05 ... 9.37864657e-02\n",
      "  4.66294519e-05 5.96462546e-03]\n",
      " [4.67774791e-02 4.66787471e-07 7.40538333e-01 ... 2.13424364e-04\n",
      "  6.99268723e-04 1.76276216e-03]\n",
      " [7.98856164e-04 3.42355445e-05 4.83871139e-02 ... 8.97497849e-03\n",
      "  2.04028904e-01 7.62514211e-03]\n",
      " ...\n",
      " [7.46064407e-01 2.33509882e-01 6.51054168e-03 ... 4.21586463e-03\n",
      "  1.06974721e-05 1.35202542e-02]\n",
      " [1.46579106e-02 5.09917459e-04 1.09318586e-01 ... 1.41016630e-01\n",
      "  1.42111858e-02 3.14441210e-02]\n",
      " [1.79989732e-01 3.49230397e-01 1.11287358e-02 ... 2.11117014e-02\n",
      "  1.27334909e-03 4.22329763e-01]] & cached\n",
      "Re-used Cached Value, runNum =  451\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  451\n",
      "Provided input from cache for runNum = 451\n",
      "Provided input from cache for runNum = 452\n",
      "activation = [[ 6.24624488  5.93333858  6.49185859 ... 12.14319512  5.09248781\n",
      "   3.70357508]\n",
      " [ 0.          3.47197687  0.         ...  0.04008987  0.\n",
      "   0.        ]\n",
      " [ 0.          3.49868714  0.74411041 ...  2.58414784  5.48535614\n",
      "   2.02813689]\n",
      " ...\n",
      " [ 0.          0.          0.14832457 ...  0.          0.\n",
      "   0.16486405]\n",
      " [ 3.48635675  3.73916348  3.56430072 ...  1.5793942   8.10486084\n",
      "   0.74763811]\n",
      " [ 5.14469801  9.33326007  0.         ...  4.52899072  2.78555658\n",
      "   3.32825064]] & cached\n",
      "activation = [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [1.27247412 0.         1.79308873 ... 0.         1.10231296 0.        ]\n",
      " [2.33910114 7.82864923 1.86155311 ... 9.68878177 3.66894817 7.07057841]\n",
      " ...\n",
      " [7.83670865 8.84755059 6.42823979 ... 5.51471417 9.2565394  5.88016257]\n",
      " [4.84000593 0.05838526 7.9396113  ... 9.28142534 4.37292445 3.09832223]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] & cached\n",
      "activation = [[1.96334395e-08 3.38585286e-08 2.54862435e-05 ... 9.35307174e-02\n",
      "  4.63736484e-05 5.93289257e-03]\n",
      " [4.64464274e-02 4.62450440e-07 7.41489299e-01 ... 2.10903191e-04\n",
      "  6.94259991e-04 1.75509175e-03]\n",
      " [7.97371637e-04 3.41379710e-05 4.83015873e-02 ... 8.96206459e-03\n",
      "  2.03306789e-01 7.58843710e-03]\n",
      " ...\n",
      " [7.46815492e-01 2.33435570e-01 6.48108631e-03 ... 4.22131876e-03\n",
      "  1.05524713e-05 1.34337859e-02]\n",
      " [1.46501552e-02 5.10798630e-04 1.08810822e-01 ... 1.41000724e-01\n",
      "  1.41589219e-02 3.14181614e-02]\n",
      " [1.79640407e-01 3.50012058e-01 1.10711800e-02 ... 2.11818900e-02\n",
      "  1.26717100e-03 4.22292305e-01]] & cached\n",
      "Re-used Cached Value, runNum =  452\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  452\n",
      "Provided input from cache for runNum = 452\n"
     ]
    }
   ],
   "source": [
    "# accuracySum = 0\n",
    "# runCount = 0\n",
    "\n",
    "telementary = 0\n",
    "\n",
    "lastWeights = nt.output_layer.weights\n",
    "\n",
    "dataLen = len(X_train)\n",
    "\n",
    "maxIt = 450\n",
    "for it in range(maxIt):\n",
    "    Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "    predictedRAW = nt.backward_prop(input_values=(X_train), trueOutput=Y_train_oneHot)\n",
    "\n",
    "    if(it % 50 == 0):\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW[0][0])\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_train))\n",
    "        # newWeights = nt.output_layer.weights\n",
    "\n",
    "        # printMap((newWeights - lastWeights)*50)\n",
    "\n",
    "        # lastWeights = newWeights #nt.output_layer.weights\n",
    "\n",
    "\n",
    "# telementary = 1\n",
    "\n",
    "# # accuracy = accuracySum/len(X_train)\n",
    "# accuracy = accuracySum/runCount\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descending Telemetry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter\n",
      "\u001b[H\u001b[2Jiterations = 0\n",
      "Accuracy = 0.767\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Errors =  [ 29.46498105  28.91447629  76.83129843 100.78609081  74.28225226\n",
      " 101.76209255  40.75013621  67.22539337 106.27445414  96.42734211]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter\n",
      "\u001b[H\u001b[2Jiterations = 100\n",
      "Accuracy = 0.785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Errors =  [ 21.5509131   22.8594286   59.70421985 110.93903361  72.18402183\n",
      " 112.89369533  31.77607297  50.00988195 102.08429617  99.42618618]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter\n",
      "\u001b[H\u001b[2Jiterations = 200\n",
      "Accuracy = 0.817\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAy60lEQVR4nO3dfViUZaI/8O/wNog5vA0ICIOIiSOGCb4Eli+XZmtqFns6ZbAapp5aPavp8YX1FKaL2NYV7tW2CuuuqKjlOYu1tceX0NAMU0TJl9hRzEC5QNfEQTRHZe7fH/wYm2QQbB6e6eb7ua77qmfmvp/58jjw5ZkZZjRCCAEiIiKJuakdgIiISGksOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpKeYmV3+fJlpKSkQKfTwc/PDy+99BIaGhpaXTNy5EhoNBq78fLLLysVkYiIOgmNUu+NOW7cONTU1CAnJwe3bt1CWloaBg8ejM2bNztcM3LkSPTp0wfLli2zXebj4wOdTqdERCIi6iQ8lNhpeXk5duzYgZKSEgwaNAgA8O677+LJJ5/E22+/jbCwMIdrfXx8EBISokQsIiLqpBQpuwMHDsDPz89WdAAwZswYuLm54eDBg3jmmWccrt20aRPy8/MREhKCiRMn4rXXXoOPj4/D+RaLBRaLxbZttVpx+fJlBAYGQqPROOcLIiKiDiOEwNWrVxEWFgY3N+c826ZI2dXW1iI4ONj+hjw8EBAQgNraWofrXnjhBURGRiIsLAzHjh3DokWLYDKZUFBQ4HBNVlYW3njjDadlJyIi13Du3DmEh4c7ZV/tKrvFixfjzTffbHVOeXn5fYeZOXOm7f8feughhIaGYvTo0Thz5gyio6NbXJOeno558+bZts1mMwwGw31n6GwmTpyIwMBAtWMAAL777jt8/PHHeOWVV9CjRw+14wAAqqursXr1auTm5iImJkbtOAAAk8mEmTNnuuS/nSsep9deew2RkZFqxwEAVFZWYvny5S55nFxRt27dnLavdpXd/Pnz8eKLL7Y6p1evXggJCcHFixftLr99+zYuX77crufjhg4dCgCoqKhwWHZarRZarbbN+yR7oaGhCA0NVTsGAMDLywsAEB0d7fDfu6N5e3sDABISEhAfH69ymiYPPPAAANf8t3PF42Q0GmE0GlVO06T5KRlXPE6uyJlPRbWr7IKCghAUFHTPeYmJibhy5QpKS0uRkJAAANizZw+sVqutwNqirKwMAFzmG5qIiH6eFPk7O6PRiF/84heYMWMGDh06hC+++AKzZ8/G888/b3slZnV1Nfr27YtDhw4BAM6cOYPly5ejtLQU3377Lf7+979jypQpGD58OOLi4pSISUREnYRif1S+adMm9O3bF6NHj8aTTz6JRx99FLm5ubbrb926BZPJhOvXrwNoehiksLAQY8eORd++fTF//nz88pe/xMcff6xURCIi6iQUeTUmAAQEBLT6B+Q9e/bED/+ePSIiAnv37lUqDhERdWJ8b0wiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKSneNm999576NmzJ7y9vTF06FAcOnTI4dy8vDxoNBq74e3trXREIiKSnKJl98EHH2DevHnIyMjAkSNHMGDAADzxxBO4ePGiwzU6nQ41NTW2UVlZqWREIiLqBBQtu3feeQczZsxAWloa+vXrhzVr1sDHxwd//etfHa7RaDQICQmxje7duysZkYiIOgEPpXZ88+ZNlJaWIj093XaZm5sbxowZgwMHDjhc19DQgMjISFitVsTHx2PFihWIjY11ON9iscBisdi26+vrAQCjR49GcHCwE76Sn+7y5cvYuXOn2jFaNHz4cBiNRrVjAADKy8uRm5uLurq6Vs/+O1JdXR0AYO7cufD19VU5TROz2QwAuHTpkspJ7mjOkpCQoHKSu1mtVlitVrVjAIAthyvenzIzMxEVFaVymibl5eVYvny5c3cqFFJdXS0AiOLiYrvLFyxYIIYMGdLimuLiYrF+/Xpx9OhRUVRUJCZMmCB0Op04d+6cw9vJyMgQADg4ODg4JBtms9lpnaTYmd39SExMRGJiom07KSkJRqMROTk5Dls+PT0d8+bNs23X19cjIiKCZ3ZtlJ+f71JndqmpqViyZAkMBoPacQAAVVVVyMzMxGOPPeZSv4l//vnnSE5Ohl6vVzsOgKYzu4KCArVjtMgV7+OueH+S/cxOsbLT6/Vwd3fHhQsX7C6/cOECQkJC2rQPT09PDBw4EBUVFQ7naLVaaLXauy739/d3mbJzZUajEfHx8WrHsGMwGNCnTx+1Y9jx9fVFYGCg2jHs6PV6hIaGqh3D5bnifdwV709RUVEu80vB9evXnb5PxV6g4uXlhYSEBOzevdt2mdVqxe7du+3O3lrT2NiI48eP8xuaiIh+EkUfxpw3bx6mTp2KQYMGYciQIVi1ahWuXbuGtLQ0AMCUKVPQo0cPZGVlAQCWLVuGRx55BL1798aVK1fw1ltvobKyEtOnT1cyJhERSU7Rsnvuuefwr3/9C6+//jpqa2vx8MMPY8eOHbY/J6iqqoKb252Ty7q6OsyYMQO1tbXw9/dHQkICiouL0a9fPyVjEhGR5BR/gcrs2bMxe/bsFq8rKiqy287OzkZ2drbSkYiIqJPhe2MSEZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9Rctu3759mDhxIsLCwqDRaPDhhx+2Or+oqAgajeauUVtbq2RMIiKSnKJld+3aNQwYMADvvfdeu9aZTCbU1NTYRnBwsEIJiYioM/BQcufjxo3DuHHj2r0uODgYfn5+zg9ERESdkqJld78efvhhWCwW9O/fH0uXLsWwYcMczrVYLLBYLLbt+vp6AMDYsWORkJCgeNa2KC8vx5YtW5Cfnw+j0ah2HABNmVJTU13mGP1Q165dodPp1I4BoCkLAHzyyScqJ7nbpUuX1I5g40pZfswV7+OueH+yWq2wWq1qxwAAZXKIDgJAbNu2rdU5//znP8WaNWvE4cOHxRdffCHS0tKEh4eHKC0tdbgmIyNDAODg4ODgkGyYzWandZDm/xeR4jQaDbZt24ann366XetGjBgBg8GAjRs3tnh9S2d2ERERyM3NdZnf6JrPolzxzM4V8Ti1TXJyMvR6vdoxADSd2RUUFKgdg34CV/q+Ky0txcyZM2E2m532KI9LPoz5Q0OGDMH+/fsdXq/VaqHVau+6PCYmBvHx8UpGazej0ehymVwRj1Pb6PV6hIaGqh2DJOFK33cNDQ1O36fL/51dWVkZv6GJiOgnUfTMrqGhARUVFbbts2fPoqysDAEBATAYDEhPT0d1dTU2bNgAAFi1ahWioqIQGxuLGzduYO3atdizZw927dqlZEwiIpKcomV3+PBhjBo1yrY9b948AMDUqVORl5eHmpoaVFVV2a6/efMm5s+fj+rqavj4+CAuLg6FhYV2+yAiImovRctu5MiRaO31L3l5eXbbCxcuxMKFC5WMREREnZDLP2dHRET0U7HsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeoqWXVZWFgYPHoxu3bohODgYTz/9NEwmU6tr8vLyoNFo7Ia3t7eSMYmISHKKlt3evXsxa9YsfPnll/j0009x69YtjB07FteuXWt1nU6nQ01NjW1UVlYqGZOIiCTnoeTOd+zYYbedl5eH4OBglJaWYvjw4Q7XaTQahISEKBmNiIg6EUXL7sfMZjMAICAgoNV5DQ0NiIyMhNVqRXx8PFasWIHY2NgW51osFlgsFtt2fX09AMBkMuGBBx5wUvKfpry83O6/rqA5S5cuXeDu7q5ymiaNjY34/vvvkZCQoHaUn4VLly6pHcGmOcuSJUtgMBhUTtOkqqoKmZmZiIiIcJmnQm7cuIFz5865ZCZX+vl0r6e77odGCCGcvtcWWK1WPPXUU7hy5Qr279/vcN6BAwdw+vRpxMXFwWw24+2338a+fftw8uRJhIeH3zV/6dKleOONN5SMTkREKjCbzdDpdE7ZV4eV3SuvvILt27dj//79LZaWI7du3YLRaMTkyZOxfPnyu65v6cwuIiICubm5LnOGUF5ejtTUVOTn58NoNKodB8CdTK54Zkdtk5ycDL1er3YMAE1ndgUFBTyzuwdXPrNzpZ9PpaWlmDlzplPLrkMexpw9ezY++eQT7Nu3r11FBwCenp4YOHAgKioqWrxeq9VCq9XedXlMTAzi4+PvK69SjEajy2Vyd3d3mbKj9tHr9QgNDVU7hh2DwYA+ffqoHcOOt7c3fHx81I5hxxUzudLPp4aGBqfvU9FXYwohMHv2bGzbtg179uxBVFRUu/fR2NiI48ePu9w3NRER/XwoemY3a9YsbN68GR999BG6deuG2tpaAICvry+6dOkCAJgyZQp69OiBrKwsAMCyZcvwyCOPoHfv3rhy5QreeustVFZWYvr06UpGJSIiiSladqtXrwYAjBw50u7ydevW4cUXXwTQ9Li6m9udE8y6ujrMmDEDtbW18Pf3R0JCAoqLi9GvXz8loxIRkcQULbu2vPalqKjIbjs7OxvZ2dkKJSIios6I741JRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0FC271atXIy4uDjqdDjqdDomJidi+fbvD+Xl5edBoNHbD29tbyYhERNQJeCi58/DwcKxcuRIPPvgghBBYv349Jk2ahKNHjyI2NrbFNTqdDiaTybat0WiUjEhERJ2AomU3ceJEu+3MzEysXr0aX375pcOy02g0CAkJUTIWERF1MoqW3Q81Njbif/7nf3Dt2jUkJiY6nNfQ0IDIyEhYrVbEx8djxYoVDosRACwWCywWi227vr4eALBs2TIEBwc77wv4CS5fvgwASEhIUDnJ3dasWQOj0ah2DABAeXk5UlNTkZ+f73KZMjMzERUVpXYcAMDZs2exZMkSXLp0Se0oNs1ZqqqqVE5yR3OWX//61+jdu7fKaZpUVFTg1VdfRa9evRAQEKB2HABNP59Onz7tkj+fnEoo7NixY6Jr167C3d1d+Pr6in/84x8O5xYXF4v169eLo0ePiqKiIjFhwgSh0+nEuXPnHK7JyMgQADg4ODg4JBtms9lpXaQRQggo6ObNm6iqqoLZbMb//u//Yu3atdi7dy/69et3z7W3bt2C0WjE5MmTsXz58hbntHRmFxERgdGjR7vUmd3OnTvVjtEiVzyLcsVMrnhml5ycDL1er3YcAE1ndgUFBViyZAkMBoPacQA0ndllZmYiOzvb5c7snnjiCZc6s3PVn09msxk6nc4p+1L8YUwvLy/bHS0hIQElJSX4wx/+gJycnHuu9fT0xMCBA1FRUeFwjlarhVarvetyf39/lyk7V2Y0GhEfH692DDuumCkqKsplCriZXq9HaGio2jHsGAwG9OnTR+0Ydnr37o3+/furHcNOQEAAfz51sA7/Ozur1Wp3JtaaxsZGHD9+3OW+oYmI6OdF0TO79PR0jBs3DgaDAVevXsXmzZtRVFRkO2WeMmUKevTogaysLABNLyp55JFH0Lt3b1y5cgVvvfUWKisrMX36dCVjEhGR5BQtu4sXL2LKlCmoqamBr68v4uLisHPnTjz++OMAmh5Td3O7c3JZV1eHGTNmoLa2Fv7+/khISEBxcXGbnt8jIiJyRNGy+8tf/tLq9UVFRXbb2dnZyM7OVjARERF1RnxvTCIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpNdhZbdy5UpoNBrMnTvX4Zy8vDxoNBq74e3t3VERiYhIUh4dcSMlJSXIyclBXFzcPefqdDqYTCbbtkajUTIaERF1AoqXXUNDA1JSUvDnP/8Zv/vd7+45X6PRICQkpM37t1gssFgstu36+noAQF1dHTw9PdsfWAGXL19WO4JD5eXlakewac7iipnOnj2rcpI7mrNcunRJ5SR3NGepqqpSOckdzVkqKipUTnJHcxZX+pngSlkUJRQ2ZcoUMXfuXCGEECNGjBBz5sxxOHfdunXC3d1dGAwGER4eLp566ilx4sSJVvefkZEhAHBwcHBwSDbMZrPTukjRM7v3338fR44cQUlJSZvmx8TE4K9//Svi4uJgNpvx9ttvIykpCSdPnkR4eHiLa9LT0zFv3jzbdn19PSIiIpyS39ny8/NhNBrVjgGg6YwlNTWVme7BlTMlJydDr9erHQdA05ldQUGBSx4nV+SKx+mJJ55AQECA2nEAABcvXsTu3buduk/Fyu7cuXOYM2cOPv300za/yCQxMRGJiYm27aSkJBiNRuTk5GD58uUtrtFqtdBqtU7JrDSj0Yj4+Hi1Y9hhprZxxUx6vR6hoaFqx7DjisfJFbnicQoICEBwcLDaMQAAt27dcvo+FSu70tJSXLx40e4ftLGxEfv27cMf//hHWCwWuLu7t7oPT09PDBw40KUecyciop8fxcpu9OjROH78uN1laWlp6Nu3LxYtWnTPogOayvH48eN48sknlYpJRESdgGJl161bN/Tv39/usq5duyIwMNB2+ZQpU9CjRw9kZWUBAJYtW4ZHHnkEvXv3xpUrV/DWW2+hsrIS06dPVyomERF1Ah3yd3aOVFVVwc3tzt+119XVYcaMGaitrYW/vz8SEhJQXFyMfv36qZiSiIh+7jq07IqKilrdzs7ORnZ2dscFIiKiToHvjUlERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfQULbulS5dCo9HYjb59+zqcn5eXd9d8b29vJSMSEVEn4KH0DcTGxqKwsPDODXq0fpM6nQ4mk8m2rdFoFMtGRESdg+Jl5+HhgZCQkDbP12g07ZpvsVhgsVhs2/X19e3K15HKy8vVjmDTnIWZWufKmS5duqRykjuas7jicXJFrpStOcvly5dVTnJHXV2d83cqFJSRkSF8fHxEaGioiIqKEi+88IKorKx0OH/dunXC3d1dGAwGER4eLp566ilx4sSJe94GAA4ODg4OyYbZbHZaH2mEEAIK2b59OxoaGhATE4Oamhq88cYbqK6uxokTJ9CtW7e75h84cACnT59GXFwczGYz3n77bezbtw8nT55EeHh4i7fR0pldREQEXnzxRURGRir1pbVLTU0NcnNzERER4TLPQd64cQPnzp3DkiVLYDAY1I4DAKiqqkJmZiYee+wx+Pr6qh0HAGA2m/H5558jPz8fRqNR7TgAmn4TT01NRXJyMvR6vdpxADSd2RUUFLhkppSUFHTv3l3tOACACxcuYNOmTZg7d67Dn2kd7fz581i1ahUCAwPh6empdhwATT/X6+rqYDabodPpnLJPRR/GHDdunO3/4+LiMHToUERGRmLr1q146aWX7pqfmJiIxMRE23ZSUhKMRiNycnKwfPnyFm9Dq9VCq9XedXlISAh69uz5078IJ/L29oaPj4/aMewYDAb06dNH7Rh2fH19ERgYqHYMO0ajEfHx8WrHsKPX6xEaGqp2DDuumKl79+6IiIhQO4ad8PBwREdHqx3DjqenZ4s/S9VgtVqdvs8O/dMDPz8/9OnTBxUVFW2a7+npiYEDB7Z5PhERUUs6tOwaGhpw5syZNv/m19jYiOPHj7vcb4pERPTzomjZ/dd//Rf27t2Lb7/9FsXFxXjmmWfg7u6OyZMnAwCmTJmC9PR02/xly5Zh165d+Oabb3DkyBGkpqaisrIS06dPVzImERFJTtHn7M6fP4/Jkyfju+++Q1BQEB599FF8+eWXCAoKAtD0ggQ3tzt9W1dXhxkzZqC2thb+/v5ISEhAcXEx+vXrp2RMIiKSnKJl9/7777d6fVFRkd12dnY2srOzFUxERESdEd8bk4iIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6SladtXV1UhNTUVgYCC6dOmChx56CIcPH3Y4v6ioCBqN5q5RW1urZEwiIpKch1I7rqurw7BhwzBq1Chs374dQUFBOH36NPz9/e+51mQyQafT2baDg4OViklERJ2AYmX35ptvIiIiAuvWrbNdFhUV1aa1wcHB8PPza9Nci8UCi8Vi266vrwcA1NbWQqvVtj2wgmpqagAAN27cUDnJHc1ZqqqqVE5yR3MWs9mscpI7mrOUl5ernOSO5iyXLl1SOckdzVlcMdOFCxdUTnJHc5bz58+rnOSO5iy3bt1SOckdimQRCjEajWLu3Lni3/7t30RQUJB4+OGHRW5ubqtrPvvsMwFAREZGipCQEDFmzBixf//+VtdkZGQIABwcHBwckg2z2ey0TtIIIQQU4O3tDQCYN28enn32WZSUlGDOnDlYs2YNpk6d2uIak8mEoqIiDBo0CBaLBWvXrsXGjRtx8OBBxMfHt7impTO7iIgIZGVloX///s7/wu5DRUUFXn31VeTn58NoNKodB0DT2UFqaipyc3PRp08fteMAAE6dOoWZM2fiscceg6+vr9pxADSd2X3++edqx2hRcnIy9Hq92jEANJ1FFRQUIDAwEJ6enmrHAdB0dvDdd9+pHeNnIyUlBd27d1c7BoCms82tW7fCbDbbPaX1Uyj2MKbVasWgQYOwYsUKAMDAgQNx4sSJVssuJiYGMTExtu2kpCScOXMG2dnZ2LhxY4trtFptiw9X9urVy2XKrpnRaHRY2mrp06cPBgwYoHYMO76+vggMDFQ7hsvT6/UIDQ1VO4YdT09Pl3n6gNqne/fuiIiIUDsGANidwDiLYq/GDA0NRb9+/ewuMxqN7X6OaMiQIaioqHBmNCIi6mQUK7thw4bBZDLZXXbq1ClERka2az9lZWUu99srERH9vCj2MOarr76KpKQkrFixAv/+7/+OQ4cOITc3F7m5ubY56enpqK6uxoYNGwAAq1atQlRUFGJjY3Hjxg2sXbsWe/bswa5du5SKSUREnYBiZTd48GBs27YN6enpWLZsGaKiorBq1SqkpKTY5tTU1Ng9rHnz5k3Mnz8f1dXV8PHxQVxcHAoLCzFq1CilYhIRUSegWNkBwIQJEzBhwgSH1+fl5dltL1y4EAsXLlQyEhERdUJ8b0wiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKSnaNn17NkTGo3mrjFr1qwW5+fl5d0119vbW8mIRETUCXgoufOSkhI0Njbatk+cOIHHH38czz77rMM1Op0OJpPJtq3RaJSMSEREnYCiZRcUFGS3vXLlSkRHR2PEiBEO12g0GoSEhLT5NiwWCywWi227vr4eAPDNN9/Ax8ennYmVUVFRAQAoLy9XOckdzVlOnTqlcpI7mrOYzWaVk9zhSll+7NKlS2pHsGnOcuvWLZWT3OFKWX4OLly4oHYEm3/961/O36noIBaLRQQGBorMzEyHc9atWyfc3d2FwWAQ4eHh4qmnnhInTpxodb8ZGRkCAAcHBweHZMNsNjutgzRCCIEOsHXrVrzwwguoqqpCWFhYi3MOHDiA06dPIy4uDmazGW+//Tb27duHkydPIjw8vMU1LZ3ZRUREKPI1UOeVn58Po9GodgwATWflqampSE5Ohl6vVzsOgKYzu4KCApc8TszUuuZMrshsNkOn0zllX4o+jPlDf/nLXzBu3DiHRQcAiYmJSExMtG0nJSXBaDQiJycHy5cvb3GNVquFVqt1el6iHzIajYiPj1c7hh29Xo/Q0FC1Y9hxxePETAR0UNlVVlaisLAQBQUF7Vrn6emJgQMH2p7zIiIiuh8d8nd269atQ3BwMMaPH9+udY2NjTh+/LjL/fZKREQ/L4qXndVqxbp16zB16lR4eNifSE6ZMgXp6em27WXLlmHXrl345ptvcOTIEaSmpqKyshLTp09XOiYREUlM8YcxCwsLUVVVhWnTpt11XVVVFdzc7vRtXV0dZsyYgdraWvj7+yMhIQHFxcXo16+f0jGJiEhiipfd2LFj4egFn0VFRXbb2dnZyM7OVjoSERF1MnxvTCIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIumx7IiISHosOyIikh7LjoiIpKdY2TU2NuK1115DVFQUunTpgujoaCxfvhxCCIdrioqKoNFo7hq1tbVKxSQiok7AQ6kdv/nmm1i9ejXWr1+P2NhYHD58GGlpafD19cVvfvObVteaTCbodDrbdnBwsFIxiYioE1Cs7IqLizFp0iSMHz8eANCzZ09s2bIFhw4duufa4OBg+Pn5tel2LBYLLBaLbbu+vv6+8hK1pry8XO0INs1ZLl26pHKSO5qzuOJxYqbWuVIWRQmFZGZmisjISGEymYQQQpSVlYng4GCRn5/vcM1nn30mAIjIyEgREhIixowZI/bv39/q7WRkZAgAHBwcHBySDbPZ7LRO0gjRypNoP4HVasVvf/tb/P73v4e7uzsaGxuRmZmJ9PR0h2tMJhOKioowaNAgWCwWrF27Fhs3bsTBgwcRHx/f4pqWzuwiIiKQm5uLhIQEp39d96O8vBypqanIz8+H0WhUOw6AO5ni4+PRrVs3teMAAK5evYojR46oHeNnIzk5GXq9Xu0YAJrO7AoKCtSO0SJX/L5zRa50nEpLSzFz5kyYzWa7p7R+CsUexty6dSs2bdqEzZs3IzY2FmVlZZg7dy7CwsIwderUFtfExMQgJibGtp2UlIQzZ84gOzsbGzdubHGNVquFVqttcV+OClItRqPR5TJ169atzQ8Zk2vR6/UIDQ1VO4bLc8XvO1fkSsepoaHB6ftUrOwWLFiAxYsX4/nnnwcAPPTQQ6isrERWVpbDsmvJkCFDsH//fqViEhFRJ6DYnx5cv34dbm72u3d3d4fVam3XfsrKyvjbKxER/SSKndlNnDgRmZmZMBgMiI2NxdGjR/HOO+9g2rRptjnp6emorq7Ghg0bAACrVq1CVFQUYmNjcePGDaxduxZ79uzBrl27lIpJRESdgGJl9+677+K1117Dr3/9a1y8eBFhYWH4j//4D7z++uu2OTU1NaiqqrJt37x5E/Pnz0d1dTV8fHwQFxeHwsJCjBo1SqmYRETUCShWdt26dcOqVauwatUqh3Py8vLsthcuXIiFCxcqFYmIiDopvjcmERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSY9kREZH0WHZERCQ9lh0REUmPZUdERNJj2RERkfRYdkREJD2WHRERSY9lR0RE0mPZERGR9Fh2REQkPZYdERFJj2VHRETSU7Tsrl69irlz5yIyMhJdunRBUlISSkpKHM4vKiqCRqO5a9TW1ioZk4iIJOeh5M6nT5+OEydOYOPGjQgLC0N+fj7GjBmDr7/+Gj169HC4zmQyQafT2baDg4OVjElERJJTrOy+//57/O1vf8NHH32E4cOHAwCWLl2Kjz/+GKtXr8bvfvc7h2uDg4Ph5+fXptuxWCywWCy2bbPZDAAoKyu77+zOZjKZAAClpaVoaGhQOU2T5kxXrlzB7du3VU7TxFWOzc9FTU0Nbt68qXYMAMB3332ndgSHXPH7zhW50nFq/vkthHDeToVC6uvrBQBRWFhod/mwYcPEiBEjWlzz2WefCQAiMjJShISEiDFjxoj9+/e3ejsZGRkCAAcHBweHZOPMmTPOqiShEcKZ1WkvKSkJXl5e2Lx5M7p3744tW7Zg6tSp6N27d4u/4ZhMJhQVFWHQoEGwWCxYu3YtNm7ciIMHDyI+Pr7F2/jxmd2VK1cQGRmJqqoq+Pr6KvWltUt9fT0iIiJw7tw5u4dn1cRMbcNMbcNMbcNMbWM2m2EwGFBXV9fmR/nuRdHn7DZu3Ihp06ahR48ecHd3R3x8PCZPnozS0tIW58fExCAmJsa2nZSUhDNnziA7OxsbN25scY1Wq4VWq73rcl9fX5f5h2um0+mYqQ2YqW2YqW2YqW1cMZObm/NeQ6noqzGjo6Oxd+9eNDQ04Ny5czh06BBu3bqFXr16tXkfQ4YMQUVFhYIpiYhIdh3yd3Zdu3ZFaGgo6urqsHPnTkyaNKnNa8vKyhAaGqpgOiIikp2iD2Pu3LkTQgjExMSgoqICCxYsQN++fZGWlgYASE9PR3V1NTZs2AAAWLVqFaKiohAbG4sbN25g7dq12LNnD3bt2tXm29RqtcjIyGjxoU21MFPbMFPbMFPbMFPbdJZMir5AZevWrUhPT8f58+cREBCAX/7yl8jMzLS9cOTFF1/Et99+i6KiIgDA73//e+Tm5qK6uho+Pj6Ii4vD66+/jlGjRikVkYiIOgFFy46IiMgV8L0xiYhIeiw7IiKSHsuOiIikx7IjIiLpSVF2ly9fRkpKCnQ6Hfz8/PDSSy/d8w1NR44ceddHCb388sv3neG9995Dz5494e3tjaFDh+LQoUMO5+bl5d11297e3vd92y3Zt28fJk6ciLCwMGg0Gnz44Yetzlf645WysrIwePBgdOvWDcHBwXj66afv+aa4Sh+n1atXIy4uzvbOEYmJidi+fbtqeVqycuVKaDQazJ07V7VcS5cuvWv/ffv2VS1Ps+rqaqSmpiIwMBBdunTBQw89hMOHDzucr/R9vGfPni3uf9asWS3O74jj1NjYiNdeew1RUVHo0qULoqOjsXz58lbfYLkjPmpNjY9/U/Tv7DpKSkoKampq8Omnn+LWrVtIS0vDzJkzsXnz5lbXzZgxA8uWLbNt+/j43Nftf/DBB5g3bx7WrFmDoUOHYtWqVXjiiSdgMpkcfjyRTqez+2Gv0Wju67YduXbtGgYMGIBp06YhOTm5zeuU+nilvXv3YtasWRg8eDBu376N3/72txg7diy+/vprdO3a1eE6JY9TeHg4Vq5ciQcffBBCCKxfvx6TJk3C0aNHERsb2+F5fqykpAQ5OTmIi4u751ylc8XGxqKwsNC27eHR+o8OpfPU1dVh2LBhGDVqFLZv346goCCcPn0a/v7+91yr1H28pKQEjY2Ntu0TJ07g8ccfx7PPPutwjdLH6c0338Tq1auxfv16xMbG4vDhw0hLS4Ovry9+85vftLpWyY9aU+Xj35z2ltIq+frrrwUAUVJSYrts+/btQqPRiOrqaofrRowYIebMmeOUDEOGDBGzZs2ybTc2NoqwsDCRlZXV4vx169YJX19fp9x2WwAQ27Zta3VO8ydO1NXVdUimixcvCgBi7969Dud09HESQgh/f3+xdu1a1fNcvXpVPPjgg+LTTz+9531V6VwZGRliwIABbZ7fEcdp0aJF4tFHH23Xmo6+j8+ZM0dER0cLq9Xa4vUdcZzGjx8vpk2bZndZcnKySElJcbhG6eN0/fp14e7uLj755BO7y+Pj48WSJUsUy/SzfxjzwIED8PPzw6BBg2yXjRkzBm5ubjh48GCrazdt2gS9Xo/+/fsjPT0d169fb/ft37x5E6WlpRgzZoztMjc3N4wZMwYHDhxwuK6hoQGRkZGIiIjApEmTcPLkyXbfthIefvhhhIaG4vHHH8cXX3yh2O00f+5gQEBAq/M66jg1Njbi/fffx7Vr15CYmKh6nlmzZmH8+PF296vWKJ3r9OnTCAsLQ69evZCSkoKqqipV8/z973/HoEGD8OyzzyI4OBgDBw7En//85zat7Yj7+M2bN5Gfn49p06a1eram9HFKSkrC7t27cerUKQDAV199hf3792PcuHH3XKvUcbp9+zYaGxvvesi2S5cu2L9/v3KZ7rsmXURmZqbo06fPXZcHBQWJP/3pTw7X5eTkiB07dohjx46J/Px80aNHD/HMM8+0+/arq6sFAFFcXGx3+YIFC8SQIUNaXFNcXCzWr18vjh49KoqKisSECROETqcT586da/fttwXacGb3z3/+U6xZs0YcPnxYfPHFFyItLU14eHiI0tJSp+dpbGwU48ePF8OGDWt1Xkccp2PHjomuXbsKd3d34evrK/7xj3+omkcIIbZs2SL69+8vvv/+eyHEvR+FUDrX//3f/4mtW7eKr776SuzYsUMkJiYKg8Eg6uvrVckjhBBarVZotVqRnp4ujhw5InJycoS3t7fIy8tzuKYj7+MffPCBcHd3b/XRpY44To2NjWLRokVCo9EIDw8PodFoxIoVK1pd0xHHKTExUYwYMUJUV1eL27dvi40bNwo3N7cWf5Y7K5PLlt2iRYvu+cF+5eXl9112P7Z7924BQFRUVLQr5/2U3Y/dvHlTREdHi//+7/9u1223VVvKriXDhw8XqampTs/z8ssvi8jIyHZ/UytxnCwWizh9+rQ4fPiwWLx4sdDr9eLkyZOq5amqqhLBwcHiq6++sl3W3ofclb4/1dXVCZ1O5/Dh3o7I4+npKRITE+0u+8///E/xyCOPtGs/St3Hx44dKyZMmNCuNUocpy1btojw8HCxZcsWcezYMbFhwwYREBDQ6i8FLXH2caqoqBDDhw8XAIS7u7sYPHiwSElJEX379lUsk8u+QGX+/Pl48cUXW53Tq1cvhISE4OLFi3aX3759G5cvX0ZISEibb2/o0KEAgIqKCkRHR7d5nV6vh7u7Oy5cuGB3+YULF9p8+56enhg4cKDLfZTRkCFD7vmwQnvNnj0bn3zyCfbt24fw8PB2rVXiOHl5eaF3794AgISEBJSUlOAPf/gDcnJyVMlTWlqKixcv2n1YcWNjI/bt24c//vGPsFgscHd37/BcP+Tn54c+ffq0ef9K5AkNDUW/fv3sLjMajfjb3/7Wrv0ocR+vrKxEYWEhCgoK2rVOieO0YMECLF68GM8//zwA4KGHHkJlZSWysrIwderUNu/H2cep+ePfrl27hvr6eoSGhuK5555r98e/tSeTyz5nFxQUhL59+7Y6vLy8kJiYiCtXrth9IOyePXtgtVptBdYWZWVlANDujxPy8vJCQkICdu/ebbvMarVi9+7drT7380ONjY04fvy4y32UkTM/XkkIgdmzZ2Pbtm3Ys2cPoqKi2r2PjjhOVqsVFotFtTyjR4/G8ePHUVZWZhuDBg1CSkoKysrK7ll0SuX6oYaGBpw5c6bN+1ciz7Bhw+7605VTp04hMjKyXftR4iPE1q1bh+DgYIwfP75d65Q4TtevX7/rA1Dd3d1htVrbtR+lPmqtQz/+7T7OQF3OL37xCzFw4EBx8OBBsX//fvHggw+KyZMn264/f/68iImJEQcPHhRCNJ1CL1u2TBw+fFicPXtWfPTRR6JXr15i+PDh93X777//vtBqtSIvL098/fXXYubMmcLPz0/U1tYKIYT41a9+JRYvXmyb/8Ybb4idO3eKM2fOiNLSUvH8888Lb2/vNj981hZXr14VR48eFUePHhUAxDvvvCOOHj0qKisrhRBCLF68WPzqV7+yzc/OzhYffvihOH36tDh+/LiYM2eOcHNzE4WFhU7J88orrwhfX19RVFQkampqbOP69eu2OR19nBYvXiz27t0rzp49K44dOyYWL14sNBqN2LVrlyp5HPnxw5gdnWv+/PmiqKhInD17VnzxxRdizJgxQq/Xi4sXL6qSRwghDh06JDw8PERmZqY4ffq02LRpk/Dx8RH5+fm2OR19Hxei6Tkyg8EgFi1adNd1ahynqVOnih49eohPPvlEnD17VhQUFAi9Xi8WLlxom6PGcdqxY4fYvn27+Oabb8SuXbvEgAEDxNChQ8XNmzcVyyRF2X333Xdi8uTJ4oEHHhA6nU6kpaWJq1ev2q4/e/asACA+++wzIUTT8yLDhw8XAQEBQqvVit69e4sFCxYIs9l83xneffddYTAYhJeXlxgyZIj48ssvbdeNGDFCTJ061bY9d+5c29zu3buLJ598Uhw5cuS+b7slzS/V/fFozjF16lQxYsQI2/w333xTREdHC29vbxEQECBGjhwp9uzZ47Q8LWUBINatW2eb09HHadq0aSIyMlJ4eXmJoKAgMXr0aFvRqZHHkR+XXUfneu6550RoaKjw8vISPXr0EM8995zdc9tqHaePP/5Y9O/fX2i1WtG3b1+Rm5trd31H38eFEGLnzp0CgDCZTHddp8Zxqq+vF3PmzBEGg0F4e3uLXr16iSVLlgiLxWKbo8Zx+uCDD0SvXr2El5eXCAkJEbNmzRJXrlxRNBM/4oeIiKTnss/ZEREROQvLjoiIpMeyIyIi6bHsiIhIeiw7IiKSHsuOiIikx7IjIiLpseyIiEh6LDsiIpIey46IiKTHsiMiIun9P2apMrkx3frTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Errors =  [26.08281931 18.36188061 54.9485684  92.53860949 56.60054001 83.25771319\n",
      " 30.33878011 42.48569852 85.56266522 84.04713895]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG8AAAGiCAYAAADpxJi+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAblElEQVR4nO2dfVBU59mHfysKiGVBlO+PlY8IiIIKYoAM6EjSaGpMpomKWpAqNi1OIFJHeWeMilG0cdRM6iiWCooatanWph2CQUSLIF+KCkYqYEB3QDTC6moFXJ73j4ybrLC4q3uQ29zXDDOes8/znHu92OWchft3ZEIIAYYkg150Acyzw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/III5m8O3fuYP78+ZDL5bC1tcWiRYugVqv7nDNlyhTIZDKdrw8++ECqEskjk+qzzenTp6O5uRkZGRno6upCfHw8Jk2ahAMHDuidM2XKFIwePRppaWnafVZWVpDL5VKUSB8hAZcvXxYARHl5uXZfbm6ukMlkQqlU6p0XFRUlkpKSpCjppWSwFN8QJSUlsLW1RUhIiHZfdHQ0Bg0ahNLSUrz77rt65+7fvx/79u2Dk5MTZs6ciVWrVsHKykrv+I6ODnR0dGi3u7u7cefOHYwYMQIymcw0T+g5EULg3r17cHFxwaBBpvtJJYm8lpYWODg46B5o8GDY2dmhpaVF77x58+ZBoVDAxcUFFy9exIoVK1BbW4sjR47onZOeno61a9earHYpuX79Otzc3Ey2nlHyVq5ciU2bNvU55ttvv33mYpYsWaL997hx4+Ds7Ixp06ahvr4e3t7evc5JTU3FsmXLtNsqlQoeHh7w8vKCtbX1M9diSu7du4eGhgaT12OUvJSUFCxcuLDPMV5eXnByckJra6vO/kePHuHOnTtwcnIy+HiTJ08GANTV1emVZ2FhAQsLix77ra2tYWtra/Cx+gNTv40bJc/e3h729vZPHRcWFob29nZUVlYiODgYAFBQUIDu7m6tEEOoqqoCADg7OxtT5s8GSa7z/P398eabbyIhIQFlZWU4c+YMli5dirlz58LFxQUAoFQq4efnh7KyMgBAfX091q1bh8rKSnz33Xf45z//idjYWERGRiIwMFCKMskj2UX6/v374efnh2nTpmHGjBl47bXXsGvXLu3jXV1dqK2txYMHDwAA5ubmyM/PxxtvvAE/Pz+kpKTg17/+Nb766iupSiSPZBfpL4q7d+/CxsYGQUFBA+ZnXnt7Oy5cuACVSmXSDxz4s03CsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzCsDzC9Iu87du3Y9SoUbC0tMTkyZO1zSW9kZ2d3SNUwNLSsj/KJIfk8g4dOoRly5Zh9erVOHfuHIKCgvDLX/6yR//eT5HL5WhubtZ+NTY2Sl0mSSSXt2XLFiQkJCA+Ph5jxozBzp07YWVlhd27d+udI5PJ4OTkpP1ydHSUukySSNKT/pjOzk5UVlYiNTVVu2/QoEGIjo5GSUmJ3nlqtRoKhQLd3d2YOHEiNmzYgICAgF7HPhkocPfuXQDA7du3te1jLxqp6pBU3u3bt6HRaHq8chwdHXHlypVe5/j6+mL37t0IDAyESqXC5s2bER4ejpqaml6b8fUFCiiVStM8iQGMpPKehbCwMISFhWm3w8PD4e/vj4yMDKxbt67H+CcDBe7evQt3d3e4urr2GQHSnzx48ECSbyZJ5Y0cORJmZma4efOmzv6bN28aHCwwZMgQTJgwAXV1db0+ri9QwMLCYsDI02g0kqwr6QmLubk5goODceLECe2+7u5unDhxQufV1RcajQaXLl3iUIFekPxtc9myZYiLi0NISAhCQ0Oxbds23L9/H/Hx8QCA2NhYuLq6Ij09HQCQlpaGV199FT4+Pmhvb8enn36KxsZGLF68WOpSySG5vDlz5uDWrVv4+OOP0dLSgvHjx+Prr7/WnsQ0NTXpRDq1tbUhISEBLS0tGD58OIKDg1FcXIwxY8ZIXSo5XtpAgYGYgMSBAowWlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYlkcYSeWdPn0aM2fOhIuLC2QyGf7xj3/0Ob6wsLBHmIBMJuvzDs8/ZySVd//+fQQFBWH79u1GzautrdUJFHjytt3MD0jaJTR9+nRMnz7d6HkODg4D5q6TA5kB19YMAOPHj0dHRwfGjh2LNWvWICIiQu9YfYECISEhJr2h/PNw48YNNDQ0mH5h0U8AEEePHu1zzJUrV8TOnTtFRUWFOHPmjIiPjxeDBw8WlZWVeuesXr1aACDxpVKpTPp/2m/9eTKZDEePHsU777xj1LyoqCh4eHggJyen18d7e+W5u7tj9uzZA+qVd/jwYZP35w3It82fEhoaiqKiIr2P6wsUsLe3h7u7u5SlGcxPv7lMyYC/zquqquIwAT1I+spTq9U6ERzXrl1DVVUV7Ozs4OHhgdTUVCiVSuzduxcAsG3bNnh6eiIgIAAPHz5EZmYmCgoKcPz4cSnLJIuk8ioqKjB16lTt9uOwm7i4OGRnZ6O5uRlNTU3axzs7O5GSkgKlUgkrKysEBgYiPz9fZw3mR17aQIHExET4+Pi86HIAAHV1ddi+fTsHCjA/wvIIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/IIw/III6m89PR0TJo0CdbW1nBwcMA777yD2traPudkZ2f3CBSwtLSUskyySCrv1KlTSExMxNmzZ/HNN9+gq6sLb7zxBu7fv9/nPLlcrhMo0NjYKGWZZJG0S+jrr7/W2c7OzoaDgwMqKysRGRmpd55MJjP4bs4/Z/q1M1alUgEA7Ozs+hynVquhUCjQ3d2NiRMnYsOGDQgICOh1rL5AgdGjRyMoKMhElT8fP70nrkkxaYd7H2g0GvHWW2+JiIiIPscVFxeLPXv2iPPnz4vCwkLxq1/9SsjlcnH9+vVex3OgQD/w+9//Hrm5uSgqKjKq0b+rqwv+/v6IiYnBunXrejyuL1Dgs88+GzCvvAsXLiApKYlmoMDSpUvxr3/9C6dPnzY6oWHIkCGYMGGCTnv0T9EXKODj4zNg5D3tBO1ZkfRsUwiBpUuX4ujRoygoKICnp6fRa2g0Gly6dIlDBXpB0ldeYmIiDhw4gGPHjsHa2lobAGdjY4OhQ4cCAGJjY+Hq6or09HQAQFpaGl599VX4+Pigvb0dn376KRobG7F48WIpSyWJpPJ27NgBAJgyZYrO/qysLCxcuBAA0NTUpHM21tbWhoSEBLS0tGD48OEIDg5GcXExxowZI2WpJJFUniHnQoWFhTrbW7duxdatWyWq6OWCP9skDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjDMsjjKTyduzYgcDAQMjlcsjlcoSFhSE3N1fveA4TMA5JexXc3NywceNGvPLKKxBCYM+ePZg1axbOnz+vt01ZLpfrJEbIZDIpSySNpPJmzpyps71+/Xrs2LEDZ8+e1SuPwwQMp98CBTQaDf72t7/h/v37CAsL0zvOmDABQH+gwJ49e3DixAnTPYHn4MaNG9IsbNIO9164ePGiGDZsmDAzMxM2Njbi3//+t96xxoYJCMGBApIGCnR2dqKpqQkqlQpffvklMjMzcerUKYOaJZ8WJgDoDxSYPXu20f3vUnHjxg0cPnyYXqCAubm59q7JwcHBKC8vx2effYaMjIynzn1amACgP1DA3t4e7u7uz164CfnpN5cp6ffrvO7uboOfDIcJ9I2kr7zU1FRMnz4dHh4euHfvHg4cOIDCwkLk5eUB4DCB50VSea2trYiNjUVzczNsbGwQGBiIvLw8vP766wA4TOB56bcEpP7i7t27sLGxQWJiovZn7Yumrq4O27dvN/kJC3+2SRiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWRxiWR5h+k7dx40bIZDIkJyfrHcOBAsbRL23N5eXlyMjIQGBg4FPHcqCA4UguT61WY/78+fjLX/6CTz755KnjjQ0U0NeTfuvWrV6bLl8Et27dkmZhkzZJ90JsbKxITk4WQggRFRUlkpKS9I7NysoSZmZmwsPDQ7i5uYm3335bVFdX97n+z7knXdJX3sGDB3Hu3DmUl5cbNN7X1xe7d+9GYGAgVCoVNm/ejPDwcNTU1OjtL09NTcWyZcu024970tPT0zF27FiTPI/npbq6GqmpqSZfVzJ5169fR1JSEr755huDTzrCwsJ0Yj7Cw8Ph7++PjIwMvYEC+nrSvby8Boy8Bw8eSLKuZPIqKyvR2tqKiRMnavdpNBqcPn0af/7zn9HR0QEzM7M+1zAkUODnjGTypk2bhkuXLunsi4+Ph5+fH1asWPFUccCPgQIzZsyQqkzSSCbP2tq6x9vWsGHDMGLECO1+DhR4Pvotvqo3OFDg+XhpAwUOHTqE0NDQF10OAKCsrAxz5szhQAHmR1geYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYVgeYSSVt2bNmh4BAX5+fnrHc6CAcUjeaBIQEID8/PwfDzi470NyoIDhSC5v8ODBRgUEmCpQoKGhAVZWVoYXKiENDQ3SLGzSDvcnWL16tbCyshLOzs7C09NTzJs3TzQ2Nuodz4ECxiFpi1dubi7UajV8fX3R3NyMtWvXQqlUorq6GtbW1j3Gl5SU4OrVqzqBAqdPn+4zUEDfTe6DgoJga2sr1VMzivb2dly4cMHkLV792p/X3t4OhUKBLVu2YNGiRU8d39XVBX9/f8TExOgNFHiSx/15ERERGDly5POWbBJu376NM2fO0O7Ps7W1xejRow0OCOBAgb7pV3lqtRr19fVwdnY2aPzjQAFDx//ckFTeH//4R5w6dQrfffcdiouL8e6778LMzAwxMTEAfggU+Gm4TFpaGo4fP46GhgacO3cOCxYs4ECBPpD0UuHGjRuIiYnB999/D3t7e7z22ms4e/Ys7O3tAXCgwPPy0gYK8AkLM6BheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYRheYSRXJ5SqcSCBQswYsQIDB06FOPGjUNFRYXe8YWFhT1CBWQyGVpaWqQulRySNpq0tbUhIiICU6dORW5uLuzt7XH16lUMHz78qXNra2t1/q7fwcFBylJJIqm8TZs2wd3dHVlZWdp9np6eBs11cHAwqC1ZX6CAWq1+avJEf6FWq6VZ2KQd7k/g7+8vkpOTxXvvvSfs7e3F+PHjxa5du/qcc/LkSQFAKBQK4eTkJKKjo0VRUZHe8RwoIBGPA3CWLVuG999/H+Xl5UhKSsLOnTsRFxfX65za2loUFhYiJCQEHR0dyMzMRE5ODkpLS3Xuuf4YfYEC06ZNGzBvta2trThx4gStQAFzc3OEhISguLhYu+/DDz9EeXk5SkpKDF4nKioKHh4eyMnJeerYx/157733HlxdXZ+pblOjVCrx5Zdf0urPc3Z27tHV6u/vj6amJqPWCQ0N5VCBXpBUXkREhE4UFQD897//hUKhMGqdqqoqDhXoBUlPxz766COEh4djw4YNmD17NsrKyrBr1y7s2rVLOyY1NRVKpRJ79+4FAGzbtg2enp4ICAjAw4cPkZmZiYKCAhw/flzKUkkiqbxJkybh6NGjSE1NRVpaGjw9PbFt2zbMnz9fO6a5uVnnbbSzsxMpKSlQKpWwsrJCYGAg8vPzMXXqVClLJclLGyjAJyzMgIblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEYblEUZSeaNGjeo1HCAxMbHX8dnZ2T3GPm7QZHoiaaNJeXk5NBqNdru6uhqvv/463n//fb1z5HK5TluYTCaTskTSSCrv8e1FH7Nx40Z4e3sjKipK7xyZTAYnJyeDj6EvUKCtrQ1DhgwxsmJpaGtrk2Zhk3a490FHR4cYMWKEWL9+vd4xWVlZwszMTHh4eAg3Nzfx9ttvi+rq6j7X5UCBfuDw4cOYN28empqa4OLi0uuYkpISXL16FYGBgVCpVNi8eTNOnz6NmpoauLm59TpHX6BAeno6xo4dK8lzMZbq6mqkpqaavMWr34JK/vrXv2L69Ol6xQFAWFgYwsLCtNvh4eHw9/dHRkYG1q1b1+scCwsLWFhY9Njv5eU1YOQ9ePBAknX7RV5jYyPy8/Nx5MgRo+YNGTIEEyZM4DABPfTLdV5WVhYcHBzw1ltvGTVPo9Hg0qVLHCagB8nldXd3IysrC3FxcT3ipGJjY5GamqrdTktLw/Hjx9HQ0IBz585hwYIFaGxsxOLFi6UukySSv23m5+ejqakJv/3tb3s81tTUhEGDfvz+aWtrQ0JCAlpaWjB8+HAEBwejuLi4R5YL8wMvbaDAoUOHEBoa+qLLAQCUlZVhzpw5HCjA/AjLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLIwzLI4yk8jQaDVatWgVPT08MHToU3t7eWLduHfr6C/vCwsJeQwhaWlqkLJUkkt/kfseOHdizZw8CAgJQUVGB+Ph42NjY4MMPP+xzbm1trc7f9Q+U22YPJCSVV1xcjFmzZmn78kaNGoUvvvgCZWVlT53r4OAAW1vbp47TFyjQ0NAAKyurZyvcxDQ0NEizsEk73J9g/fr1QqFQiNraWiGEEFVVVcLBwUHs27dP75yTJ08KAEKhUAgnJycRHR0tioqK9I7nQAGJ6O7uxv/93//hT3/6E8zMzKDRaLB+/Xqdhsonqa2tRWFhIUJCQtDR0YHMzEzk5OSgtLQUEydO7DGeAwUk4vDhw9i/fz8OHDiAgIAAVFVVITk5GS4uLoiLi+t1jq+vL3x9fbXb4eHhqK+vx9atW5GTk9NjPAcKSMTy5cuxcuVKzJ07FwAwbtw4NDY2Ij09Xa+83ggNDUVRUZFUZZJF0kuFBw8e6LQtA4CZmRm6u7uNWqeqqopDBXpB0lfezJkzsX79enh4eCAgIADnz5/Hli1bdPrTU1NToVQqsXfvXgDAtm3b4OnpiYCAADx8+BCZmZkoKCjA8ePHpSyVJJLK+/zzz7Fq1Sr84Q9/QGtrK1xcXPC73/0OH3/8sXZMc3MzmpqatNudnZ1ISUmBUqmElZUVAgMDkZ+fj6lTp0pZKkk4UKAf4EABpgcsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzAsjzCSyrt37x6Sk5OhUCgwdOhQhIeHo7y8XO94DhMwDkkbTRYvXozq6mrk5OTAxcUF+/btQ3R0NC5fvgxXV1e98zhMwDAkk/e///0Pf//733Hs2DFERkYCANasWYOvvvoKO3bswCeffKJ3rqFhAkDPtmaVSgUAuHz58rMXb2Ie12Lynh6Tdrj/hLt37woAIj8/X2d/RESEiIqK6nWOsWECQtAKFKivrzfVf68QQuJAgfDwcJibm+PAgQNwdHTEF198gbi4OPj4+KC2trbHeGPDBICer7z29nYoFAo0NTXBxsZGqqdmFCqVCh4eHmhrazP4HcUgTPqt8AR1dXUiMjJSABBmZmZi0qRJYv78+cLPz8/gNSIjI8WCBQsMHq9SqSSJzXgepKpJ0rNNb29vnDp1Cmq1GtevX0dZWRm6urrg5eVl8BqhoaGoq6uTsEq69Mt13rBhw+Ds7Iy2tjbk5eVh1qxZBs/lMAH9SHqpkJeXByEEfH19UVdXh+XLl8PPzw/x8fEApAkTsLCwwOrVq3vNZnlRSFaTSd+En+DQoUPCy8tLmJubCycnJ5GYmCja29u1j8fFxemceW7atEl4e3sLS0tLYWdnJ6ZMmSIKCgqkLJE0L12gwM8J/myTMCyPMCyPMCyPMC+FvDt37mD+/PmQy+WwtbXFokWLoFar+5wzZcqUHr96+uCDD565hu3bt2PUqFGwtLTE5MmT+0zzzc7O7nFsS0tL4w/6ok93TcGbb74pgoKCxNmzZ8V//vMf4ePjI2JiYvqcExUVJRISEkRzc7P261k/vjp48KAwNzcXu3fvFjU1NSIhIUHY2tqKmzdv9jo+KytLyOVynWO3tLQYfVzy8i5fviwAiPLycu2+3NxcIZPJhFKp1DsvKipKJCUlmaSG0NBQkZiYqN3WaDTCxcVFpKen9zo+KytL2NjYPPdxyb9tlpSUwNbWFiEhIdp90dHRGDRoEEpLS/ucu3//fowcORJjx45FamrqMyXSdnZ2orKyEtHR0dp9gwYNQnR0NEpKSvTOU6vVUCgUcHd3x6xZs1BTU2P0sSX9eKw/aGlp6fGb9sGDB8POzq7PP5+YN28eFAoFXFxccPHiRaxYsQK1tbU4cuSIUce/ffs2NBoNHB0ddfY7OjriypUrvc7x9fXF7t27ERgYCJVKhc2bNyM8PBw1NTVwc3Mz+NgDVt7KlSuxadOmPsd8++23z7z+kiVLtP8eN24cnJ2dMW3aNNTX18Pb2/uZ1zWEsLAwhIWFabfDw8Ph7++PjIwMrFu3zuB1Bqy8lJQULFy4sM8xXl5ecHJyQmtrq87+R48e4c6dO3BycjL4eJMnTwYA1NXVGSVv5MiRMDMzw82bN3X237x50+DjDxkyBBMmTDD+V1/P/VPzBfP4hKWiokK7Ly8v76knLE9SVFQkAIgLFy4YXUNoaKhYunSpdluj0QhXV1e9JyxP8ujRI+Hr6ys++ugjo45LXp4QP1wqTJgwQZSWloqioiLxyiuv6Fwq3LhxQ/j6+orS0lIhxA+/4U9LSxMVFRXi2rVr4tixY8LLy0tERkY+0/EPHjwoLCwsRHZ2trh8+bJYsmSJsLW11Z7+/+Y3vxErV67Ujl+7dq3Iy8sT9fX1orKyUsydO1dYWlqKmpoao477Usj7/vvvRUxMjPjFL34h5HK5iI+PF/fu3dM+fu3aNQFAnDx5UgghRFNTk4iMjBR2dnbCwsJC+Pj4iOXLlz/Xnyl8/vnnwsPDQ5ibm4vQ0FBx9uxZ7WNRUVEiLi5Ou52cnKwd6+joKGbMmCHOnTtn9DH5V0KEIX+d93OG5RGG5RGG5RGG5RGG5RGG5RGG5RGG5RGG5RGG5RHm/wH/NsE9neI/lwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter\n",
      "\u001b[H\u001b[2Jiterations = 300\n",
      "Accuracy = 0.822\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Errors =  [23.28639034 20.98431874 54.49065598 85.73512159 59.26398318 82.15889869\n",
      " 28.68230096 49.15539537 83.24294461 82.08557467]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter\n",
      "\u001b[H\u001b[2Jiterations = 400\n",
      "Accuracy = 0.85\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Errors =  [22.35877024 17.36661863 51.99764755 76.44649432 48.56211501 70.9773525\n",
      " 31.21180644 35.05675782 69.15276049 70.33787029]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracySum = 0\n",
    "# runCount = 0\n",
    "\n",
    "telementary = 0\n",
    "lastWeights = nt.output_layer.weights\n",
    "\n",
    "dataLen = len(X_train[0])\n",
    "batchSize = 1000\n",
    "maxIt = 450\n",
    "for it in range(maxIt):\n",
    "    spliceRange = [(batchSize*it)%dataLen, batchSize*(it+1)%dataLen]    ## [start, end]\n",
    "\n",
    "    if(spliceRange[0] >= spliceRange[1]):\n",
    "        # spliceRange[0] = dataLen - spliceRange[0]\n",
    "        # if(spliceRange[0] >= spliceRange[1]):\n",
    "        spliceRange[0] = 0\n",
    "        spliceRange[1] = batchSize\n",
    "\n",
    "    ## Verifying batch size\n",
    "    X_Train_Batch = X_train.T[spliceRange[0]:spliceRange[1]].T\n",
    "    Y_Train_Batch = Y_train[spliceRange[0]:spliceRange[1]]\n",
    "    if(telementary==2): print(\"SPLICE:\", spliceRange)\n",
    "    if(telementary==2): print(\"X size:\", X_Train_Batch.shape)\n",
    "    if(telementary==2): print(\"Y size:\", Y_Train_Batch.shape)\n",
    "    if(X_Train_Batch.shape[1] == 0):\n",
    "        print(\"error no X: spliceRange\", spliceRange)\n",
    "\n",
    "    Y_train_oneHot = one_hot(Y_Train_Batch, maxExpected=9)\n",
    "    predictedRAW = nt.backward_prop(input_values=(X_Train_Batch), trueOutput=Y_train_oneHot)\n",
    "\n",
    "    if(it % 100 == 0):\n",
    "        print(\"Please press enter\")\n",
    "        pauser = input(\"Press enter for next iteration.\")\n",
    "        clearScreen()\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW[0][0])\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_Train_Batch))\n",
    "        # print(\"Weights of output:\")\n",
    "        # print(nt.output_layer.weights)\n",
    "        newWeights = nt.output_layer.weights\n",
    "\n",
    "        # print weight changes made\n",
    "        printMap((newWeights - lastWeights)*50)\n",
    "\n",
    "        finalMAE = np.sum(np.absolute(predictedRAW[1]), axis=1) ## MAE of output activation errors over last ran single batch\n",
    "\n",
    "        print(\"Activation Errors = \", finalMAE)\n",
    "        printMap((finalMAE[:, np.newaxis]/100) - 0.5, sizes=[1,10])\n",
    "\n",
    "        lastWeights = newWeights #nt.output_layer.weights\n",
    "\n",
    "\n",
    "# telementary = 1\n",
    "\n",
    "# # accuracy = accuracySum/len(X_train)\n",
    "# accuracy = accuracySum/runCount\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with Netwok 784-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = nnetwork(784, 10, insertDefault=0)\n",
    "n1.addLayerAtLast(40,isDynamic=1,activationFn=\"relu\")\n",
    "n1.addLayerAtLast(10,isDynamic=1,activationFn=\"relu\")\n",
    "n1.output_layer.activationFn = \"softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(n1.output_layer.weights.shape)\n",
    "# print(n1.output_layer.input_layers[0].weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache for runNum = 0\n",
      "Input values should be a 2D array.\n",
      "activation = [[0.        ]\n",
      " [2.9916857 ]\n",
      " [0.        ]\n",
      " [3.41190866]\n",
      " [0.        ]\n",
      " [1.63375024]\n",
      " [1.86181064]\n",
      " [1.53379836]\n",
      " [2.44559512]\n",
      " [0.        ]\n",
      " [2.4896451 ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.34338224]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.03607738]\n",
      " [1.30454781]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.76392279]\n",
      " [0.        ]\n",
      " [6.29498297]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.07445885]\n",
      " [0.        ]\n",
      " [6.01667431]\n",
      " [3.13601822]\n",
      " [0.        ]\n",
      " [1.48490926]\n",
      " [0.99568503]\n",
      " [0.        ]\n",
      " [1.42662526]\n",
      " [3.15315278]\n",
      " [0.        ]\n",
      " [2.96703067]] & cached\n",
      "activation = [[0.        ]\n",
      " [1.98936641]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [6.65770714]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [9.20185923]\n",
      " [7.67072094]\n",
      " [7.92496732]] & cached\n",
      "activation = [[7.04880609e-05]\n",
      " [2.35781675e-04]\n",
      " [4.22366523e-05]\n",
      " [9.21687142e-10]\n",
      " [4.50151094e-04]\n",
      " [9.85376763e-01]\n",
      " [4.25853695e-06]\n",
      " [1.38131551e-02]\n",
      " [3.41332322e-09]\n",
      " [7.16182733e-06]] & cached\n",
      "[[7.04880609e-05]\n",
      " [2.35781675e-04]\n",
      " [4.22366523e-05]\n",
      " [9.21687142e-10]\n",
      " [4.50151094e-04]\n",
      " [9.85376763e-01]\n",
      " [4.25853695e-06]\n",
      " [1.38131551e-02]\n",
      " [3.41332322e-09]\n",
      " [7.16182733e-06]]\n"
     ]
    }
   ],
   "source": [
    "out = (n1.forward_prop(X_train.T[0].T))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache for runNum = 2\n",
      "activation = [[0.        ]\n",
      " [2.9916857 ]\n",
      " [0.        ]\n",
      " [3.41190866]\n",
      " [0.        ]\n",
      " [1.63375024]\n",
      " [1.86181064]\n",
      " [1.53379836]\n",
      " [2.44559512]\n",
      " [0.        ]\n",
      " [2.4896451 ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.34338224]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.03607738]\n",
      " [1.30454781]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.76392279]\n",
      " [0.        ]\n",
      " [6.29498297]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [4.07445885]\n",
      " [0.        ]\n",
      " [6.01667431]\n",
      " [3.13601822]\n",
      " [0.        ]\n",
      " [1.48490926]\n",
      " [0.99568503]\n",
      " [0.        ]\n",
      " [1.42662526]\n",
      " [3.15315278]\n",
      " [0.        ]\n",
      " [2.96703067]] & cached\n",
      "activation = [[0.        ]\n",
      " [1.98936641]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [6.65770714]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [9.20185923]\n",
      " [7.67072094]\n",
      " [7.92496732]] & cached\n",
      "activation = [[7.04880609e-05]\n",
      " [2.35781675e-04]\n",
      " [4.22366523e-05]\n",
      " [9.21687142e-10]\n",
      " [4.50151094e-04]\n",
      " [9.85376763e-01]\n",
      " [4.25853695e-06]\n",
      " [1.38131551e-02]\n",
      " [3.41332322e-09]\n",
      " [7.16182733e-06]] & cached\n",
      "Re-used Cached Value, runNum =  2\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  2\n",
      "Provided input from cache for runNum = 2\n"
     ]
    }
   ],
   "source": [
    "predRaw = n1.backward_prop(input_values=X_train.T[0:1].T, trueOutput=one_hot(Y_train[0:1], maxExpected=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache for runNum = 453\n",
      "activation = [[0.         3.19359189 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08314149 0.         ... 0.         0.         0.        ]\n",
      " [0.         2.36289087 2.60708641 ... 2.17205416 0.         0.        ]\n",
      " ...\n",
      " [1.57041935 0.         4.05998458 ... 0.07499223 1.45345901 5.61164976]\n",
      " [0.         0.36930347 0.         ... 0.         2.99686682 1.44734996]\n",
      " [2.87316367 4.87721114 2.37247782 ... 3.5457885  0.         0.        ]] & cached\n",
      "activation = [[ 3.62707353  2.03540834  0.         ...  2.47817527  1.2373401\n",
      "   7.73234434]\n",
      " [ 3.00948017  2.44218422  3.97616488 ...  8.83450879  3.4065443\n",
      "   4.40396014]\n",
      " [ 0.5263285   5.96626725  1.84581033 ...  4.9584489   0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 8.89927779  0.73326342  8.66549539 ...  6.60894747 19.45523364\n",
      "   9.60252296]\n",
      " [ 1.49257645  0.          0.12576186 ...  0.64990569 11.76872827\n",
      "   2.88631439]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] & cached\n",
      "activation = [[1.51401366e-02 1.63643611e-04 4.21573684e-01 ... 1.17486985e-03\n",
      "  2.08166410e-04 9.70813066e-01]\n",
      " [3.44567283e-05 6.16438501e-03 8.62337810e-05 ... 1.13247379e-04\n",
      "  1.70301681e-07 4.96012114e-05]\n",
      " [4.65512757e-02 9.40241300e-01 8.41296002e-02 ... 1.67116999e-02\n",
      "  1.27288520e-05 1.19991979e-03]\n",
      " ...\n",
      " [1.56498596e-05 1.71362049e-05 5.23106738e-05 ... 3.45384730e-05\n",
      "  4.86480161e-02 1.04325777e-02]\n",
      " [7.36224136e-01 6.13547808e-03 3.91827172e-01 ... 1.42563912e-02\n",
      "  2.64274399e-03 5.25027072e-03]\n",
      " [7.84552920e-04 1.18553758e-06 2.01824908e-05 ... 5.10591925e-06\n",
      "  9.42051228e-01 1.83731240e-04]] & cached\n",
      "Re-used Cached Value, runNum =  453\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  453\n",
      "Provided input from cache for runNum = 453\n",
      "iterations = 0\n",
      "Accuracy = 0.8050487804878049\n",
      "Provided input from cache for runNum = 454\n",
      "activation = [[0.         3.19565403 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08349727 0.         ... 0.         0.         0.        ]\n",
      " [0.         2.36514926 2.60812045 ... 2.17316518 0.         0.        ]\n",
      " ...\n",
      " [1.5701419  0.         4.06109978 ... 0.0749967  1.45431641 5.61596603]\n",
      " [0.         0.36981422 0.         ... 0.         2.99950854 1.44837979]\n",
      " [2.87536205 4.87867936 2.3744862  ... 3.54758392 0.         0.        ]] & cached\n",
      "activation = [[ 3.63000307  2.03640131  0.         ...  2.48712136  1.24324709\n",
      "   7.75004207]\n",
      " [ 3.00882883  2.43857799  3.97562647 ...  8.84479534  3.40849485\n",
      "   4.40600807]\n",
      " [ 0.53579401  5.97603856  1.85080596 ...  4.96201851  0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 8.90785369  0.73906061  8.67797332 ...  6.621092   19.46855741\n",
      "   9.60599239]\n",
      " [ 1.50235013  0.          0.13512525 ...  0.65512747 11.78682236\n",
      "   2.89023743]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] & cached\n",
      "activation = [[1.50408124e-02 1.61697277e-04 4.19863368e-01 ... 1.16654628e-03\n",
      "  2.07385004e-04 9.71099448e-01]\n",
      " [3.40625782e-05 6.15888814e-03 8.53005171e-05 ... 1.12964749e-04\n",
      "  1.69045249e-07 4.88998522e-05]\n",
      " [4.67159096e-02 9.40438173e-01 8.43825126e-02 ... 1.65568563e-02\n",
      "  1.27504551e-05 1.19211502e-03]\n",
      " ...\n",
      " [1.56348522e-05 1.70932891e-05 5.23381142e-05 ... 3.43644114e-05\n",
      "  4.88164252e-02 1.02944871e-02]\n",
      " [7.36338883e-01 6.14380608e-03 3.93313669e-01 ... 1.42008912e-02\n",
      "  2.63697237e-03 5.19529862e-03]\n",
      " [7.82128982e-04 1.18102842e-06 2.01771913e-05 ... 5.08047374e-06\n",
      "  9.41922578e-01 1.81512338e-04]] & cached\n",
      "Re-used Cached Value, runNum =  454\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  454\n",
      "Provided input from cache for runNum = 454\n",
      "Provided input from cache for runNum = 455\n",
      "activation = [[0.         3.19770137 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08384976 0.         ... 0.         0.         0.        ]\n",
      " [0.         2.3674185  2.60917805 ... 2.17428331 0.         0.        ]\n",
      " ...\n",
      " [1.56985651 0.         4.06220792 ... 0.07499895 1.45516772 5.62026815]\n",
      " [0.         0.37032273 0.         ... 0.         3.00213817 1.44940646]\n",
      " [2.87756352 4.88014993 2.3764945  ... 3.5493789  0.         0.        ]] & cached\n",
      "activation = [[ 3.63258364  2.03715764  0.         ...  2.49575109  1.24874822\n",
      "   7.76740533]\n",
      " [ 3.00835825  2.43512266  3.97529307 ...  8.85520013  3.41068325\n",
      "   4.40820606]\n",
      " [ 0.54532352  5.98590007  1.85586005 ...  4.96560194  0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 8.91627919  0.7447275   8.69028063 ...  6.63312527 19.48173566\n",
      "   9.6093515 ]\n",
      " [ 1.51209002  0.          0.14448498 ...  0.66033091 11.80489315\n",
      "   2.89412862]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] & cached\n",
      "activation = [[1.49438947e-02 1.59765464e-04 4.18201230e-01 ... 1.15825862e-03\n",
      "  2.06634498e-04 9.71384233e-01]\n",
      " [3.36833305e-05 6.15455139e-03 8.43943400e-05 ... 1.12711548e-04\n",
      "  1.67881647e-07 4.82163087e-05]\n",
      " [4.68852637e-02 9.40631558e-01 8.46401610e-02 ... 1.64027360e-02\n",
      "  1.27727696e-05 1.18429056e-03]\n",
      " ...\n",
      " [1.56230460e-05 1.70518711e-05 5.23733461e-05 ... 3.41934114e-05\n",
      "  4.89973895e-02 1.01585409e-02]\n",
      " [7.36404952e-01 6.15111228e-03 3.94739726e-01 ... 1.41432133e-02\n",
      "  2.63096563e-03 5.14001603e-03]\n",
      " [7.79703969e-04 1.17638553e-06 2.01693190e-05 ... 5.05459056e-06\n",
      "  9.41781939e-01 1.79296693e-04]] & cached\n",
      "Re-used Cached Value, runNum =  455\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  455\n",
      "Provided input from cache for runNum = 455\n",
      "Provided input from cache for runNum = 456\n",
      "activation = [[0.         3.19972534 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08418932 0.         ... 0.         0.         0.        ]\n",
      " [0.         2.36966066 2.61019056 ... 2.17537593 0.         0.        ]\n",
      " ...\n",
      " [1.56956175 0.         4.0633048  ... 0.07499724 1.4560079  5.6245525 ]\n",
      " [0.         0.37082719 0.         ... 0.         3.00475892 1.4504317 ]\n",
      " [2.87977184 4.88162396 2.37851019 ... 3.55117689 0.         0.        ]] & cached\n",
      "activation = [[ 3.63505899  2.03780153  0.         ...  2.50425929  1.25420232\n",
      "   7.78464711]\n",
      " [ 3.00781672  2.43160318  3.97485554 ...  8.86547627  3.41276762\n",
      "   4.41031992]\n",
      " [ 0.55495693  5.99578091  1.86101694 ...  4.96925514  0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 8.92463502  0.75033104  8.70247485 ...  6.64510545 19.49485195\n",
      "   9.61267623]\n",
      " [ 1.52134627  0.          0.15327032 ...  0.66507026 11.82241008\n",
      "   2.89759272]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] & cached\n",
      "activation = [[1.48448203e-02 1.57850747e-04 4.16505186e-01 ... 1.15011226e-03\n",
      "  2.05932248e-04 9.71667424e-01]\n",
      " [3.33024283e-05 6.14961911e-03 8.34873147e-05 ... 1.12466566e-04\n",
      "  1.66756721e-07 4.75437457e-05]\n",
      " [4.70452505e-02 9.40825510e-01 8.48859898e-02 ... 1.62505692e-02\n",
      "  1.27964186e-05 1.17647392e-03]\n",
      " ...\n",
      " [1.56052439e-05 1.70103915e-05 5.23899092e-05 ... 3.40183288e-05\n",
      "  4.91713510e-02 1.00219413e-02]\n",
      " [7.36518123e-01 6.15842878e-03 3.96231812e-01 ... 1.40900623e-02\n",
      "  2.62618458e-03 5.08666743e-03]\n",
      " [7.77028523e-04 1.17174778e-06 2.01550723e-05 ... 5.02857370e-06\n",
      "  9.41645922e-01 1.77091726e-04]] & cached\n",
      "Re-used Cached Value, runNum =  456\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  456\n",
      "Provided input from cache for runNum = 456\n",
      "Provided input from cache for runNum = 457\n",
      "activation = [[0.         3.20176524 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08453995 0.         ... 0.         0.         0.        ]\n",
      " [0.         2.37191506 2.61122575 ... 2.17648084 0.         0.        ]\n",
      " ...\n",
      " [1.56930171 0.         4.06443377 ... 0.07501917 1.45688893 5.62885781]\n",
      " [0.         0.37131688 0.         ... 0.         3.00734834 1.45143877]\n",
      " [2.88196166 4.88308763 2.38050934 ... 3.55296354 0.         0.        ]] & cached\n",
      "activation = [[ 3.63734702  2.03835641  0.         ...  2.51257457  1.25940123\n",
      "   7.8016534 ]\n",
      " [ 3.00712473  2.427997    3.97429898 ...  8.87557118  3.41468737\n",
      "   4.41229137]\n",
      " [ 0.56410874  6.00530377  1.86560983 ...  4.97244384  0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 8.93296681  0.7559234   8.71469567 ...  6.6570437  19.50793866\n",
      "   9.61594129]\n",
      " [ 1.5307825   0.          0.16223113 ...  0.66995594 11.84016157\n",
      "   2.90127275]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]] & cached\n",
      "activation = [[1.47529316e-02 1.56004165e-04 4.14923642e-01 ... 1.14255819e-03\n",
      "  2.05205217e-04 9.71947814e-01]\n",
      " [3.29389100e-05 6.14614734e-03 8.26051114e-05 ... 1.12262644e-04\n",
      "  1.65605750e-07 4.68816048e-05]\n",
      " [4.72091641e-02 9.41021618e-01 8.51185685e-02 ... 1.61023303e-02\n",
      "  1.28178551e-05 1.16873443e-03]\n",
      " ...\n",
      " [1.55919312e-05 1.69698038e-05 5.24076205e-05 ... 3.38513569e-05\n",
      "  4.93532091e-02 9.88994272e-03]\n",
      " [7.36629386e-01 6.16590405e-03 3.97655788e-01 ... 1.40380111e-02\n",
      "  2.62017606e-03 5.03264103e-03]\n",
      " [7.74688215e-04 1.16740097e-06 2.01456791e-05 ... 5.00486398e-06\n",
      "  9.41504605e-01 1.74932880e-04]] & cached\n",
      "Re-used Cached Value, runNum =  457\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value, runNum =  457\n",
      "Provided input from cache for runNum = 457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(maxIt):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Y_train_oneHot \u001b[39m=\u001b[39m one_hot(Y_train, maxExpected\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     predictedRAW \u001b[39m=\u001b[39m n1\u001b[39m.\u001b[39;49mbackward_prop(input_values\u001b[39m=\u001b[39;49m(X_train), trueOutput\u001b[39m=\u001b[39;49mY_train_oneHot)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m(it \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39miterations =\u001b[39m\u001b[39m\"\u001b[39m, it)\n",
      "File \u001b[0;32m~/Documents/Development/ML/tensorGPU/adaptive-network/adaptivenetworks/nnetwork.py:97\u001b[0m, in \u001b[0;36mnnetwork.backward_prop\u001b[0;34m(self, input_values, trueOutput)\u001b[0m\n\u001b[1;32m     95\u001b[0m iftelemetry: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstarting backprop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m activError \u001b[39m=\u001b[39m predictedOutput \u001b[39m-\u001b[39m trueOutput   \u001b[39m## error in output activations\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mcorrect_error(activError, runNum\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunNum)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m [predictions, activError]\n",
      "File \u001b[0;32m~/Documents/Development/ML/tensorGPU/adaptive-network/adaptivenetworks/nlayer.py:282\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error, runNum)\u001b[0m\n\u001b[1;32m    279\u001b[0m     lengthTillNow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layerLengths[layerIndx]\n\u001b[1;32m    280\u001b[0m     splitPoints\u001b[39m.\u001b[39mappend(lengthTillNow)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layerIndx]\u001b[39m.\u001b[39;49mcorrect_error(dIZ[splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]:splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]], runNum\u001b[39m=\u001b[39;49mrunNum)\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue]\n",
      "File \u001b[0;32m~/Documents/Development/ML/tensorGPU/adaptive-network/adaptivenetworks/nlayer.py:282\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error, runNum)\u001b[0m\n\u001b[1;32m    279\u001b[0m     lengthTillNow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layerLengths[layerIndx]\n\u001b[1;32m    280\u001b[0m     splitPoints\u001b[39m.\u001b[39mappend(lengthTillNow)\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layerIndx]\u001b[39m.\u001b[39;49mcorrect_error(dIZ[splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]:splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]], runNum\u001b[39m=\u001b[39;49mrunNum)\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue]\n",
      "File \u001b[0;32m~/Documents/Development/ML/tensorGPU/adaptive-network/adaptivenetworks/nlayer.py:263\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error, runNum)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m# if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m#     inputArrT = inputArr[np.newaxis].T\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[39m# dZ = self.cacheValue - activation_error\u001b[39;00m\n\u001b[1;32m    261\u001b[0m dZ \u001b[39m=\u001b[39m activation_error\n\u001b[0;32m--> 263\u001b[0m dW \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mbatch_size)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ, inputArr\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    264\u001b[0m dB \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mbatch_size)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ)\n\u001b[1;32m    266\u001b[0m oldWeights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "telementary = 0\n",
    "\n",
    "lastWeights = nt.output_layer.weights\n",
    "\n",
    "dataLen = len(X_train)\n",
    "\n",
    "maxIt = 450\n",
    "for it in range(maxIt):\n",
    "    Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "    predictedRAW = n1.backward_prop(input_values=(X_train), trueOutput=Y_train_oneHot)\n",
    "\n",
    "    if(it % 50 == 0):\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW[0][0])\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descending Telemetry\n",
    "### For each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# accuracySum = 0\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# runCount = 0\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m telementary \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lastWeights \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39mweights\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dataLen \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_train[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aharnish/Documents/Development/ML/tensorGPU/adaptive-network/main.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m batchSize \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nt' is not defined"
     ]
    }
   ],
   "source": [
    "# accuracySum = 0\n",
    "# runCount = 0\n",
    "\n",
    "telementary = 0\n",
    "lastWeights = nt.output_layer.weights\n",
    "\n",
    "dataLen = len(X_train[0])\n",
    "batchSize = 1000\n",
    "maxIt = 450\n",
    "for it in range(maxIt):\n",
    "    spliceRange = [(batchSize*it)%dataLen, batchSize*(it+1)%dataLen]    ## [start, end]\n",
    "\n",
    "    if(spliceRange[0] >= spliceRange[1]):\n",
    "        # spliceRange[0] = dataLen - spliceRange[0]\n",
    "        # if(spliceRange[0] >= spliceRange[1]):\n",
    "        spliceRange[0] = 0\n",
    "        spliceRange[1] = batchSize\n",
    "\n",
    "    ## Verifying batch size\n",
    "    X_Train_Batch = X_train.T[spliceRange[0]:spliceRange[1]].T\n",
    "    Y_Train_Batch = Y_train[spliceRange[0]:spliceRange[1]]\n",
    "    if(telementary==2): print(\"SPLICE:\", spliceRange)\n",
    "    if(telementary==2): print(\"X size:\", X_Train_Batch.shape)\n",
    "    if(telementary==2): print(\"Y size:\", Y_Train_Batch.shape)\n",
    "    if(X_Train_Batch.shape[1] == 0):\n",
    "        print(\"error no X: spliceRange\", spliceRange)\n",
    "\n",
    "    Y_train_oneHot = one_hot(Y_Train_Batch, maxExpected=9)\n",
    "    predictedRAW = n1.backward_prop(input_values=(X_Train_Batch), trueOutput=Y_train_oneHot)\n",
    "\n",
    "    if(it % 50 == 0):\n",
    "        print(\"Please press enter\")\n",
    "        pauser = input(\"Press enter for next iteration.\")\n",
    "        clearScreen()\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW[0][0])\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_Train_Batch))\n",
    "        # print(\"Weights of output:\")\n",
    "        # print(n1.output_layer.weights)\n",
    "        newWeights = n1.output_layer.weights\n",
    "\n",
    "        # print weight changes made\n",
    "        if(newWeights.shape == lastWeights.shape):\n",
    "            printMap((newWeights - lastWeights)*50)\n",
    "\n",
    "        finalMAE = np.sum(np.absolute(predictedRAW[1]), axis=1) ## MAE of output activation errors over last ran single batch\n",
    "        finalMAE_MAX = np.max(finalMAE)\n",
    "\n",
    "        print(\"Activation Errors = \", finalMAE)\n",
    "        printMap((finalMAE[:, np.newaxis]/finalMAE_MAX) - 0.5, sizes=[1,10])\n",
    "\n",
    "        lastWeights = newWeights #n1.output_layer.weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Variance in Activation Errors\n",
    "To predict where more detailed data separation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "overNN = nnetwork(784, 10, insertDefault=0)\n",
    "overNN.output_layer.activationFn = \"softmax\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Layer\n",
      "10\n",
      "(10, 784) wts.shape = (output.shape, input.shape)\n",
      "(10, 1)\n",
      "2nd Last\n",
      "784\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m2nd Last\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(overNN\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39minput_layers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m \u001b[39mprint\u001b[39m(overNN\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49minput_layers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mshape, \u001b[39m\"\u001b[39m\u001b[39mwts.shape = (output.shape, input.shape)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(overNN\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39minput_layers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"Last Layer\")\n",
    "print(overNN.output_layer.shape)\n",
    "print(overNN.output_layer.weights.shape, \"wts.shape = (output.shape, input.shape)\")\n",
    "print(overNN.output_layer.bias.shape)\n",
    "\n",
    "print(\"2nd Last\")\n",
    "print(overNN.output_layer.input_layers[0].shape)\n",
    "print(overNN.output_layer.input_layers[0].weights.shape, \"wts.shape = (output.shape, input.shape)\")\n",
    "print(overNN.output_layer.input_layers[0].bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "overNN.addLayerAtLast(40,isDynamic=1, activationFn=\"relu\", transferWeights=0)\n",
    "# overNN.addLayerAtLast(10,isDynamic=1, activationFn=\"relu\")\n",
    "\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Layer\n",
      "10\n",
      "(10, 864) wts.shape = (output.shape, input.shape)\n",
      "(10, 1)\n",
      "2nd Last\n",
      "40\n",
      "(40, 40) wts.shape = (output.shape, input.shape)\n",
      "(40, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Layer\")\n",
    "print(overNN.output_layer.shape)\n",
    "print(overNN.output_layer.weights.shape, \"wts.shape = (output.shape, input.shape)\")\n",
    "print(overNN.output_layer.bias.shape)\n",
    "\n",
    "print(\"2nd Last\")\n",
    "print(overNN.output_layer.input_layers[0].shape)\n",
    "print(overNN.output_layer.input_layers[0].weights.shape, \"wts.shape = (output.shape, input.shape)\")\n",
    "print(overNN.output_layer.input_layers[0].bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overNN.output_layer.input_layers[0].weights = np.random.rand(40,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "Input values should be a 2D array.\n",
      "Adding to weight.shape[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m telementary \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m (overNN\u001b[39m.\u001b[39;49mforward_prop(X_train\u001b[39m.\u001b[39;49mT[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mT))\n\u001b[1;32m      3\u001b[0m \u001b[39m# telementary = 1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
      "Cell \u001b[0;32mIn[18], line 68\u001b[0m, in \u001b[0;36mnetwork.forward_prop\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue \u001b[39m=\u001b[39m input_values\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcachedRun \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue) \u001b[39m!=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m---> 68\u001b[0m     output_activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mgetActivation()\n\u001b[1;32m     69\u001b[0m     runNum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m output_activations\n",
      "Cell \u001b[0;32mIn[17], line 128\u001b[0m, in \u001b[0;36mnlayer.getActivation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         inputArr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((inputArr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layers[layrIndx]\u001b[39m.\u001b[39mgetActivation()))\n\u001b[1;32m    127\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         inputArr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layrIndx]\u001b[39m.\u001b[39;49mgetActivation()\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeingEvaluated \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[39m##  Checking dimensions of input matrix\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 146\u001b[0m, in \u001b[0;36mnlayer.getActivation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[39mif\u001b[39;00m(telementary): \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAdding to weight.shape[0]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m         \u001b[39m# Generating new weights for weights matrix but shape value is already present so updateShapeValue = 0\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maddWidth_to_Layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], updateShapeValue\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    152\u001b[0m \u001b[39m# Checking if shape matches\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m(inputArr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[17], line 75\u001b[0m, in \u001b[0;36mnlayer.addWidth_to_Layer\u001b[0;34m(self, addWidth, updateShapeValue)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m(addWidth \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m(updateShapeValue): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m addWidth\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, (np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrand(addWidth) \u001b[39m-\u001b[39;49m \u001b[39m0.5\u001b[39;49m)))\n\u001b[1;32m     77\u001b[0m     \u001b[39m## generating new row of random weights\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     generatedRow \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(addWidth, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "telementary = 1\n",
    "out = (overNN.forward_prop(X_train.T[0].T))\n",
    "# telementary = 1\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations = 0\n",
      "Accuracy = 0.08102439024390244\n",
      "iterations = 50\n",
      "Accuracy = 0.09629268292682927\n",
      "iterations = 100\n",
      "Accuracy = 0.16651219512195123\n",
      "iterations = 150\n",
      "Accuracy = 0.19514634146341464\n",
      "iterations = 200\n",
      "Accuracy = 0.2105121951219512\n",
      "iterations = 250\n",
      "Accuracy = 0.22439024390243903\n",
      "iterations = 300\n",
      "Accuracy = 0.2355121951219512\n",
      "iterations = 350\n",
      "Accuracy = 0.2474390243902439\n",
      "iterations = 400\n",
      "Accuracy = 0.2581951219512195\n",
      "iterations = 450\n",
      "Accuracy = 0.2674146341463415\n",
      "iterations = 500\n",
      "Accuracy = 0.2760975609756098\n",
      "iterations = 550\n",
      "Accuracy = 0.28580487804878046\n",
      "iterations = 600\n",
      "Accuracy = 0.29517073170731706\n",
      "iterations = 650\n",
      "Accuracy = 0.30585365853658536\n",
      "iterations = 700\n",
      "Accuracy = 0.31726829268292683\n",
      "iterations = 750\n",
      "Accuracy = 0.3289268292682927\n",
      "iterations = 800\n",
      "Accuracy = 0.33963414634146344\n",
      "iterations = 850\n",
      "Accuracy = 0.34685365853658534\n",
      "iterations = 900\n",
      "Accuracy = 0.35460975609756096\n",
      "iterations = 950\n",
      "Accuracy = 0.36085365853658535\n",
      "iterations = 1000\n",
      "Accuracy = 0.36860975609756097\n",
      "iterations = 1050\n",
      "Accuracy = 0.3744390243902439\n",
      "iterations = 1100\n",
      "Accuracy = 0.3808048780487805\n",
      "iterations = 1150\n",
      "Accuracy = 0.38721951219512196\n",
      "iterations = 1200\n",
      "Accuracy = 0.3931219512195122\n",
      "iterations = 1250\n",
      "Accuracy = 0.39946341463414636\n",
      "iterations = 1300\n",
      "Accuracy = 0.40673170731707314\n",
      "iterations = 1350\n",
      "Accuracy = 0.4141951219512195\n",
      "iterations = 1400\n",
      "Accuracy = 0.42048780487804877\n",
      "iterations = 1450\n",
      "Accuracy = 0.42521951219512194\n",
      "iterations = 1500\n",
      "Accuracy = 0.4332439024390244\n",
      "iterations = 1550\n",
      "Accuracy = 0.4404390243902439\n",
      "iterations = 1600\n",
      "Accuracy = 0.45121951219512196\n",
      "iterations = 1650\n",
      "Accuracy = 0.45446341463414636\n",
      "iterations = 1700\n",
      "Accuracy = 0.45639024390243904\n",
      "iterations = 1750\n",
      "Accuracy = 0.453\n",
      "iterations = 1800\n",
      "Accuracy = 0.4580975609756098\n",
      "iterations = 1850\n",
      "Accuracy = 0.4528536585365854\n",
      "iterations = 1900\n",
      "Accuracy = 0.45082926829268294\n",
      "iterations = 1950\n",
      "Accuracy = 0.45165853658536587\n",
      "iterations = 2000\n",
      "Accuracy = 0.4635365853658537\n",
      "iterations = 2050\n",
      "Accuracy = 0.4612926829268293\n",
      "iterations = 2100\n",
      "Accuracy = 0.4601707317073171\n",
      "iterations = 2150\n",
      "Accuracy = 0.458609756097561\n",
      "iterations = 2200\n",
      "Accuracy = 0.45921951219512197\n",
      "iterations = 2250\n",
      "Accuracy = 0.46026829268292685\n",
      "iterations = 2300\n",
      "Accuracy = 0.464\n",
      "iterations = 2350\n",
      "Accuracy = 0.45834146341463416\n",
      "iterations = 2400\n",
      "Accuracy = 0.46331707317073173\n",
      "iterations = 2450\n",
      "Accuracy = 0.46897560975609753\n",
      "iterations = 2500\n",
      "Accuracy = 0.47368292682926827\n",
      "iterations = 2550\n",
      "Accuracy = 0.47721951219512193\n",
      "iterations = 2600\n",
      "Accuracy = 0.4801219512195122\n",
      "iterations = 2650\n",
      "Accuracy = 0.48234146341463413\n",
      "iterations = 2700\n",
      "Accuracy = 0.48453658536585364\n",
      "iterations = 2750\n",
      "Accuracy = 0.48617073170731706\n",
      "iterations = 2800\n",
      "Accuracy = 0.48904878048780487\n",
      "iterations = 2850\n",
      "Accuracy = 0.4911463414634146\n",
      "iterations = 2900\n",
      "Accuracy = 0.4924390243902439\n",
      "iterations = 2950\n",
      "Accuracy = 0.49373170731707317\n",
      "iterations = 3000\n",
      "Accuracy = 0.49521951219512195\n",
      "iterations = 3050\n",
      "Accuracy = 0.49621951219512195\n",
      "iterations = 3100\n",
      "Accuracy = 0.4976829268292683\n",
      "iterations = 3150\n",
      "Accuracy = 0.49853658536585366\n",
      "iterations = 3200\n",
      "Accuracy = 0.5003658536585366\n",
      "iterations = 3250\n",
      "Accuracy = 0.5014634146341463\n",
      "iterations = 3300\n",
      "Accuracy = 0.5017560975609756\n",
      "iterations = 3350\n",
      "Accuracy = 0.5031707317073171\n",
      "iterations = 3400\n",
      "Accuracy = 0.5037560975609756\n",
      "iterations = 3450\n",
      "Accuracy = 0.5141463414634146\n",
      "iterations = 3500\n",
      "Accuracy = 0.5179512195121951\n",
      "iterations = 3550\n",
      "Accuracy = 0.5209268292682927\n",
      "iterations = 3600\n",
      "Accuracy = 0.5234146341463415\n",
      "iterations = 3650\n",
      "Accuracy = 0.5254878048780488\n",
      "iterations = 3700\n",
      "Accuracy = 0.527219512195122\n",
      "iterations = 3750\n",
      "Accuracy = 0.5284878048780488\n",
      "iterations = 3800\n",
      "Accuracy = 0.5299512195121951\n",
      "iterations = 3850\n",
      "Accuracy = 0.5315365853658537\n",
      "iterations = 3900\n",
      "Accuracy = 0.531609756097561\n",
      "iterations = 3950\n",
      "Accuracy = 0.5312926829268293\n",
      "iterations = 4000\n",
      "Accuracy = 0.5320487804878049\n",
      "iterations = 4050\n",
      "Accuracy = 0.5330487804878049\n",
      "iterations = 4100\n",
      "Accuracy = 0.5339024390243903\n",
      "iterations = 4150\n",
      "Accuracy = 0.5331219512195122\n",
      "iterations = 4200\n",
      "Accuracy = 0.5325365853658537\n",
      "iterations = 4250\n",
      "Accuracy = 0.5320975609756098\n",
      "iterations = 4300\n",
      "Accuracy = 0.5320243902439025\n",
      "iterations = 4350\n",
      "Accuracy = 0.5326829268292683\n",
      "iterations = 4400\n",
      "Accuracy = 0.5334878048780488\n",
      "iterations = 4450\n",
      "Accuracy = 0.534\n",
      "iterations = 4500\n",
      "Accuracy = 0.5347804878048781\n",
      "iterations = 4550\n",
      "Accuracy = 0.5360731707317073\n",
      "iterations = 4600\n",
      "Accuracy = 0.5372439024390244\n",
      "iterations = 4650\n",
      "Accuracy = 0.5380975609756098\n",
      "iterations = 4700\n",
      "Accuracy = 0.5393414634146342\n",
      "iterations = 4750\n",
      "Accuracy = 0.5406829268292683\n",
      "iterations = 4800\n",
      "Accuracy = 0.5420243902439025\n",
      "iterations = 4850\n",
      "Accuracy = 0.5426341463414635\n",
      "iterations = 4900\n",
      "Accuracy = 0.544\n",
      "iterations = 4950\n",
      "Accuracy = 0.5447804878048781\n"
     ]
    }
   ],
   "source": [
    "# accuracySum = 0\n",
    "# runCount = 0\n",
    "\n",
    "telementary = 0\n",
    "\n",
    "# lastWeights = overNN.output_layer.weights\n",
    "\n",
    "dataLen = len(X_train)\n",
    "\n",
    "maxIt = 5000\n",
    "for it in range(maxIt):\n",
    "    Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "    predictedRAW = overNN.backward_prop(input_values=(X_train), trueOutput=Y_train_oneHot)\n",
    "\n",
    "    if(it % 50 == 0):\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW[0][0])\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_train))\n",
    "        # newWeights = nt.output_layer.weights\n",
    "\n",
    "        # printMap((newWeights - lastWeights)*50)\n",
    "\n",
    "        # lastWeights = newWeights #nt.output_layer.weights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nt.output_layer.input_layers[0].input_layers[0].weights\n",
    "# print(wt.shape)\n",
    "\n",
    "# x1 = X_trainT[0:2]\n",
    "x1 = X_trainT[0]\n",
    "\n",
    "print(\"wt\", wt.shape)\n",
    "print(\"x1\", x1.shape)\n",
    "print(\"x1T\", x1.T.shape)\n",
    "\n",
    "act1 = np.matmul(wt, x1.T)\n",
    "act1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [5,6,7]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0), dtype=float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empt = np.array([[]])\n",
    "empt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(empt.shape[1] > 0):\n",
    "    empt = np.concatenate((empt, x))\n",
    "else:\n",
    "    empt = x\n",
    "\n",
    "print(empt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4],\n",
       "       [4, 6, 8]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[-1,0,1]])\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21379049  0.318942  ]\n",
      " [ 0.33342107 -0.20130355]\n",
      " [-0.22479054  0.23240682]\n",
      " [-0.10478904 -0.04715419]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(4,2) - 0.5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDerivActivationFn(input):\n",
    "    return (input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False],\n",
       "       [False,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applyDerivActivationFn(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = nnetwork(2,1)\n",
    "\n",
    "in1 = np.array([0,1,2])\n",
    "wtMat = np.array([[5,6,7],[8,9,10]])\n",
    "# biases = np.array([5,25])\n",
    "biases = np.array([0.5,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.5  29.25]\n"
     ]
    }
   ],
   "source": [
    "output_activations = np.matmul(wtMat, in1) + biases\n",
    "print(output_activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
