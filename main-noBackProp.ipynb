{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "telementary = 1\n",
    "runNum = 0      ## Increment to utilise caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking in input shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlayer:\n",
    "    id = 0\n",
    "    shape = 1               ## Defines self dimension (1D)\n",
    "    input_layers = []        ## store layer pointer\n",
    "    weights = None          ## assuming all input activations are concatenated (sorted on layer ID).\n",
    "    bias = np.array([])      ## store self biases\n",
    "    activationFn = \"linear\"         ## store self activation function\n",
    "\n",
    "\n",
    "    ## Caching\n",
    "    \n",
    "    #### store last activation as cache to speed up when multiple layers use this layer as input. So this is evaluated only once.\n",
    "    cachedRun = -2      # runNum when cache was calculated, can be old\n",
    "    ## cachedRun = -1 & isAdaptive = 0 is for input layers\n",
    "    cacheValue = None\n",
    "    \n",
    "    ## Flag indicating if it was being evaluated.\n",
    "    #### This can help in case of self loops, when a layer was being evaluated was evaluated again\n",
    "    #  meaning one of this layer's input_layer has this layer as one of the inputs (called self-loop is a graph).\n",
    "    #  In this situation, the last cached value of this layer will be returned.\n",
    "    # this may be used to simulate LSTM Network.\n",
    "    beingEvaluated = 0  \n",
    "\n",
    "    ## Error variance\n",
    "    #### Store absolute sum of errors in terms of array of sum per node in 1D np array\n",
    "    isAdaptive = 1\n",
    "    \n",
    "\n",
    "\n",
    "    ## Methods\n",
    "    def __init__(self, shape=1, inputLayers=[], isInput=0, setInputValues=[], ActivationFn=\"linear\", isFixed=0) -> None:\n",
    "        self.shape = shape\n",
    "        self.activationFn = ActivationFn\n",
    "        self.bias = np.zeros(shape)\n",
    "        self.input_layers = []  ## Clearing on reinitializing\n",
    "\n",
    "        if(isFixed==1):\n",
    "            self.isAdaptive = 0\n",
    "        if(isInput):\n",
    "            self.cachedRun = -1\n",
    "            self.isAdaptive = 0\n",
    "            if(len(setInputValues) != 0):\n",
    "                self.cacheValue = np.array(setInputValues)\n",
    "        else:\n",
    "            # generating random weights if given\n",
    "            if(type(inputLayers) == type([])):\n",
    "                if(len(inputLayers) != 0):\n",
    "                    for layer in inputLayers:\n",
    "                        self.addInputLayer(layer)\n",
    "            else:\n",
    "                print(\"inputLayers should be a List.\")\n",
    "                if(type(inputLayers) == type(nlayer(1))):\n",
    "                    self.addInputLayer(inputLayers)\n",
    "\n",
    "    def addInputLayer(self, newInputLayer):\n",
    "        # check if it doesn't already exists\n",
    "        for layr in self.input_layers:\n",
    "            if(newInputLayer == layr):\n",
    "                print(\"Layer already exists.\")\n",
    "                return -1\n",
    "\n",
    "        self.input_layers.append(newInputLayer)\n",
    "        ## DONE: Generate random weights\n",
    "        generatedColumn = np.random.rand(self.shape, newInputLayer.shape)\n",
    "        if(type(self.weights) == type(None)):\n",
    "            self.weights = generatedColumn\n",
    "        else:\n",
    "            self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "    def addWidth_to_Layer(self, addWidth):\n",
    "        if(addWidth > 0):\n",
    "            self.shape += addWidth\n",
    "            self.bias = np.concatenate((self.bias, np.random.rand(addWidth)))\n",
    "            \n",
    "            ## generating new row of random weights\n",
    "            generatedRow = np.random.rand(addWidth, self.weights.shape[1])\n",
    "            self.weights = np.concatenate((self.weights, generatedRow))\n",
    "        else:\n",
    "            print(\"error, doesn't support decrease.\")\n",
    "\n",
    "\n",
    "    def calcActivationFn(self,rawActivation):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return rawActivation\n",
    "\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return np.maximum(rawActivation, 0)\n",
    "\n",
    "    def getActivation(self):    ## return np array of activation of current layer\n",
    "        ## beingEvaluated == 1 means the node was triggered by a loop in the network. Returning last value cached prevents infinite loops.\n",
    "        if(self.cachedRun == runNum or self.cachedRun == -1 or self.beingEvaluated == 1):   ## if activation was already calculated for this run OR is an input layer\n",
    "            return(self.cacheValue)\n",
    "        else:\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            inputArr = np.array([])\n",
    "\n",
    "            self.beingEvaluated = 1\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "\n",
    "\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "            # Checking if shape matches\n",
    "            if(inputArr.shape[0] > self.weights.shape[1]):\n",
    "                if(telementary): print(\"!!!SHAPE MISMATCH!!!\")  \n",
    "                ## Adjust matrix dimension & adding new random weights to match size\n",
    "                generatedColumn = np.random.rand(self.weights.shape[0], (inputArr.shape[0] - self.weights.shape[1]))\n",
    "                self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "\n",
    "            elif(inputArr.shape[0] < self.weights.shape[1]):       ## input layer is removed causing weight matrix to be larger than inputs\n",
    "                print(\"!! Input Layer was removed. Unstable !!\")\n",
    "                return -1\n",
    "            \n",
    "            rawActivation = np.matmul(self.weights, inputArr) + self.bias\n",
    "            activation = self.calcActivationFn(rawActivation=rawActivation)\n",
    "\n",
    "            self.cachedRun = runNum\n",
    "            # self.cacheValue = activation          ## storing a pointer to activation calculated\n",
    "            self.cacheValue = np.copy(activation)   ## duplicating array\n",
    "\n",
    "            if(telementary): print(\"activation =\", activation, \"& cached\")  \n",
    "\n",
    "            return activation\n",
    "\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    input_shape=1  # Currently only 1D\n",
    "    output_shape=1 # Currently only 1D\n",
    " \n",
    "    input_layer = None      ## Pointer to input nlayer\n",
    "    output_layer = None     ## Pointer to output nlayer\n",
    "\n",
    "    layers = []\n",
    "    numberOfLayers = 0      ## used to assign ID to new layer in matrix\n",
    "\n",
    "    adaptive = 1\n",
    "\n",
    "    def __init__(self, input_shape, output_shape) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # Connect output with 1 adaptive neuron input\n",
    "        self.input_layer = nlayer(input_shape, isInput=1)\n",
    "\n",
    "        hiddenLayer = nlayer(1,inputLayers=[self.input_layer],ActivationFn=\"relu\")\n",
    "\n",
    "        self.output_layer = nlayer(output_shape, inputLayers=[hiddenLayer], isFixed=1)\n",
    "\n",
    "    def setInput(self, input_values):\n",
    "        # print(\"SETTING INPUT LAYER & STORING VALUES\")\n",
    "        if(type(self.input_layer) != type(None)):\n",
    "            if(len(input_values) < self.input_layer.shape):\n",
    "                print(\"ERROR: Unable to reduce input layer shape. Insert len(input values) >= input_shape\")\n",
    "            else:\n",
    "                self.input_layer.shape = len(input_values)\n",
    "                self.input_layer.cachedRun = -1\n",
    "                self.input_layer.isAdaptive = 0\n",
    "                self.input_layer.cacheValue = np.array(input_values)\n",
    "        else:   ## Initialize new input layer\n",
    "                self.input_layer = nlayer(len(input_values), isInput=1, setInputValues=np.array(input_values))\n",
    "\n",
    "                \n",
    "                linker = self.output_layer\n",
    "                if(type(linker) != type(None)):\n",
    "                    while(len(linker.input_layers) > 0):    ## following only oldest (1st in list) links to reach input\n",
    "                        linker = linker.input_layers[0]                               \n",
    "                    linker.input_layer = [self.input_layer]\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self, input_values=None):    # find result activation from input activation and weights\n",
    "        if(type(input_values) != type(None)):\n",
    "            self.input_layer.cacheValue = input_values\n",
    "\n",
    "        if(self.input_layer.cachedRun == -1 and type(self.input_layer.cacheValue) != type(None)):\n",
    "            output_activations = self.output_layer.getActivation()\n",
    "            return output_activations\n",
    "        else:\n",
    "            print(\"Input uninitialized\")\n",
    "            return -1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.setInput([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.output_layer.input_layers[0].input_layers[0].cacheValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "print(n1.output_layer.input_layers[0].cachedRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.setInput([3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2491882]\n"
     ]
    }
   ],
   "source": [
    "print(n1.forward_prop())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layerTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inLay = nlayer(2, isInput=1, setInputValues=[2,3])\n",
    "\n",
    "hidLay = nlayer(1, inputLayers=[inLay], ActivationFn=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.nlayer at 0x7fdc1378ecb0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidLay.bias\n",
    "# hidLay.weights\n",
    "# hidLay.cacheValue\n",
    "hidLay.input_layers\n",
    "# inLay.input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation = [4.91548855] & cached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.91548855])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.getActivation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newInLay = nlayer(3,isInput=1,setInputValues=[2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidLay.addInputLayer(newInputLayer=newInLay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99885793, 0.9725909 , 0.72486948, 0.35375928, 0.07909738]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidLay.addWidth_to_Layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99885793, 0.9725909 , 0.72486948, 0.35375928, 0.07909738],\n",
       "       [0.10659773, 0.55853347, 0.62134468, 0.07597125, 0.98782191],\n",
       "       [0.06992685, 0.24441412, 0.73520054, 0.26422487, 0.89743243]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.99525239, 6.29679149, 7.71707944])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.getActivation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empArr = np.empty(1,dtype=float)\n",
    "empArr = np.array([4,],dtype=float)\n",
    "inArr = np.array([1,2],dtype=float)\n",
    "# inArr = np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(inArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 1. 2. 1. 2.]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "empArr = np.concatenate((empArr, inArr))\n",
    "print(empArr)\n",
    "print(empArr.shape)\n",
    "# print(newArr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.array([[3,2,1],[1,0,-1]],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wts (2, 3)\n",
      "emp (3,)\n",
      "act [25.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"wts\", wts.shape)\n",
    "print(\"emp\", empArr.shape)\n",
    "act = np.matmul(wts,empArr)\n",
    "print(\"act\", act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 1., 2., 1., 2.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to add = empArr.shape[0] - wts.shape[1]\n",
    "newCols = np.random.rand(wts.shape[0], empArr.shape[0] - wts.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  2.,  1.],\n",
       "       [ 1.,  0., -1.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.concatenate((wts, newCols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21734973, 0.22785161, 0.77439285],\n",
       "       [0.26069918, 0.76749742, 0.33375012]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2,3]\n",
    "y = x.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(2,1)\n",
    "\n",
    "in1 = np.array([0,1,2])\n",
    "wtMat = np.array([[5,6,7],[8,9,10]])\n",
    "# biases = np.array([5,25])\n",
    "biases = np.array([0.5,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.5  29.25]\n"
     ]
    }
   ],
   "source": [
    "output_activations = np.matmul(wtMat, in1) + biases\n",
    "print(output_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.5 , 29.25])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.forward_prop(input_activations=in1, weight_matrix=wtMat, bias_ndarrray=biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb745f7c98c08174c20a2b94d108f69dcc884658473fbc3700597e1ee8c317ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
