{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "telementary = 1\n",
    "runNum = 0      ## Increment to utilise caching\n",
    "# batch_size=100       ## assumed size of dataset\n",
    "learningRate = 0.02"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking in input shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlayer:\n",
    "    id = 0\n",
    "    shape = 1               ## Defines self dimension (1D)\n",
    "    input_layers = []        ## store layer pointer\n",
    "    weights = None          ## assuming all input activations are concatenated (sorted on layer ID).\n",
    "    bias = np.array([])      ## store self biases\n",
    "    activationFn = \"linear\"         ## store self activation function\n",
    "\n",
    "\n",
    "    ## Caching\n",
    "    \n",
    "    #### store last activation as cache to speed up when multiple layers use this layer as input. So this is evaluated only once.\n",
    "    cachedRun = -2      # runNum when cache was calculated, can be old\n",
    "    ## cachedRun = -1 & isAdaptive = 0 is for input layers\n",
    "    cacheValue = None\n",
    "    \n",
    "    ## Flag indicating if it was being evaluated.\n",
    "    #### This can help in case of self loops, when a layer was being evaluated was evaluated again\n",
    "    #  meaning one of this layer's input_layer has this layer as one of the inputs (called self-loop is a graph).\n",
    "    #  In this situation, the last cached value of this layer will be returned.\n",
    "    # this may be used to simulate LSTM Network.\n",
    "    beingEvaluated = 0  \n",
    "\n",
    "    ## Error variance\n",
    "    #### Store absolute sum of errors in terms of array of sum per node in 1D np array\n",
    "\n",
    "    ## supports changing widths & depths. Not suitable for inputs and outputs\n",
    "    isDynamic = 1\n",
    "    \n",
    "\n",
    "\n",
    "    ## Methods\n",
    "    def __init__(self, shape=1, inputLayers=[], isInput=0, setInputValues=[], activationFn=\"linear\", isDynamic=0) -> None:\n",
    "        self.shape = shape\n",
    "        self.activationFn = activationFn\n",
    "        self.bias = np.random.rand(shape, 1)\n",
    "        self.input_layers = []  ## Clearing on reinitializing\n",
    "\n",
    "        if(isDynamic==0):\n",
    "            self.isDynamic = 0\n",
    "        if(isInput):\n",
    "            self.cachedRun = -1\n",
    "            self.isDynamic = 0\n",
    "            if(len(setInputValues) != 0):\n",
    "                self.cacheValue = np.array(setInputValues)\n",
    "        else:\n",
    "            # generating random weights if given\n",
    "            if(type(inputLayers) == type([])):\n",
    "                if(len(inputLayers) != 0):\n",
    "                    for layer in inputLayers:\n",
    "                        self.addInputLayer(layer)\n",
    "            else:\n",
    "                print(\"inputLayers should be a List.\")\n",
    "                if(type(inputLayers) == type(nlayer(1))):\n",
    "                    self.addInputLayer(inputLayers)\n",
    "\n",
    "    def addInputLayer(self, newInputLayer):\n",
    "        # check if it doesn't already exists\n",
    "        for layr in self.input_layers:\n",
    "            if(newInputLayer == layr):\n",
    "                print(\"Layer already exists.\")\n",
    "                return -1\n",
    "\n",
    "        self.input_layers.append(newInputLayer)\n",
    "        ## DONE: Generate random weights\n",
    "        generatedColumn = np.random.rand(self.shape, newInputLayer.shape) - 0.5\n",
    "        if(type(self.weights) == type(None)):\n",
    "            self.weights = generatedColumn\n",
    "        else:\n",
    "            self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "    def addWidth_to_Layer(self, addWidth):\n",
    "        if(addWidth > 0):\n",
    "            self.shape += addWidth\n",
    "            self.bias = np.concatenate((self.bias, (np.random.rand(addWidth) - 0.5)))\n",
    "            \n",
    "            ## generating new row of random weights\n",
    "            generatedRow = np.random.rand(addWidth, self.weights.shape[1]) - 0.5\n",
    "            self.weights = np.concatenate((self.weights, generatedRow))\n",
    "        else:\n",
    "            print(\"error, doesn't support decrease.\")\n",
    "\n",
    "\n",
    "    def applyActivationFn(self,rawActivation):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return rawActivation\n",
    "\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return np.maximum(rawActivation, 0)\n",
    "\n",
    "        if(self.activationFn == \"softmax\"):\n",
    "            A = np.exp(rawActivation) / sum(np.exp(rawActivation))\n",
    "            return A\n",
    " \n",
    "    def applyDerivActivationFn(self, input):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return 1\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return (input > 0)\n",
    "        else:\n",
    "            if(telementary): print(\"Activation Function =\", self.activationFn, \" didn't match, returning as ReLU\")\n",
    "            return (input > 0)\n",
    "            \n",
    "      \n",
    "\n",
    "    def getActivation(self):    ## return np array of activation of current layer\n",
    "        ## beingEvaluated == 1 means the node was triggered by a loop in the network. Returning last value cached prevents infinite loops.\n",
    "\n",
    "        \n",
    "        if(self.cachedRun == runNum or self.cachedRun == -1 or self.beingEvaluated == 1):   ## if activation was already calculated for this run OR is an input layer\n",
    "            if(telementary): \n",
    "                if(self.cachedRun == -1):\n",
    "                    print(\"Provided input from cache\")\n",
    "                else:\n",
    "                    print(\"Re-used Cached Value\")\n",
    "            return(self.cacheValue)\n",
    "        else:\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            # inputArr = np.array([])\n",
    "            inputArr = np.array([[]])\n",
    "\n",
    "            self.beingEvaluated = 1\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                if(inputArr.shape[1] > 0):      ##  Handle first situation when inputArr is empty.\n",
    "                    inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "                else:\n",
    "                    inputArr = self.input_layers[layrIndx].getActivation()\n",
    "\n",
    "\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "            ##  Checking dimensions of input matrix\n",
    "            if(len(inputArr.shape) == 1):\n",
    "                if(telementary): print(\"Input values should be a 2D array.\")\n",
    "                inputArr = inputArr[:, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "            # Checking if shape matches\n",
    "            if(inputArr.shape[0] > self.weights.shape[1]):\n",
    "                if(telementary): print(\"!!!SHAPE MISMATCH!!!\", \"inputArr.shape[0] =\", inputArr.shape[0], \"self.weights.shape[1] =\", self.weights.shape[1])\n",
    "                ## Adjust matrix dimension & adding new random weights to match size\n",
    "                generatedColumn = np.random.rand(self.weights.shape[0], (inputArr.shape[0] - self.weights.shape[1])) - 0.5\n",
    "                self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "\n",
    "            elif(inputArr.shape[0] < self.weights.shape[1]):       ## input layer may have been removed causing weight matrix to be larger than inputs\n",
    "                print(\"!! Input Layer smaller than expected. !!\", \"inputArr.shape[0] =\", inputArr.shape[0], \"self.weights.shape[1] =\", self.weights.shape[1])\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "            rawActivation = np.matmul(self.weights, inputArr) + self.bias\n",
    "            activation = self.applyActivationFn(rawActivation=rawActivation)\n",
    "\n",
    "            self.cachedRun = runNum\n",
    "            # self.cacheValue = activation          ## storing a pointer to activation calculated\n",
    "            self.cacheValue = np.copy(activation)   ## duplicating array\n",
    "\n",
    "            if(telementary): print(\"activation =\", activation, \"& cached\")  \n",
    "\n",
    "            return activation\n",
    "\n",
    "\n",
    "    def correct_error(self, activation_error):\n",
    "        # if(type(self.cacheValue)==type(None)):\n",
    "        #     self.\n",
    "        if(self.cachedRun >= 0):    ## check if is run before\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            inputArr = np.array([[]])\n",
    "            self.beingEvaluated = 1\n",
    "            layerLengths = []   ## Store each layer's length to distribute corrections to them later\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                layerLengths.append(self.input_layers[layrIndx].shape)\n",
    "                if(inputArr.shape[1] > 0):\n",
    "                    inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "                else:\n",
    "                    inputArr = self.input_layers[layrIndx].getActivation()\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "\n",
    "            # inputArr2 = inputArr[np.newaxis]\n",
    "\n",
    "            batch_size = activation_error.shape[1]\n",
    "\n",
    "            # if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\n",
    "            #     inputArrT = inputArr[np.newaxis].T\n",
    "            # else:\n",
    "            #     inputArrT = inputArr.T\n",
    "\n",
    "\n",
    "            # dZ = self.cacheValue - activation_error\n",
    "            dZ = activation_error\n",
    "\n",
    "            dW = (1/batch_size)*np.matmul(dZ, inputArr.T)\n",
    "            dB = (1/batch_size)*np.sum(dZ)\n",
    "\n",
    "            oldWeights = self.weights\n",
    "            ## Updating self weights & biases\n",
    "            self.weights = self.weights - learningRate*dW\n",
    "            self.bias = self.bias - learningRate*dB\n",
    "\n",
    "            ## Finding errors for input layers\n",
    "            # dIZ = np.matmul(np.transpose(self.weights),dZ)\n",
    "            dIZ = np.matmul((oldWeights.T), dZ) * self.applyDerivActivationFn(inputArr)\n",
    "\n",
    "            ## Splitting input corrections to their corresponding layers\n",
    "            splitPoints = [0]\n",
    "            lengthTillNow = 0\n",
    "            for layerIndx in range(len(layerLengths)):\n",
    "                lengthTillNow += layerLengths[layerIndx]\n",
    "                splitPoints.append(lengthTillNow)\n",
    "\n",
    "                self.input_layers[layerIndx].correct_error(dIZ[splitPoints[-2]:splitPoints[-1]])\n",
    "\n",
    "            return [self.cacheValue]\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    input_shape=1  # Currently only 1D\n",
    "    output_shape=1 # Currently only 1D\n",
    " \n",
    "    input_layer = None      ## Pointer to input nlayer\n",
    "    output_layer = None     ## Pointer to output nlayer\n",
    "\n",
    "    layers = []\n",
    "    numberOfLayers = 0      ## used to assign ID to new layer in matrix\n",
    "\n",
    "    adaptive = 1\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, insertDefault=0) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # Connect output with 1 adaptive neuron input\n",
    "        self.input_layer = nlayer(input_shape, isInput=1)\n",
    "\n",
    "        if(insertDefault==1):\n",
    "            hiddenLayer = nlayer(1,inputLayers=[self.input_layer],activationFn=\"relu\")\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[hiddenLayer], isDynamic=1)\n",
    "        else:\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[self.input_layer], isDynamic=1)\n",
    "\n",
    "\n",
    "    def addLayerAtLast(self, shape, isDynamic=1, activationFn=\"linear\"):\n",
    "        oldInputs = self.output_layer.input_layers\n",
    "        newLayer = nlayer(shape=shape, inputLayers=oldInputs, isDynamic=isDynamic, activationFn=activationFn)\n",
    "        newLayer.weights = self.output_layer.weights\n",
    "        self.output_layer.weights = None\n",
    "        self.output_layer.input_layers=[]\n",
    "        self.output_layer.addInputLayer(newLayer)\n",
    "\n",
    "        ## Transferring Weight matrix\n",
    "\n",
    "    def setInput(self, input_values):\n",
    "        # print(\"SETTING INPUT LAYER & STORING VALUES\")\n",
    "        if(type(self.input_layer) != type(None)):\n",
    "            if(len(input_values) < self.input_layer.shape):\n",
    "                print(\"ERROR: Unable to reduce input layer shape. Insert len(input values) >= input_shape\")\n",
    "            else:\n",
    "                self.input_layer.shape = len(input_values)\n",
    "                self.input_layer.cachedRun = -1\n",
    "                self.input_layer.isDynamic = 0\n",
    "                self.input_layer.cacheValue = np.array(input_values)\n",
    "        else:   ## Initialize new input layer\n",
    "                self.input_layer = nlayer(len(input_values), isInput=1, setInputValues=np.array(input_values))\n",
    "\n",
    "                \n",
    "                linker = self.output_layer\n",
    "                if(type(linker) != type(None)):\n",
    "                    while(len(linker.input_layers) > 0):    ## following only oldest (1st in list) links to reach input\n",
    "                        linker = linker.input_layers[0]                               \n",
    "                    linker.input_layer = [self.input_layer]\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self, input_values=None):    # find result activation from input activation and weights\n",
    "        global runNum\n",
    "        if(type(input_values) != type(None)):\n",
    "            self.input_layer.cacheValue = input_values\n",
    "\n",
    "        if(self.input_layer.cachedRun == -1 and type(self.input_layer.cacheValue) != type(None)):\n",
    "            output_activations = self.output_layer.getActivation()\n",
    "            runNum += 1\n",
    "            return output_activations\n",
    "        else:\n",
    "            print(\"Input uninitialized\")\n",
    "            return -1\n",
    "\n",
    "    def backward_prop(self, input_values, trueOutput):\n",
    "        global runNum\n",
    "        self.input_layer.cacheValue = input_values\n",
    "\n",
    "        if(telementary): print(\"getting forward prop predictions\")\n",
    "\n",
    "        predictedOutput = self.output_layer.getActivation()\n",
    "\n",
    "        if(telementary): print(\"starting backprop\")\n",
    "        predictions = self.output_layer.correct_error(predictedOutput - trueOutput)\n",
    "        runNum += 1\n",
    "        return predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = network(784, 10, insertDefault=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "\n",
    "nt.output_layer.activationFn = \"softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('mnist-train.csv')\n",
    "# Adaptive-Matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "X_trainT = X_train.T\n",
    "\n",
    "def one_hot(Y, maxExpected):\n",
    "    one_hot_Y = np.zeros((Y.size, maxExpected + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "## find the index of most probable number guessed by network\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "## find ratio of correct predictions to all data\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trainT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11452963],\n",
       "       [0.09934475],\n",
       "       [0.05360177],\n",
       "       [0.07270456],\n",
       "       [0.15829415],\n",
       "       [0.06190901],\n",
       "       [0.2381791 ],\n",
       "       [0.03075123],\n",
       "       [0.14226216],\n",
       "       [0.02842365]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telementary = 0\n",
    "out = (nt.forward_prop(X_train.T[0].T))\n",
    "telementary = 1\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "activation = [[5.38530324 4.57071658 0.28362176 ... 3.47333291 4.0276361  3.06415105]\n",
      " [0.5119491  2.47515365 0.         ... 2.67386402 1.63788273 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [3.60482738 0.         0.01530972 ... 6.36090621 3.95942382 3.28466385]\n",
      " [0.         0.         0.         ... 0.         0.         2.88317708]\n",
      " [1.41110897 0.7565865  0.         ... 0.         1.00644792 0.        ]] & cached\n",
      "activation = [[0.79707908 1.14811668 0.         ... 2.10023655 0.99539751 0.40653972]\n",
      " [2.04753067 1.90278643 0.         ... 0.42258991 1.55846223 0.09055716]\n",
      " [0.         0.         0.         ... 1.04882467 0.0651973  0.56414718]\n",
      " ...\n",
      " [0.         0.         0.11350083 ... 0.         0.         0.        ]\n",
      " [1.54486532 1.83627676 0.         ... 2.11924752 1.12363961 2.4525121 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.49777765]] & cached\n",
      "activation = [[0.11452963 0.11261101 0.07030187 ... 0.01329778 0.07343584 0.02124017]\n",
      " [0.09934475 0.03676331 0.1402811  ... 0.14556269 0.13400683 0.16521818]\n",
      " [0.05360177 0.07191561 0.01794609 ... 0.00808113 0.01401672 0.03034651]\n",
      " ...\n",
      " [0.03075123 0.07573937 0.12033003 ... 0.0748914  0.04561638 0.05209729]\n",
      " [0.14226216 0.1120613  0.21516777 ... 0.24639994 0.20773896 0.17964347]\n",
      " [0.02842365 0.05630857 0.0074192  ... 0.00775887 0.00714    0.00574769]] & cached\n",
      "[[0.11452963 0.11261101 0.07030187 ... 0.01329778 0.07343584 0.02124017]\n",
      " [0.09934475 0.03676331 0.1402811  ... 0.14556269 0.13400683 0.16521818]\n",
      " [0.05360177 0.07191561 0.01794609 ... 0.00808113 0.01401672 0.03034651]\n",
      " ...\n",
      " [0.03075123 0.07573937 0.12033003 ... 0.0748914  0.04561638 0.05209729]\n",
      " [0.14226216 0.1120613  0.21516777 ... 0.24639994 0.20773896 0.17964347]\n",
      " [0.02842365 0.05630857 0.0074192  ... 0.00775887 0.00714    0.00574769]]\n"
     ]
    }
   ],
   "source": [
    "out = (nt.forward_prop(X_train))\n",
    "print(out)\n",
    "# print(nt.forward_prop(X_train))\n",
    "# print(nt.forward_prop(X_train.T[0:2].T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting forward prop predictions\n",
      "Provided input from cache\n",
      "activation = [[5.38530324 4.57071658 0.28362176 ... 3.47333291 4.0276361  3.06415105]\n",
      " [0.5119491  2.47515365 0.         ... 2.67386402 1.63788273 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [3.60482738 0.         0.01530972 ... 6.36090621 3.95942382 3.28466385]\n",
      " [0.         0.         0.         ... 0.         0.         2.88317708]\n",
      " [1.41110897 0.7565865  0.         ... 0.         1.00644792 0.        ]] & cached\n",
      "activation = [[0.79707908 1.14811668 0.         ... 2.10023655 0.99539751 0.40653972]\n",
      " [2.04753067 1.90278643 0.         ... 0.42258991 1.55846223 0.09055716]\n",
      " [0.         0.         0.         ... 1.04882467 0.0651973  0.56414718]\n",
      " ...\n",
      " [0.         0.         0.11350083 ... 0.         0.         0.        ]\n",
      " [1.54486532 1.83627676 0.         ... 2.11924752 1.12363961 2.4525121 ]\n",
      " [0.         0.         0.         ... 0.         0.         0.49777765]] & cached\n",
      "activation = [[0.11452963 0.11261101 0.07030187 ... 0.01329778 0.07343584 0.02124017]\n",
      " [0.09934475 0.03676331 0.1402811  ... 0.14556269 0.13400683 0.16521818]\n",
      " [0.05360177 0.07191561 0.01794609 ... 0.00808113 0.01401672 0.03034651]\n",
      " ...\n",
      " [0.03075123 0.07573937 0.12033003 ... 0.0748914  0.04561638 0.05209729]\n",
      " [0.14226216 0.1120613  0.21516777 ... 0.24639994 0.20773896 0.17964347]\n",
      " [0.02842365 0.05630857 0.0074192  ... 0.00775887 0.00714    0.00574769]] & cached\n",
      "starting backprop\n",
      "Re-used Cached Value\n",
      "Activation Function = softmax  didn't match, returning as ReLU\n",
      "Re-used Cached Value\n",
      "Provided input from cache\n"
     ]
    }
   ],
   "source": [
    "Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "predictions = nt.backward_prop(input_values=X_train, trueOutput=Y_train_oneHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04781619  0.00595349  0.00704501  0.04998885 -0.00300287  0.08181268\n",
      "  -0.00114323 -0.00234215  0.07187464 -0.00654329]\n",
      " [-0.02851183  0.03851538  0.00809333 -0.02422061  0.02542729 -0.06588512\n",
      "  -0.00399128 -0.00731265  0.00786051  0.0119373 ]\n",
      " [ 0.06391461  0.09195657  0.03667291  0.08143166  0.00913149  0.07738329\n",
      "   0.00387466  0.00446119  0.05195837  0.00161715]\n",
      " [ 0.05279666  0.0322814   0.03640766  0.0565544  -0.0007606   0.04187297\n",
      "   0.00526429  0.00673634  0.048056   -0.0008378 ]\n",
      " [ 0.01277355 -0.03769351 -0.00627722 -0.01468997 -0.00830263 -0.034437\n",
      "  -0.00361026 -0.00091876 -0.00089168 -0.01762817]\n",
      " [-0.11535952 -0.10020584 -0.08771097 -0.16534689 -0.01744377 -0.11784822\n",
      "  -0.01107233 -0.00912338 -0.1285089  -0.0068546 ]\n",
      " [-0.04171493 -0.06551522 -0.01921415 -0.06260114 -0.00547124 -0.06407336\n",
      "   0.01594849  0.0079549  -0.08227586  0.017372  ]\n",
      " [-0.00584451  0.0205351  -0.00322728  0.03009845 -0.00075193  0.05765138\n",
      "  -0.00226119 -0.00160356  0.01450366 -0.00024477]\n",
      " [-0.02075653 -0.00305686  0.00975423 -0.05615418 -0.001919   -0.09022915\n",
      "  -0.00507753 -0.00770447 -0.00901957 -0.00615951]\n",
      " [ 0.0348863   0.01722949  0.01845648  0.10493942  0.00309324  0.11375253\n",
      "   0.00206837  0.00985253  0.02644284  0.00734169]]\n"
     ]
    }
   ],
   "source": [
    "telementary = 0\n",
    "\n",
    "oldWeights = nt.output_layer.weights\n",
    "\n",
    "Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "predictions = nt.backward_prop(input_values=X_train, trueOutput=Y_train_oneHot)\n",
    "\n",
    "newWeights = nt.output_layer.weights\n",
    "\n",
    "diffWeight = newWeights - oldWeights\n",
    "\n",
    "print(diffWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12476102 0.12450594 0.07358703 ... 0.01754357 0.08459271 0.02450812]\n",
      " [0.10018581 0.03892316 0.13957181 ... 0.14857635 0.13203048 0.16235255]\n",
      " [0.05839076 0.07863778 0.01939762 ... 0.01048137 0.0161731  0.03518837]\n",
      " ...\n",
      " [0.03008702 0.07500503 0.12190086 ... 0.07443209 0.04485464 0.05220022]\n",
      " [0.13372779 0.10999085 0.21180755 ... 0.25266259 0.20021159 0.17904962]\n",
      " [0.03070728 0.06034503 0.00814539 ... 0.00901936 0.00791292 0.00638979]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations = 0\n",
      "[0 0 4 ... 0 0 4] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.12921951219512195\n",
      "iterations = 10\n",
      "[0 0 7 ... 0 7 1] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.3607560975609756\n",
      "iterations = 20\n",
      "[0 2 9 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.34087804878048783\n",
      "iterations = 30\n",
      "[0 2 9 ... 4 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.4948780487804878\n",
      "iterations = 40\n",
      "[0 2 9 ... 4 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.6324146341463415\n",
      "iterations = 50\n",
      "[0 2 9 ... 4 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.6360243902439024\n",
      "iterations = 60\n",
      "[0 2 9 ... 4 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.6778048780487805\n",
      "iterations = 70\n",
      "[0 2 4 ... 4 7 5] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.6685853658536586\n",
      "iterations = 80\n",
      "[0 2 4 ... 4 7 5] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7091951219512195\n",
      "iterations = 90\n",
      "[0 2 9 ... 4 7 5] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7036097560975609\n",
      "iterations = 100\n",
      "[0 2 9 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7628780487804878\n",
      "iterations = 110\n",
      "[0 2 9 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7648048780487805\n",
      "iterations = 120\n",
      "[0 2 9 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7801219512195122\n",
      "iterations = 130\n",
      "[0 2 9 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7937073170731708\n",
      "iterations = 140\n",
      "[0 2 9 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.798219512195122\n",
      "iterations = 150\n",
      "[0 2 4 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7977073170731708\n",
      "iterations = 160\n",
      "[0 2 4 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8357317073170731\n",
      "iterations = 170\n",
      "[0 2 4 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8398048780487805\n",
      "iterations = 180\n",
      "[0 2 9 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8278780487804878\n",
      "iterations = 190\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8277317073170731\n",
      "iterations = 200\n",
      "[2 2 4 ... 9 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8278536585365853\n",
      "iterations = 210\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8575853658536585\n",
      "iterations = 220\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.833390243902439\n",
      "iterations = 230\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8642926829268293\n",
      "iterations = 240\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8678048780487805\n",
      "iterations = 250\n",
      "[2 2 4 ... 7 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.7912926829268293\n",
      "iterations = 260\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8729756097560976\n",
      "iterations = 270\n",
      "[4 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8207560975609756\n",
      "iterations = 280\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8785609756097561\n",
      "iterations = 290\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.876780487804878\n",
      "iterations = 300\n",
      "[4 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8232195121951219\n",
      "iterations = 310\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8810975609756098\n",
      "iterations = 320\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8453658536585366\n",
      "iterations = 330\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8884390243902439\n",
      "iterations = 340\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8898536585365854\n",
      "iterations = 350\n",
      "[2 2 4 ... 9 7 8] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8854878048780488\n",
      "iterations = 360\n",
      "[2 2 4 ... 7 7 3] [4 2 4 ... 9 7 8]\n",
      "Accuracy = 0.8264634146341463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_trainT)):\n\u001b[1;32m      7\u001b[0m     Y_train_oneHot \u001b[39m=\u001b[39m one_hot(Y_train, maxExpected\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     predictedRAW \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39;49mbackward_prop(input_values\u001b[39m=\u001b[39;49mX_train, trueOutput\u001b[39m=\u001b[39;49mY_train_oneHot)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m(it \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39miterations =\u001b[39m\u001b[39m\"\u001b[39m, it)\n",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m, in \u001b[0;36mnetwork.backward_prop\u001b[0;34m(self, input_values, trueOutput)\u001b[0m\n\u001b[1;32m     78\u001b[0m predictedOutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39mgetActivation()\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m(telementary): \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstarting backprop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mcorrect_error(predictedOutput \u001b[39m-\u001b[39;49m trueOutput)\n\u001b[1;32m     82\u001b[0m runNum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[0;32mIn[2], line 215\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error)\u001b[0m\n\u001b[1;32m    212\u001b[0m     lengthTillNow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layerLengths[layerIndx]\n\u001b[1;32m    213\u001b[0m     splitPoints\u001b[39m.\u001b[39mappend(lengthTillNow)\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layerIndx]\u001b[39m.\u001b[39;49mcorrect_error(dIZ[splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]:splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]])\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue]\n",
      "Cell \u001b[0;32mIn[2], line 215\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error)\u001b[0m\n\u001b[1;32m    212\u001b[0m     lengthTillNow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layerLengths[layerIndx]\n\u001b[1;32m    213\u001b[0m     splitPoints\u001b[39m.\u001b[39mappend(lengthTillNow)\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layerIndx]\u001b[39m.\u001b[39;49mcorrect_error(dIZ[splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]:splitPoints[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]])\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue]\n",
      "Cell \u001b[0;32mIn[2], line 206\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m-\u001b[39m learningRate\u001b[39m*\u001b[39mdB\n\u001b[1;32m    204\u001b[0m \u001b[39m## Finding errors for input layers\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m# dIZ = np.matmul(np.transpose(self.weights),dZ)\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m dIZ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul((oldWeights\u001b[39m.\u001b[39mT), dZ) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapplyDerivActivationFn(inputArr)\n\u001b[1;32m    208\u001b[0m \u001b[39m## Splitting input corrections to their corresponding layers\u001b[39;00m\n\u001b[1;32m    209\u001b[0m splitPoints \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# accuracySum = 0\n",
    "# runCount = 0\n",
    "\n",
    "telementary = 0\n",
    "\n",
    "for it in range(250):\n",
    "    Y_train_oneHot = one_hot(Y_train, maxExpected=9)\n",
    "    predictedRAW = nt.backward_prop(input_values=X_train, trueOutput=Y_train_oneHot)[0]\n",
    "\n",
    "    if(it % 10 == 0):\n",
    "        print(\"iterations =\", it)\n",
    "        predictions = get_predictions(predictedRAW)\n",
    "        print(\"Accuracy =\", get_accuracy(predictions, Y_train))\n",
    "    \n",
    "\n",
    "telementary = 1\n",
    "\n",
    "# # accuracy = accuracySum/len(X_train)\n",
    "# accuracy = accuracySum/runCount\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nt.output_layer.input_layers[0].input_layers[0].weights\n",
    "# print(wt.shape)\n",
    "\n",
    "# x1 = X_trainT[0:2]\n",
    "x1 = X_trainT[0]\n",
    "\n",
    "print(\"wt\", wt.shape)\n",
    "print(\"x1\", x1.shape)\n",
    "print(\"x1T\", x1.T.shape)\n",
    "\n",
    "act1 = np.matmul(wt, x1.T)\n",
    "act1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [5,6,7]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0), dtype=float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empt = np.array([[]])\n",
    "empt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(empt.shape[1] > 0):\n",
    "    empt = np.concatenate((empt, x))\n",
    "else:\n",
    "    empt = x\n",
    "\n",
    "print(empt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4],\n",
       "       [4, 6, 8]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[-1,0,1]])\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21379049  0.318942  ]\n",
      " [ 0.33342107 -0.20130355]\n",
      " [-0.22479054  0.23240682]\n",
      " [-0.10478904 -0.04715419]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(4,2) - 0.5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDerivActivationFn(input):\n",
    "    return (input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False],\n",
       "       [False,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applyDerivActivationFn(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(2,1)\n",
    "\n",
    "in1 = np.array([0,1,2])\n",
    "wtMat = np.array([[5,6,7],[8,9,10]])\n",
    "# biases = np.array([5,25])\n",
    "biases = np.array([0.5,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.5  29.25]\n"
     ]
    }
   ],
   "source": [
    "output_activations = np.matmul(wtMat, in1) + biases\n",
    "print(output_activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb745f7c98c08174c20a2b94d108f69dcc884658473fbc3700597e1ee8c317ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
