{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "telementary = 1\n",
    "runNum = 0      ## Increment to utilise caching\n",
    "batch_size=100       ## assumed size of dataset\n",
    "learningRate = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking in input shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlayer:\n",
    "    id = 0\n",
    "    shape = 1               ## Defines self dimension (1D)\n",
    "    input_layers = []        ## store layer pointer\n",
    "    weights = None          ## assuming all input activations are concatenated (sorted on layer ID).\n",
    "    bias = np.array([])      ## store self biases\n",
    "    activationFn = \"linear\"         ## store self activation function\n",
    "\n",
    "\n",
    "    ## Caching\n",
    "    \n",
    "    #### store last activation as cache to speed up when multiple layers use this layer as input. So this is evaluated only once.\n",
    "    cachedRun = -2      # runNum when cache was calculated, can be old\n",
    "    ## cachedRun = -1 & isAdaptive = 0 is for input layers\n",
    "    cacheValue = None\n",
    "    \n",
    "    ## Flag indicating if it was being evaluated.\n",
    "    #### This can help in case of self loops, when a layer was being evaluated was evaluated again\n",
    "    #  meaning one of this layer's input_layer has this layer as one of the inputs (called self-loop is a graph).\n",
    "    #  In this situation, the last cached value of this layer will be returned.\n",
    "    # this may be used to simulate LSTM Network.\n",
    "    beingEvaluated = 0  \n",
    "\n",
    "    ## Error variance\n",
    "    #### Store absolute sum of errors in terms of array of sum per node in 1D np array\n",
    "\n",
    "    ## supports changing widths & depths. Not suitable for inputs and outputs\n",
    "    isDynamic = 1\n",
    "    \n",
    "\n",
    "\n",
    "    ## Methods\n",
    "    def __init__(self, shape=1, inputLayers=[], isInput=0, setInputValues=[], activationFn=\"linear\", isDynamic=0) -> None:\n",
    "        self.shape = shape\n",
    "        self.activationFn = activationFn\n",
    "        self.bias = np.zeros(shape)\n",
    "        self.input_layers = []  ## Clearing on reinitializing\n",
    "\n",
    "        if(isDynamic==0):\n",
    "            self.isDynamic = 0\n",
    "        if(isInput):\n",
    "            self.cachedRun = -1\n",
    "            self.isDynamic = 0\n",
    "            if(len(setInputValues) != 0):\n",
    "                self.cacheValue = np.array(setInputValues)\n",
    "        else:\n",
    "            # generating random weights if given\n",
    "            if(type(inputLayers) == type([])):\n",
    "                if(len(inputLayers) != 0):\n",
    "                    for layer in inputLayers:\n",
    "                        self.addInputLayer(layer)\n",
    "            else:\n",
    "                print(\"inputLayers should be a List.\")\n",
    "                if(type(inputLayers) == type(nlayer(1))):\n",
    "                    self.addInputLayer(inputLayers)\n",
    "\n",
    "    def addInputLayer(self, newInputLayer):\n",
    "        # check if it doesn't already exists\n",
    "        for layr in self.input_layers:\n",
    "            if(newInputLayer == layr):\n",
    "                print(\"Layer already exists.\")\n",
    "                return -1\n",
    "\n",
    "        self.input_layers.append(newInputLayer)\n",
    "        ## DONE: Generate random weights\n",
    "        generatedColumn = np.random.rand(self.shape, newInputLayer.shape) - 0.5\n",
    "        if(type(self.weights) == type(None)):\n",
    "            self.weights = generatedColumn\n",
    "        else:\n",
    "            self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "    def addWidth_to_Layer(self, addWidth):\n",
    "        if(addWidth > 0):\n",
    "            self.shape += addWidth\n",
    "            self.bias = np.concatenate((self.bias, (np.random.rand(addWidth) - 0.5)))\n",
    "            \n",
    "            ## generating new row of random weights\n",
    "            generatedRow = np.random.rand(addWidth, self.weights.shape[1]) - 0.5\n",
    "            self.weights = np.concatenate((self.weights, generatedRow))\n",
    "        else:\n",
    "            print(\"error, doesn't support decrease.\")\n",
    "\n",
    "\n",
    "    def applyActivationFn(self,rawActivation):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return rawActivation\n",
    "\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return np.maximum(rawActivation, 0)\n",
    "\n",
    "        if(self.activationFn == \"softmax\"):\n",
    "            A = np.exp(rawActivation) / sum(np.exp(rawActivation))\n",
    "            return A\n",
    " \n",
    "    def applyDerivActivationFn(self, input):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return 1\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return (input > 0)\n",
    "            \n",
    "      \n",
    "\n",
    "    def getActivation(self):    ## return np array of activation of current layer\n",
    "        ## beingEvaluated == 1 means the node was triggered by a loop in the network. Returning last value cached prevents infinite loops.\n",
    "\n",
    "        \n",
    "        if(self.cachedRun == runNum or self.cachedRun == -1 or self.beingEvaluated == 1):   ## if activation was already calculated for this run OR is an input layer\n",
    "            if(telementary): \n",
    "                if(self.cachedRun == -1):\n",
    "                    print(\"Provided input from cache\")\n",
    "                else:\n",
    "                    print(\"Re-used Cached Value\")\n",
    "            return(self.cacheValue)\n",
    "        else:\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            # inputArr = np.array([])\n",
    "            inputArr = np.array([[]])\n",
    "\n",
    "            self.beingEvaluated = 1\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                if(inputArr.shape[1] > 0):      ##  Handle first situation when inputArr is empty.\n",
    "                    inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "                else:\n",
    "                    inputArr = self.input_layers[layrIndx].getActivation()\n",
    "\n",
    "\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "            # Checking if shape matches\n",
    "            if(inputArr.shape[0] > self.weights.shape[1]):\n",
    "                if(telementary): print(\"!!!SHAPE MISMATCH!!!\")  \n",
    "                ## Adjust matrix dimension & adding new random weights to match size\n",
    "                generatedColumn = np.random.rand(self.weights.shape[0], (inputArr.shape[0] - self.weights.shape[1])) - 0.5\n",
    "                self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "\n",
    "            elif(inputArr.shape[0] < self.weights.shape[1]):       ## input layer may have been removed causing weight matrix to be larger than inputs\n",
    "                print(\"!! Input Layer smaller than expected. !!\")\n",
    "                print(\"inputArr.shape[0] =\", inputArr.shape[0], \"self.weights.shape[1] =\", self.weights.shape[1])\n",
    "                return -1\n",
    "            \n",
    "            rawActivation = np.matmul(self.weights, inputArr.T) + self.bias\n",
    "            activation = self.applyActivationFn(rawActivation=rawActivation)\n",
    "\n",
    "            self.cachedRun = runNum\n",
    "            # self.cacheValue = activation          ## storing a pointer to activation calculated\n",
    "            self.cacheValue = np.copy(activation)   ## duplicating array\n",
    "\n",
    "            if(telementary): print(\"activation =\", activation, \"& cached\")  \n",
    "\n",
    "            return activation\n",
    "\n",
    "\n",
    "    def correct_error(self, activation_error):\n",
    "        # if(type(self.cacheValue)==type(None)):\n",
    "        #     self.\n",
    "        if(self.cachedRun >= 0):    ## check if is run before\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            inputArr = np.array([])\n",
    "            self.beingEvaluated = 1\n",
    "            layerLengths = []   ## Store each layer's length to distribute corrections to them later\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                layerLengths.append(self.input_layers[layrIndx].shape)\n",
    "                if(inputArr.shape[1] > 0):\n",
    "                    inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "                else:\n",
    "                    inputArr = self.input_layers[layrIndx].getActivation()\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "\n",
    "            inputArr2 = inputArr[np.newaxis]\n",
    "\n",
    "            # if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\n",
    "            #     inputArrT = inputArr[np.newaxis].T\n",
    "            # else:\n",
    "            #     inputArrT = inputArr.T\n",
    "\n",
    "\n",
    "            dZ = self.cacheValue - activation_error\n",
    "\n",
    "            dW = (1/batch_size)*np.matmul(dZ, inputArr2)\n",
    "            dB = (1/batch_size)*np.sum(dZ)\n",
    "\n",
    "            oldWeights = self.weights\n",
    "            ## Updating self weights & biases\n",
    "            self.weights = self.weights - learningRate*dW\n",
    "            self.bias = self.bias - learningRate*dB\n",
    "\n",
    "            ## Finding errors for input layers\n",
    "            # dIZ = np.matmul(np.transpose(self.weights),dZ)\n",
    "            dIZ = np.matmul((oldWeights.T), dZ) * self.applyDerivActivationFn(inputArr)\n",
    "\n",
    "            ## Splitting input corrections to their corresponding layers\n",
    "            splitPoints = [0]\n",
    "            lengthTillNow = 0\n",
    "            for layerIndx in range(len(layerLengths)):\n",
    "                lengthTillNow += layerLengths[layerIndx]\n",
    "                splitPoints.append(lengthTillNow)\n",
    "\n",
    "                self.input_layers[layerIndx].correct_error(dIZ[splitPoints[-2]:splitPoints[-1]])\n",
    "\n",
    "            return [self.cacheValue]\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    input_shape=1  # Currently only 1D\n",
    "    output_shape=1 # Currently only 1D\n",
    " \n",
    "    input_layer = None      ## Pointer to input nlayer\n",
    "    output_layer = None     ## Pointer to output nlayer\n",
    "\n",
    "    layers = []\n",
    "    numberOfLayers = 0      ## used to assign ID to new layer in matrix\n",
    "\n",
    "    adaptive = 1\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, insertDefault=0) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # Connect output with 1 adaptive neuron input\n",
    "        self.input_layer = nlayer(input_shape, isInput=1)\n",
    "\n",
    "        if(insertDefault==1):\n",
    "            hiddenLayer = nlayer(1,inputLayers=[self.input_layer],activationFn=\"relu\")\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[hiddenLayer], isDynamic=1)\n",
    "        else:\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[self.input_layer], isDynamic=1)\n",
    "\n",
    "\n",
    "    def addLayerAtLast(self, shape, isDynamic=1, activationFn=\"linear\"):\n",
    "        oldInputs = self.output_layer.input_layers\n",
    "        newLayer = nlayer(shape=shape, inputLayers=oldInputs, isDynamic=isDynamic, activationFn=activationFn)\n",
    "        newLayer.weights = self.output_layer.weights\n",
    "        self.output_layer.weights = None\n",
    "        self.output_layer.input_layers=[]\n",
    "        self.output_layer.addInputLayer(newLayer)\n",
    "\n",
    "        ## Transferring Weight matrix\n",
    "\n",
    "    def setInput(self, input_values):\n",
    "        # print(\"SETTING INPUT LAYER & STORING VALUES\")\n",
    "        if(type(self.input_layer) != type(None)):\n",
    "            if(len(input_values) < self.input_layer.shape):\n",
    "                print(\"ERROR: Unable to reduce input layer shape. Insert len(input values) >= input_shape\")\n",
    "            else:\n",
    "                self.input_layer.shape = len(input_values)\n",
    "                self.input_layer.cachedRun = -1\n",
    "                self.input_layer.isDynamic = 0\n",
    "                self.input_layer.cacheValue = np.array(input_values)\n",
    "        else:   ## Initialize new input layer\n",
    "                self.input_layer = nlayer(len(input_values), isInput=1, setInputValues=np.array(input_values))\n",
    "\n",
    "                \n",
    "                linker = self.output_layer\n",
    "                if(type(linker) != type(None)):\n",
    "                    while(len(linker.input_layers) > 0):    ## following only oldest (1st in list) links to reach input\n",
    "                        linker = linker.input_layers[0]                               \n",
    "                    linker.input_layer = [self.input_layer]\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self, input_values=None):    # find result activation from input activation and weights\n",
    "        global runNum\n",
    "        if(type(input_values) != type(None)):\n",
    "            self.input_layer.cacheValue = input_values\n",
    "\n",
    "        if(self.input_layer.cachedRun == -1 and type(self.input_layer.cacheValue) != type(None)):\n",
    "            output_activations = self.output_layer.getActivation()\n",
    "            runNum += 1\n",
    "            return output_activations\n",
    "        else:\n",
    "            print(\"Input uninitialized\")\n",
    "            return -1\n",
    "\n",
    "    def backward_prop(self, input_values, trueOutput):\n",
    "        self.input_layer.cacheValue = input_values\n",
    "\n",
    "        predictedOutput = self.output_layer.getActivation()\n",
    "        \n",
    "        return self.output_layer.correct_error(trueOutput - predictedOutput)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = network(784, 10, insertDefault=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('mnist-train.csv')\n",
    "# Adaptive-Matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "X_trainT = X_train.T\n",
    "\n",
    "def one_hot(Y, maxExpected):\n",
    "    one_hot_Y = np.zeros((Y.size, maxExpected + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "## find the most probable guessed by network\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trainT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.output_layer.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nt.forward_prop(X_trainT[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "!! Input Layer smaller than expected. !!\n",
      "inputArr.shape[0] = 2 self.weights.shape[1] = 784\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(nt\u001b[39m.\u001b[39;49mforward_prop(X_trainT[\u001b[39m0\u001b[39;49m:\u001b[39m2\u001b[39;49m]))\n",
      "Cell \u001b[0;32mIn[105], line 65\u001b[0m, in \u001b[0;36mnetwork.forward_prop\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue \u001b[39m=\u001b[39m input_values\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcachedRun \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue) \u001b[39m!=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m---> 65\u001b[0m     output_activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mgetActivation()\n\u001b[1;32m     66\u001b[0m     runNum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m output_activations\n",
      "Cell \u001b[0;32mIn[104], line 125\u001b[0m, in \u001b[0;36mnlayer.getActivation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         inputArr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((inputArr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layers[layrIndx]\u001b[39m.\u001b[39mgetActivation()))\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m         inputArr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layers[layrIndx]\u001b[39m.\u001b[39;49mgetActivation()\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeingEvaluated \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39m# Checking if shape matches\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[104], line 131\u001b[0m, in \u001b[0;36mnlayer.getActivation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeingEvaluated \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39m# Checking if shape matches\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[39mif\u001b[39;00m(inputArr\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m(telementary): \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m!!!SHAPE MISMATCH!!!\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[1;32m    133\u001b[0m     \u001b[39m## Adjust matrix dimension & adding new random weights to match size\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(nt.forward_prop(X_trainT[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "activation = [34.5778307  40.3057012  36.65048974 36.13773697 39.60475285 38.40595201\n",
      " 36.18061522 33.5145827  40.8114093  35.43933799] & cached\n",
      "activation = [194.43320581 181.71247634 155.29674334 208.59397513 168.15118968\n",
      " 226.45109374 230.2225758  200.42286536 152.38421142 169.54796298] & cached\n",
      "activation = [ 972.43402768 1287.642818    807.98647891  847.43150933  722.90491912\n",
      "  881.09102522  852.34527912  824.60244821  916.85821253 1100.53535724] & cached\n",
      "Re-used Cached Value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_trainT)):\n\u001b[1;32m      5\u001b[0m     one_hot_y \u001b[39m=\u001b[39m one_hot(Y_train[it], \u001b[39m9\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     pred \u001b[39m=\u001b[39m get_predictions(nt\u001b[39m.\u001b[39;49mbackward_prop(X_trainT[it],one_hot_y))\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m(pred\u001b[39m.\u001b[39mall() \u001b[39m==\u001b[39m one_hot_y\u001b[39m.\u001b[39mall()):\n\u001b[1;32m      8\u001b[0m         accuracySum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mnetwork.backward_prop\u001b[0;34m(self, input_values, trueOutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue \u001b[39m=\u001b[39m input_values\n\u001b[1;32m     75\u001b[0m predictedOutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39mgetActivation()\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mcorrect_error(trueOutput \u001b[39m-\u001b[39;49m predictedOutput)\n",
      "Cell \u001b[0;32mIn[2], line 175\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m# if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m#     inputArrT = inputArr[np.newaxis].T\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m#     inputArrT = inputArr.T\u001b[39;00m\n\u001b[1;32m    173\u001b[0m dZ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue \u001b[39m-\u001b[39m activation_error\n\u001b[0;32m--> 175\u001b[0m dW \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mbatch_size)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ, inputArr2)\n\u001b[1;32m    176\u001b[0m dB \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mbatch_size)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ)\n\u001b[1;32m    178\u001b[0m oldWeights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
     ]
    }
   ],
   "source": [
    "accuracySum = 0\n",
    "runCount = 0\n",
    "\n",
    "for it in range(len(X_trainT)):\n",
    "    one_hot_y = one_hot(Y_train[it], 9)\n",
    "    pred = get_predictions(nt.backward_prop(X_trainT[it],one_hot_y))\n",
    "    if(pred.all() == one_hot_y.all()):\n",
    "        accuracySum += 1\n",
    "    runCount += 1\n",
    "    \n",
    "# accuracy = accuracySum/len(X_train)\n",
    "accuracy = accuracySum/runCount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nt.output_layer.input_layers[0].input_layers[0].weights\n",
    "# print(wt.shape)\n",
    "\n",
    "# x1 = X_trainT[0:2]\n",
    "x1 = X_trainT[0]\n",
    "\n",
    "print(\"wt\", wt.shape)\n",
    "print(\"x1\", x1.shape)\n",
    "print(\"x1T\", x1.T.shape)\n",
    "\n",
    "act1 = np.matmul(wt, x1.T)\n",
    "act1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [5,6,7]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(1, 0), dtype=float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empt = np.array([[]])\n",
    "empt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]\n",
      " [1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(empt.shape[1] > 0):\n",
    "    empt = np.concatenate((empt, x))\n",
    "else:\n",
    "    empt = x\n",
    "\n",
    "print(empt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4],\n",
       "       [4, 6, 8]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([[-1,0,1]])\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12561546 -0.19739658]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(1,2) - 0.5\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(2,1)\n",
    "\n",
    "in1 = np.array([0,1,2])\n",
    "wtMat = np.array([[5,6,7],[8,9,10]])\n",
    "# biases = np.array([5,25])\n",
    "biases = np.array([0.5,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.5  29.25]\n"
     ]
    }
   ],
   "source": [
    "output_activations = np.matmul(wtMat, in1) + biases\n",
    "print(output_activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb745f7c98c08174c20a2b94d108f69dcc884658473fbc3700597e1ee8c317ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
