{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "telementary = 1\n",
    "runNum = 0      ## Increment to utilise caching\n",
    "k=100       ## assumed size of dataset\n",
    "learningRate = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking in input shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlayer:\n",
    "    id = 0\n",
    "    shape = 1               ## Defines self dimension (1D)\n",
    "    input_layers = []        ## store layer pointer\n",
    "    weights = None          ## assuming all input activations are concatenated (sorted on layer ID).\n",
    "    bias = np.array([])      ## store self biases\n",
    "    activationFn = \"linear\"         ## store self activation function\n",
    "\n",
    "\n",
    "    ## Caching\n",
    "    \n",
    "    #### store last activation as cache to speed up when multiple layers use this layer as input. So this is evaluated only once.\n",
    "    cachedRun = -2      # runNum when cache was calculated, can be old\n",
    "    ## cachedRun = -1 & isAdaptive = 0 is for input layers\n",
    "    cacheValue = None\n",
    "    \n",
    "    ## Flag indicating if it was being evaluated.\n",
    "    #### This can help in case of self loops, when a layer was being evaluated was evaluated again\n",
    "    #  meaning one of this layer's input_layer has this layer as one of the inputs (called self-loop is a graph).\n",
    "    #  In this situation, the last cached value of this layer will be returned.\n",
    "    # this may be used to simulate LSTM Network.\n",
    "    beingEvaluated = 0  \n",
    "\n",
    "    ## Error variance\n",
    "    #### Store absolute sum of errors in terms of array of sum per node in 1D np array\n",
    "\n",
    "    ## supports changing widths & depths. Not suitable for inputs and outputs\n",
    "    isDynamic = 1\n",
    "    \n",
    "\n",
    "\n",
    "    ## Methods\n",
    "    def __init__(self, shape=1, inputLayers=[], isInput=0, setInputValues=[], activationFn=\"linear\", isDynamic=0) -> None:\n",
    "        self.shape = shape\n",
    "        self.activationFn = activationFn\n",
    "        self.bias = np.zeros(shape)\n",
    "        self.input_layers = []  ## Clearing on reinitializing\n",
    "\n",
    "        if(isDynamic==0):\n",
    "            self.isDynamic = 0\n",
    "        if(isInput):\n",
    "            self.cachedRun = -1\n",
    "            self.isDynamic = 0\n",
    "            if(len(setInputValues) != 0):\n",
    "                self.cacheValue = np.array(setInputValues)\n",
    "        else:\n",
    "            # generating random weights if given\n",
    "            if(type(inputLayers) == type([])):\n",
    "                if(len(inputLayers) != 0):\n",
    "                    for layer in inputLayers:\n",
    "                        self.addInputLayer(layer)\n",
    "            else:\n",
    "                print(\"inputLayers should be a List.\")\n",
    "                if(type(inputLayers) == type(nlayer(1))):\n",
    "                    self.addInputLayer(inputLayers)\n",
    "\n",
    "    def addInputLayer(self, newInputLayer):\n",
    "        # check if it doesn't already exists\n",
    "        for layr in self.input_layers:\n",
    "            if(newInputLayer == layr):\n",
    "                print(\"Layer already exists.\")\n",
    "                return -1\n",
    "\n",
    "        self.input_layers.append(newInputLayer)\n",
    "        ## DONE: Generate random weights\n",
    "        generatedColumn = np.random.rand(self.shape, newInputLayer.shape)\n",
    "        if(type(self.weights) == type(None)):\n",
    "            self.weights = generatedColumn\n",
    "        else:\n",
    "            self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "    def addWidth_to_Layer(self, addWidth):\n",
    "        if(addWidth > 0):\n",
    "            self.shape += addWidth\n",
    "            self.bias = np.concatenate((self.bias, np.random.rand(addWidth)))\n",
    "            \n",
    "            ## generating new row of random weights\n",
    "            generatedRow = np.random.rand(addWidth, self.weights.shape[1])\n",
    "            self.weights = np.concatenate((self.weights, generatedRow))\n",
    "        else:\n",
    "            print(\"error, doesn't support decrease.\")\n",
    "\n",
    "\n",
    "    def applyActivationFn(self,rawActivation):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return rawActivation\n",
    "\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return np.maximum(rawActivation, 0)\n",
    "        if(self.activationFn == \"softmax\"):\n",
    "            A = np.exp(Z) / sum(np.exp(Z))\n",
    "            return A\n",
    " \n",
    "    def applyDerivActivationFn(self, input):\n",
    "        if(self.activationFn == \"linear\"):\n",
    "            return 1\n",
    "        if(self.activationFn == \"relu\"):\n",
    "            return (input > 0)\n",
    "            \n",
    "      \n",
    "\n",
    "    def getActivation(self):    ## return np array of activation of current layer\n",
    "        ## beingEvaluated == 1 means the node was triggered by a loop in the network. Returning last value cached prevents infinite loops.\n",
    "\n",
    "        \n",
    "        if(self.cachedRun == runNum or self.cachedRun == -1 or self.beingEvaluated == 1):   ## if activation was already calculated for this run OR is an input layer\n",
    "            if(telementary): \n",
    "                if(self.cachedRun == -1):\n",
    "                    print(\"Provided input from cache\")\n",
    "                else:\n",
    "                    print(\"Re-used Cached Value\")\n",
    "            return(self.cacheValue)\n",
    "        else:\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            inputArr = np.array([])\n",
    "\n",
    "            self.beingEvaluated = 1\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "\n",
    "\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "            # Checking if shape matches\n",
    "            if(inputArr.shape[0] > self.weights.shape[1]):\n",
    "                if(telementary): print(\"!!!SHAPE MISMATCH!!!\")  \n",
    "                ## Adjust matrix dimension & adding new random weights to match size\n",
    "                generatedColumn = np.random.rand(self.weights.shape[0], (inputArr.shape[0] - self.weights.shape[1]))\n",
    "                self.weights = np.concatenate((self.weights, generatedColumn), axis=1)\n",
    "\n",
    "\n",
    "            elif(inputArr.shape[0] < self.weights.shape[1]):       ## input layer may have been removed causing weight matrix to be larger than inputs\n",
    "                print(\"!! Input Layer smaller than expected. !!\")\n",
    "                print(\"inputArr.shape[0] =\", inputArr.shape[0], \"self.weights.shape[1] =\", self.weights.shape[1])\n",
    "                return -1\n",
    "            \n",
    "            rawActivation = np.matmul(self.weights, inputArr) + self.bias\n",
    "            activation = self.applyActivationFn(rawActivation=rawActivation)\n",
    "\n",
    "            self.cachedRun = runNum\n",
    "            # self.cacheValue = activation          ## storing a pointer to activation calculated\n",
    "            self.cacheValue = np.copy(activation)   ## duplicating array\n",
    "\n",
    "            if(telementary): print(\"activation =\", activation, \"& cached\")  \n",
    "\n",
    "            return activation\n",
    "\n",
    "\n",
    "    def correct_error(self, activation_error):\n",
    "        # if(type(self.cacheValue)==type(None)):\n",
    "        #     self.\n",
    "        if(self.cachedRun >= 0):    ## check if is run before\n",
    "            ## compiling a numpy array of all activation values listed in input layer. \n",
    "            inputArr = np.array([])\n",
    "            self.beingEvaluated = 1\n",
    "            layerLengths = []   ## Store each layer's length to distribute corrections to them later\n",
    "\n",
    "            for layrIndx in range(len(self.input_layers)):\n",
    "                layerLengths.append(self.input_layers[layrIndx].shape)\n",
    "                inputArr = np.concatenate((inputArr, self.input_layers[layrIndx].getActivation()))\n",
    "            self.beingEvaluated = 0\n",
    "\n",
    "\n",
    "            inputArr2 = inputArr[np.newaxis]\n",
    "\n",
    "            # if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\n",
    "            #     inputArrT = inputArr[np.newaxis].T\n",
    "            # else:\n",
    "            #     inputArrT = inputArr.T\n",
    "\n",
    "\n",
    "            dZ = self.cacheValue - activation_error\n",
    "\n",
    "            dW = (1/k)*np.matmul(dZ, inputArr2)\n",
    "            dB = (1/k)*np.sum(dZ)\n",
    "\n",
    "            oldWeights = self.weights\n",
    "            ## Updating self weights & biases\n",
    "            self.weights = self.weights - learningRate*dW\n",
    "            self.bias = self.bias - learningRate*dB\n",
    "\n",
    "            ## Finding errors for input layers\n",
    "            # dIZ = np.matmul(np.transpose(self.weights),dZ)\n",
    "            dIZ = np.matmul((oldWeights.T), dZ) * self.applyDerivActivationFn(inputArr)\n",
    "\n",
    "            ## Splitting input corrections to their corresponding layers\n",
    "            splitPoints = [0]\n",
    "            lengthTillNow = 0\n",
    "            for layerIndx in range(len(layerLengths)):\n",
    "                lengthTillNow += layerLengths[layerIndx]\n",
    "                splitPoints.append(lengthTillNow)\n",
    "\n",
    "                self.input_layers[layerIndx].correct_error(dIZ[splitPoints[-2]:splitPoints[-1]])\n",
    "\n",
    "            return [self.cacheValue]\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    input_shape=1  # Currently only 1D\n",
    "    output_shape=1 # Currently only 1D\n",
    " \n",
    "    input_layer = None      ## Pointer to input nlayer\n",
    "    output_layer = None     ## Pointer to output nlayer\n",
    "\n",
    "    layers = []\n",
    "    numberOfLayers = 0      ## used to assign ID to new layer in matrix\n",
    "\n",
    "    adaptive = 1\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, insertDefault=0) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # Connect output with 1 adaptive neuron input\n",
    "        self.input_layer = nlayer(input_shape, isInput=1)\n",
    "\n",
    "        if(insertDefault==1):\n",
    "            hiddenLayer = nlayer(1,inputLayers=[self.input_layer],activationFn=\"relu\")\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[hiddenLayer], isDynamic=1)\n",
    "        else:\n",
    "            self.output_layer = nlayer(output_shape, inputLayers=[self.input_layer], isDynamic=1)\n",
    "\n",
    "\n",
    "    def addLayerAtLast(self, shape, isDynamic=1, activationFn=\"linear\"):\n",
    "        oldInputs = self.output_layer.input_layers\n",
    "        newLayer = nlayer(shape=shape, inputLayers=oldInputs, isDynamic=isDynamic, activationFn=activationFn)\n",
    "        newLayer.weights = self.output_layer.weights\n",
    "        self.output_layer.weights = None\n",
    "        self.output_layer.input_layers=[]\n",
    "        self.output_layer.addInputLayer(newLayer)\n",
    "\n",
    "        ## Transferring Weight matrix\n",
    "\n",
    "    def setInput(self, input_values):\n",
    "        # print(\"SETTING INPUT LAYER & STORING VALUES\")\n",
    "        if(type(self.input_layer) != type(None)):\n",
    "            if(len(input_values) < self.input_layer.shape):\n",
    "                print(\"ERROR: Unable to reduce input layer shape. Insert len(input values) >= input_shape\")\n",
    "            else:\n",
    "                self.input_layer.shape = len(input_values)\n",
    "                self.input_layer.cachedRun = -1\n",
    "                self.input_layer.isDynamic = 0\n",
    "                self.input_layer.cacheValue = np.array(input_values)\n",
    "        else:   ## Initialize new input layer\n",
    "                self.input_layer = nlayer(len(input_values), isInput=1, setInputValues=np.array(input_values))\n",
    "\n",
    "                \n",
    "                linker = self.output_layer\n",
    "                if(type(linker) != type(None)):\n",
    "                    while(len(linker.input_layers) > 0):    ## following only oldest (1st in list) links to reach input\n",
    "                        linker = linker.input_layers[0]                               \n",
    "                    linker.input_layer = [self.input_layer]\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self, input_values=None):    # find result activation from input activation and weights\n",
    "        global runNum\n",
    "        if(type(input_values) != type(None)):\n",
    "            self.input_layer.cacheValue = input_values\n",
    "\n",
    "        if(self.input_layer.cachedRun == -1 and type(self.input_layer.cacheValue) != type(None)):\n",
    "            output_activations = self.output_layer.getActivation()\n",
    "            runNum += 1\n",
    "            return output_activations\n",
    "        else:\n",
    "            print(\"Input uninitialized\")\n",
    "            return -1\n",
    "\n",
    "    def backward_prop(self, input_values, trueOutput):\n",
    "        self.input_layer.cacheValue = input_values\n",
    "\n",
    "        predictedOutput = self.output_layer.getActivation()\n",
    "        \n",
    "        return self.output_layer.correct_error(trueOutput - predictedOutput)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = network(784, 10, insertDefault=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")\n",
    "nt.addLayerAtLast(10,isDynamic=0,activationFn=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('mnist-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "X_trainT = X_train.T\n",
    "\n",
    "def one_hot(Y, maxExpected):\n",
    "    one_hot_Y = np.zeros((Y.size, maxExpected + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "## find the most probable guessed by network\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "telementary = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trainT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNum = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "activation = [43.546764   53.10551581 47.09198547 49.83707745 45.33959999 48.93422512\n",
      " 49.12711997 45.77491603 43.69927044 51.34468906] & cached\n",
      "activation = [252.09332699 242.82509197 256.76448106 185.45450935 191.50785967\n",
      " 341.27766395 229.05875157 176.65125559 180.06905017 257.47177331] & cached\n",
      "activation = [ 885.94460699 1223.12284865 1173.48071959 1379.26932166  984.12192475\n",
      "  970.73582389 1396.2793819  1085.62100297 1136.85461841 1460.29327339] & cached\n",
      "[ 885.94460699 1223.12284865 1173.48071959 1379.26932166  984.12192475\n",
      "  970.73582389 1396.2793819  1085.62100297 1136.85461841 1460.29327339]\n"
     ]
    }
   ],
   "source": [
    "print(nt.forward_prop(X_trainT[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nt.output_layer.cachedRun\n",
    "nt.output_layer.input_layers[0].cachedRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_yTEST = one_hot(Y_train[0], 9)\n",
    "print(one_hot_yTEST) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided input from cache\n",
      "activation = [34.85892915 43.17644113 37.71560267 41.87985868 38.88328244 34.79771956\n",
      " 40.01467537 40.64950868 33.69887808 38.73568654] & cached\n",
      "activation = [203.02878551 194.22660707 207.51177633 148.18399825 156.46385964\n",
      " 271.9075824  185.72602105 141.0139782  145.01433635 208.57230914] & cached\n",
      "activation = [ 713.88356574  986.24371744  945.9522797  1109.41114889  792.47459727\n",
      "  780.84847419 1122.97428945  875.75381593  916.99325241 1173.94844948] & cached\n",
      "Re-used Cached Value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[366], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_trainT)):\n\u001b[1;32m      5\u001b[0m     one_hot_y \u001b[39m=\u001b[39m one_hot(Y_train[it], \u001b[39m9\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     pred \u001b[39m=\u001b[39m get_predictions(nt\u001b[39m.\u001b[39;49mbackward_prop(X_trainT[it],one_hot_y))\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m(pred\u001b[39m.\u001b[39mall() \u001b[39m==\u001b[39m one_hot_y\u001b[39m.\u001b[39mall()):\n\u001b[1;32m      8\u001b[0m         accuracySum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[332], line 77\u001b[0m, in \u001b[0;36mnetwork.backward_prop\u001b[0;34m(self, input_values, trueOutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer\u001b[39m.\u001b[39mcacheValue \u001b[39m=\u001b[39m input_values\n\u001b[1;32m     75\u001b[0m predictedOutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer\u001b[39m.\u001b[39mgetActivation()\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer\u001b[39m.\u001b[39;49mcorrect_error(trueOutput \u001b[39m-\u001b[39;49m predictedOutput)\n",
      "Cell \u001b[0;32mIn[331], line 175\u001b[0m, in \u001b[0;36mnlayer.correct_error\u001b[0;34m(self, activation_error)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m# if(len(inputArr.shape) == 1):       ## if array is 1D, convert to 2D to support Transpose.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m#     inputArrT = inputArr[np.newaxis].T\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m#     inputArrT = inputArr.T\u001b[39;00m\n\u001b[1;32m    173\u001b[0m dZ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcacheValue \u001b[39m-\u001b[39m activation_error\n\u001b[0;32m--> 175\u001b[0m dW \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mk)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mmatmul(dZ, inputArr2)\n\u001b[1;32m    176\u001b[0m dB \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mk)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39msum(dZ)\n\u001b[1;32m    178\u001b[0m oldWeights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
     ]
    }
   ],
   "source": [
    "accuracySum = 0\n",
    "runCount = 0\n",
    "\n",
    "for it in range(len(X_trainT)):\n",
    "    one_hot_y = one_hot(Y_train[it], 9)\n",
    "    pred = get_predictions(nt.backward_prop(X_trainT[it],one_hot_y))\n",
    "    if(pred.all() == one_hot_y.all()):\n",
    "        accuracySum += 1\n",
    "    runCount += 1\n",
    "    \n",
    "# accuracy = accuracySum/len(X_train)\n",
    "accuracy = accuracySum/runCount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979591836734694"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(3,1, insertDefault=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.setInput([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.output_layer.input_layers[0].input_layers[0].cacheValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "print(n1.output_layer.input_layers[0].cachedRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.setInput([3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73495495]\n"
     ]
    }
   ],
   "source": [
    "print(n1.forward_prop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "xIn = np.array([1.0,2.0,3.0])\n",
    "yOut = np.array([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16821053]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.output_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(n1.backward_prop(xIn, yOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "telementary = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71219564]\n",
      "[-0.14245337]\n",
      "[-0.1139627]\n",
      "[-0.09117016]\n",
      "[-0.07293613]\n",
      "[-0.0583489]\n",
      "[-0.04667912]\n",
      "[-0.0373433]\n",
      "[-0.02987464]\n",
      "[-0.02389971]\n",
      "[-0.01911977]\n",
      "[-0.01529581]\n",
      "[-0.01223665]\n",
      "[-0.00978932]\n",
      "[-0.00783146]\n",
      "[-0.00626517]\n",
      "[-0.00501213]\n",
      "[-0.00400971]\n",
      "[-0.00320776]\n",
      "[-0.00256621]\n"
     ]
    }
   ],
   "source": [
    "for j in range(20):\n",
    "    for i in range(10000):\n",
    "        n1.backward_prop(xIn, yOut)\n",
    "\n",
    "    print(n1.forward_prop(xIn))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.4012041])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.forward_prop(xIn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layerTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inLay = nlayer(2, isInput=1, setInputValues=[2,3])\n",
    "\n",
    "hidLay = nlayer(1, inputLayers=[inLay], activationFn=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.nlayer at 0x7fb6699f7d00>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidLay.bias\n",
    "# hidLay.weights\n",
    "# hidLay.cacheValue\n",
    "hidLay.input_layers\n",
    "# inLay.input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation = [0.97440479] & cached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.97440479])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.getActivation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newInLay = nlayer(3,isInput=1,setInputValues=[2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidLay.addInputLayer(newInputLayer=newInLay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99885793, 0.9725909 , 0.72486948, 0.35375928, 0.07909738]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidLay.addWidth_to_Layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99885793, 0.9725909 , 0.72486948, 0.35375928, 0.07909738],\n",
       "       [0.10659773, 0.55853347, 0.62134468, 0.07597125, 0.98782191],\n",
       "       [0.06992685, 0.24441412, 0.73520054, 0.26422487, 0.89743243]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.99525239, 6.29679149, 7.71707944])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidLay.getActivation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerLengths = [2,3,1]\n",
    "        \n",
    "        ## Splitting input corrections to their corresponding layers\n",
    "splitPoints = []\n",
    "lengthTillNow = 0\n",
    "for length in layerLengths:\n",
    "    lengthTillNow += length\n",
    "    splitPoints.append(lengthTillNow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 2)\n",
      "range(2, 5)\n",
      "range(5, 6)\n"
     ]
    }
   ],
   "source": [
    "## Splitting input corrections to their corresponding layers\n",
    "\n",
    "layerLengths = [2,3,1]\n",
    "dIZ = range(6)\n",
    "splitPoints = [0]\n",
    "lengthTillNow = 0\n",
    "for layerIndx in range(len(layerLengths)):\n",
    "    lengthTillNow += layerLengths[layerIndx]\n",
    "    splitPoints.append(lengthTillNow)\n",
    "\n",
    "    print(dIZ[splitPoints[-2]:splitPoints[-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 6]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[2,3,4]])\n",
    "# x = np.array([2,3,4])\n",
    "# x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = x[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m]])\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39;49mmatmul(y, x\u001b[39m.\u001b[39;49mT))\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
     ]
    }
   ],
   "source": [
    "y = np.array([[1]])\n",
    "\n",
    "print(np.matmul(y, x.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51025164, 0.76537746, 1.02050328])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ = np.array([0.25512582])\n",
    "inputArr = np.array([[2.0,3.0,4.0]])\n",
    "np.matmul(dZ,inputArr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51025164, 0.76537746, 1.02050328])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(dZ,inputArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empArr = np.empty(1,dtype=float)\n",
    "empArr = np.array([4,],dtype=float)\n",
    "inArr = np.array([1,2],dtype=float)\n",
    "# inArr = np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(inArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 1. 2. 1. 2.]\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "empArr = np.concatenate((empArr, inArr))\n",
    "print(empArr)\n",
    "print(empArr.shape)\n",
    "# print(newArr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.array([[3,2,1],[1,0,-1]],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wts (2, 3)\n",
      "emp (3,)\n",
      "act [25.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"wts\", wts.shape)\n",
    "print(\"emp\", empArr.shape)\n",
    "act = np.matmul(wts,empArr)\n",
    "print(\"act\", act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 1., 2., 1., 2.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to add = empArr.shape[0] - wts.shape[1]\n",
    "newCols = np.random.rand(wts.shape[0], empArr.shape[0] - wts.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  2.,  1.],\n",
       "       [ 1.,  0., -1.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.concatenate((wts, newCols), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21734973, 0.22785161, 0.77439285],\n",
       "       [0.26069918, 0.76749742, 0.33375012]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2,3]\n",
    "y = x.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 0], [0, 1]])\n",
    "b = np.array([[4, 1], [2, 2]])\n",
    "a.dot(b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = network(2,1)\n",
    "\n",
    "in1 = np.array([0,1,2])\n",
    "wtMat = np.array([[5,6,7],[8,9,10]])\n",
    "# biases = np.array([5,25])\n",
    "biases = np.array([0.5,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.5  29.25]\n"
     ]
    }
   ],
   "source": [
    "output_activations = np.matmul(wtMat, in1) + biases\n",
    "print(output_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.5 , 29.25])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.forward_prop(input_activations=in1, weight_matrix=wtMat, bias_ndarrray=biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb745f7c98c08174c20a2b94d108f69dcc884658473fbc3700597e1ee8c317ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
